Project 8. Emoticons generation Analysis
This project further explores the connection between emoticons and emotion text using some existing lexicons
and embedding-based analysis. We particularly focus on the eight emotions: anger, anticipation, disgust, fear,
joy, sadness, surprise, trust.
1. We consider emoTag1200 dataset https://github.com/abushoeb/EmoTag/blob/master/data/EmoTag1200-scores.csv
where for each emoticon is assigned scores to each of the above eight emotions based on some
manual annotation task. Identify a resource where each emoticon is assigned the corresponding text.
2. We want to test the compatibility between the emoticons‚Äô definitions and Tag1200 labeling. For this
purpose, suggest a script that generates an Embedding vector (i.e., word2vec embedding) of the textual
definition of a given emoticon. Then, use the same embedding for each for the eight emoticons, and
calculate the cosine similarity between the embedding of the emoticon‚Äôs textual definition and the
embedding vector of each of the 8 emotion names. This yields an eight dimension vector quantifying the
similarity to each of the emotions. Finally compute the Pearson correlation coefficient as a measure that
evaluates the mapping between emoticon definition and Tag1200‚Äôs labeling.
3. Repeat the process in 2) when different embedding strategies were employed, e.g., doc2vec, FastText,
Glove, BERT. Summarize the result in appropriate table and Comment on the findings.
4. Consider the Twitter dataset available in Kaggle Tweets With Emoji (kaggle.com) where for each emoji
there are around 20K tweets. Choose 6 emoji of your choice, containing antagonist emoji, e.g.,
happiness and sadness, with their associated 20K tweets. Suggest a script that determines for each emoji
the 30 most frequent terms in their associated 20K tweet messages. Write down a script that generates a
matrix showing the number of common wording among the thirty most frequent words identified.
5. Write a script that inputs the 20K tweet messages for each emoji and outputs the embedding vector using
DistilBERT. Then use the available TSNE and its scikit-learn implementation TSNE ‚Äî scikit-learn
1.5.2 documentation to project each embedding vector into a 2D space. Finally, draw in a 2D space the
corresponding the result of each of the six emojis. Comment on the proximity between these emojis
based on the expected intuition behind each emoiji.
6. Repeat 5) when the embedding is constructed using the Empath categorization
https://github.com/Ejhfast/empath-client for each emoji, and draw the corresponding 2D plot where each
emoji is represented as a point in the graph.
7. Instead of using the Empath categorization, we want to use topic modelling to handle the category-
record matching. For this purpose, write a script that uses LDA topic modelling with one topic and 8
words per topic. For each emoiji labelled datata (20K tweet messages), we would like to investigate this
matching by studying the mapping between the six keywords generated by the LDA and the
corresponding labelled emoji. Suggest an approach that would allow you to match between the eight
generated keywords and the emoji label in view of emoTag construction. For instance, assume the
output of LDA is keywords K1, K2,.., K8, and assume the emoji G has weight vector [x1, x2, x3, ‚Ä¶, x8]
Emoji_weight = ‚àë ùë•ùëñùëñ‚ààùêº where I stands for the set of keywords among K1-K8 that coincide with
keywords generated by LDA scheme.
8. Instead of using LDA, we use another state-of-the-art topic modelling technique based on transformer:
BerTopic, see https://github.com/MaartenGr/BERTopic. Study the example provided in github and
suggest how you will proceed to imitate the reasoning of 7) to test how BERTopic generated topics can
be matched with emoTag labelling. Use the illustration provided in BERTopic to draw the result of this
topic modelling. Summarize the result in a table.
9. We want to study the reverse process from text to emoji. For this purpose, study the existing MIT
DeepMoji project https://github.com/huggingface/torchMoji where for a given textual post, an
automatically generated emoji is outputted. Suggest a script that output for each tweet the corresponding
emoji. Next, we want to use to this approach to evaluate the result with respect to the emoji label. For
this purpose, for the 20K tweets associated with a given emoji label, say, G, calculate the proportion of
emoji G generated by DeepMoiji. Summarize the result in a table showing the proportion of matching
for each class.
10. We want to explore the antagonism relationship to perform another evaluation using NLP modules,
building on the assumption that such a relation should yield disparate evidence. For this purpose,
consider dataframe corresponding to two antagonist emoji (e.g., sadness and happiness), and consider
again the set of 20 most frequent terms of each dataframe, excluding stopwords and uncommon
characters, say H1 and H2 (sadness and happiness), and then we want to evaluate the proportion of
words between the two sets H1 and H2 that are linked through antonymy relation. Suggest a script that
enables you to implement this reasoning using wordnet or any lexical database of your choice.
11. Use appropriate literature to comment on the findings. Also, identify any additional input that would
allow you to further elucidate any of the preceding, and use appropriate literature of corpus linguistic
literature to justify your findings and comment on the obtained results. Finally, comment on the
limitations and structural weakness of the data processing pipeline