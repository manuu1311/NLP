{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] is compatible.\n"
     ]
    }
   ],
   "source": [
    "# This notebook requires Python 3.12.3 or higher\n",
    "\n",
    "import sys\n",
    "required_version = (3, 12, 3)\n",
    "if sys.version_info < required_version:\n",
    "    raise Exception(f\"This notebook requires Python {required_version} or higher!\")\n",
    "else:\n",
    "    print(f\"Python version {sys.version} is compatible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3\n",
    "\n",
    "The aim of this lab is to test the similarity between two sentences using online lexical database WordNet. The students can refer to the original paper of Mihalcea et al. (Corpus-based and Knowledge-based Measures of Text Semantic Similarity), appeared in AAAI 2006. See, (https://www.aaai.org/Papers/AAAI/2006/AAAI06-123.pdf)\n",
    "\n",
    "## Task 1: For early practice, study Section 5 of Chapter 2 of NLTK online book, and try to reproduce the coding examples and try to use your own examples of wording to identify the synsets, hyponyms, hypernyms, and various semantic similarity between two words of your choice. Suggest a script that retrieves the first hypernym and the list of all hyponyms of words ‘car’ and ‘bus’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from collections.abc import Callable\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: car\n",
      "First Hypernym: motor_vehicle\n",
      "List of Hyponyms: ['minicar', 'compact', 'hot_rod', 'cruiser', 'hatchback', 'sedan', 'stock_car', 'sports_car', 'cab', 'racer', 'hardtop', 'model_t', 'minivan', 'limousine', 'used-car', 'bus', 'sport_utility', 'horseless_carriage', 'ambulance', 'roadster', 'convertible', 'gas_guzzler', 'subcompact', 'touring_car', 'beach_wagon', 'coupe', 'pace_car', 'stanley_steamer', 'jeep', 'electric', 'loaner', 'handcar', 'freight_car', 'tender', 'mail_car', 'cabin_car', \"guard's_van\", 'club_car', 'passenger_car', 'van', 'baggage_car', 'slip_coach']\n",
      "----------------------------------------\n",
      "Word: bus\n",
      "First Hypernym: public_transport\n",
      "List of Hyponyms: ['trolleybus', 'school_bus', 'minibus']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_hypernym_hyponyms(word: str) -> tuple[Synset, list[Synset]]:\n",
    "    synsets = wn.synsets(word)\n",
    "\n",
    "    if not synsets:\n",
    "        return None, []\n",
    "\n",
    "    # Get the first hypernym from the first synset (most common usage)\n",
    "    hypernyms = synsets[0].hypernyms()\n",
    "    first_hypernym = hypernyms[0] if hypernyms else None\n",
    "\n",
    "    # Get all hyponyms\n",
    "    hyponyms = []\n",
    "    for synset in synsets:\n",
    "        hyponyms.extend(synset.hyponyms())\n",
    "\n",
    "    return first_hypernym, hyponyms\n",
    "\n",
    "\n",
    "def get_hypernym_hyponyms_str(word: str) -> tuple[str, list[str]]:\n",
    "    first_hypernym, hyponyms = get_hypernym_hyponyms(word)\n",
    "    first_hypernym_name = first_hypernym.name().split('.')[0] if first_hypernym else None\n",
    "\n",
    "    hyponym_names = [hyponym.name().split('.')[0] for hyponym in hyponyms]\n",
    "\n",
    "    return first_hypernym_name, hyponym_names\n",
    "\n",
    "# Words to test\n",
    "words = ['car', 'bus']\n",
    "\n",
    "for word in words:\n",
    "    hypernym, hyponyms = get_hypernym_hyponyms_str(word)\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"First Hypernym: {hypernym}\")\n",
    "    print(f\"List of Hyponyms: {hyponyms}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Suggest another script that extracts the synsets of the word “car” and rank them in the order of their frequency of occurrence (most common synset first, less common synset at the end). For this purpose, you may use the coding:\n",
    "```python \n",
    "car = wn.synsets('car', 'n')[0]  # Get the most common synset\n",
    "print car.lemmas()[0].count()  # Get the first lemma\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets of 'car' ranked by frequency:\n",
      "Synset: car.n.01, Frequency: 89, Definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Synset: car.n.02, Frequency: 2, Definition: a wheeled vehicle adapted to the rails of railroad\n",
      "Synset: car.n.03, Frequency: 0, Definition: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "Synset: car.n.04, Frequency: 0, Definition: where passengers ride up and down\n",
      "Synset: cable_car.n.01, Frequency: 0, Definition: a conveyance for passengers or freight on a cable railway\n"
     ]
    }
   ],
   "source": [
    "def rank_synsets_by_frequency(word: str) -> list[tuple[Synset, int]]:\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "\n",
    "    # Create a list of tuples (synset, frequency) sorted by frequency in descending order\n",
    "    synset_frequencies = []\n",
    "    for synset in synsets:\n",
    "        lemmas = synset.lemmas()\n",
    "        if lemmas:\n",
    "            frequency = sum([lemma.count() for lemma in lemmas])\n",
    "            synset_frequencies.append((synset, frequency))\n",
    "\n",
    "    # We should not need to do because synsets supposed to sort this already\n",
    "    # But there is too few documentation on this, only 3rd party claims, so sorting it anyway\n",
    "    synset_frequencies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return synset_frequencies\n",
    "\n",
    "\n",
    "# Display the synsets for 'car' ranked by frequency\n",
    "synset_frequencies = rank_synsets_by_frequency('car')\n",
    "print(f\"Synsets of 'car' ranked by frequency:\")\n",
    "for synset, freq in synset_frequencies:\n",
    "    print(f\"Synset: {synset.name()}, Frequency: {freq}, Definition: {synset.definition()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Now we want to use the WordNet semantic similarity to evaluate the similarity between the words. Suggest a script that calculates the Wu and Palmer semantic similarity between words ‘car’ and ‘bus’ in terms of maximum S1, minimum  S2 and average S3 over all synsets of these words (in other words, combination of synsets that yields the maximum, minimum Wu and Palmer similarity as well as the average similarity over all combination of synsets in ‘car’ and ‘bus’). Repeat this process by calculating the Wu and Palmer similarity between the first hypernym of ‘car’ and first hypernym of ‘bus’, and the new values for S1, S2 and S3. Next, repeat this process for hyponyms words; that is calculate the Wu and Palmer between every hyponym of ‘car’ and that of ‘bus’ and then take arithmetic average of all hyponym-pairs as the new Hyponym-based similarity values, and then consider the new evaluations of S1, S2 and S3 when all synsets are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu & Palmer Similarity between all synsets of 'car' and 'bus':\n",
      "Max (S1): 0.96, Min (S2): 0.09523809523809523, Avg (S3): 0.46739299830604175\n",
      "Wu & Palmer Similarity between first hypernyms of 'car' and 'bus':\n",
      "Max (S1): 0.7368421052631579, Min (S2): 0.7368421052631579, Avg (S3): 0.7368421052631579\n",
      "Wu & Palmer Similarity between hyponyms of 'car' and 'bus':\n",
      "Max (S1): 0.9473684210526315, Min (S2): 0.1, Avg (S3): 0.5957758922012587\n",
      "Merged Similarity Score:\n",
      "Max (S1): 0.96, Min (S2): 0.09523809523809523, Avg (S3): 0.6000036652568195\n"
     ]
    }
   ],
   "source": [
    "def wu_palmer_similarity(synset1, synset2):\n",
    "    try:\n",
    "        return synset1.wup_similarity(synset2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def similarity(word1: str, word2: str, compare_function: Callable[[Synset, Synset], float | None], word_type: str=wn.NOUN) -> tuple[float, float, float]:\n",
    "    synsets1 = wn.synsets(word1, word_type)\n",
    "    synsets2 = wn.synsets(word2, word_type)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            sim = compare_function(synset1, synset2)\n",
    "            if sim is not None:\n",
    "                similarities.append(sim)\n",
    "\n",
    "    if similarities:\n",
    "        s1 = max(similarities)  # Maximum similarity\n",
    "        s2 = min(similarities)  # Minimum similarity\n",
    "        s3 = sum(similarities) / len(similarities)  # Average similarity\n",
    "        return s1, s2, s3\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def hyponym_similarity(hyponyms1: list[str], hyponyms2: list[str], compare_function: Callable[[Synset, Synset], float | None], word_type: str=wn.NOUN) -> tuple[float, float, float]:\n",
    "    s1_scores = []\n",
    "    s2_scores = []\n",
    "    s3_scores = []\n",
    "\n",
    "    for hyponym1 in hyponyms1:\n",
    "        for hyponym2 in hyponyms2:\n",
    "            s1, s2, s3 = similarity(hyponym1, hyponym2, compare_function, word_type)\n",
    "            if s1 is not None:\n",
    "                s1_scores.append(s1)\n",
    "\n",
    "            if s2 is not None:\n",
    "                s2_scores.append(s2)\n",
    "\n",
    "            if s3 is not None:\n",
    "                s3_scores.append(s3)\n",
    "\n",
    "    s1 = max(s1_scores) if s1_scores else None\n",
    "    s2 = min(s2_scores) if s2_scores else None\n",
    "    s3 = sum(s3_scores) / len(s3_scores) if s3_scores else None\n",
    "    return s1, s2, s3\n",
    "\n",
    "def print_word_similarity(word1: str, word2: str, compare_function: Callable[[Synset, Synset], float | None], word_type: str=wn.NOUN) -> None:\n",
    "    hypernym1, hyponyms1 = get_hypernym_hyponyms_str(word1)\n",
    "    hypernym2, hyponyms2 = get_hypernym_hyponyms_str(word2)\n",
    "    s1_synset, s2_synset, s3_synset = similarity(word1, word2, compare_function, word_type)\n",
    "    s1_hypernym, s2_hypernym, s3_hypernym = similarity(hypernym1, hypernym2, compare_function, word_type)\n",
    "    s1_hyponym, s2_hyponym, s3_hyponym = hyponym_similarity(hyponyms1, hyponyms2, compare_function, word_type)\n",
    "\n",
    "    print(f\"Wu & Palmer Similarity between all synsets of '{word1}' and '{word2}':\")\n",
    "    print(f\"Max (S1): {s1_synset}, Min (S2): {s2_synset}, Avg (S3): {s3_synset}\")\n",
    "    print(f\"Wu & Palmer Similarity between first hypernyms of '{word1}' and '{word2}':\")\n",
    "    print(f\"Max (S1): {s1_hypernym}, Min (S2): {s2_hypernym}, Avg (S3): {s3_hypernym}\")\n",
    "    print(f\"Wu & Palmer Similarity between hyponyms of '{word1}' and '{word2}':\")\n",
    "    print(f\"Max (S1): {s1_hyponym}, Min (S2): {s2_hyponym}, Avg (S3): {s3_hyponym}\")\n",
    "\n",
    "    print(f\"Merged Similarity Score:\")\n",
    "    s1_list = list(filter(None, [s1_synset, s1_hypernym, s1_hyponym]))\n",
    "    s2_list = list(filter(None, [s2_synset, s2_hypernym, s2_hyponym]))\n",
    "    s3_list = list(filter(None, [s3_synset, s3_hypernym, s3_hyponym]))\n",
    "    max_s1 = max(s1_list)\n",
    "    min_s2 = min(s2_list)\n",
    "    avg_s3 = sum(s3_list) / len(s3_list)\n",
    "    print(f\"Max (S1): {max_s1}, Min (S2): {min_s2}, Avg (S3): {avg_s3}\")\n",
    "\n",
    "\n",
    "word1 = 'car'\n",
    "word2 = 'bus'\n",
    "print_word_similarity(word1, word2, compare_function=wu_palmer_similarity, word_type=wn.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Repeat Task-3: when Jiang-Conrath similarity is employed where the corpus consists of Brown corpus, see https://www.nltk.org/howto/wordnet.html for examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu & Palmer Similarity between all synsets of 'car' and 'bus':\n",
      "Max (S1): 0.34659468740185323, Min (S2): 0.05161364962677664, Avg (S3): 0.09387159388812354\n",
      "Wu & Palmer Similarity between first hypernyms of 'car' and 'bus':\n",
      "Max (S1): 0.27016908921466043, Min (S2): 0.27016908921466043, Avg (S3): 0.27016908921466043\n",
      "Wu & Palmer Similarity between hyponyms of 'car' and 'bus':\n",
      "Max (S1): 1e-300, Min (S2): 5e-301, Avg (S3): 7.44047619047619e-301\n",
      "Merged Similarity Score:\n",
      "Max (S1): 0.34659468740185323, Min (S2): 5e-301, Avg (S3): 0.12134689436759466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "\n",
    "nltk.download('wordnet_ic')\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def jiang_conrath_similarity(synset1, synset2):\n",
    "    try:\n",
    "        return synset1.jcn_similarity(synset2, brown_ic)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "word1 = 'car'\n",
    "word2 = 'bus'\n",
    "print_word_similarity(word1, word2, compare_function=jiang_conrath_similarity, word_type=wn.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Now consider two sentences T1 and T2, each constituted with a set of tokens. For this purpose, study expression (1) of the aforementioned Mihalcea et al.’s paper above (see below).  You can check with a potential implementation available in Mihalcea’s resources and elsewhere. Start with sentences: T1: “Students feel unhappy today about the class today”. T2: ”Several students study hard at classes in recent days”,  and study the influence of various preprocessing (stopword removal, stemming) on the result of the sentence-to-sentence similarity above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Consider a new approach of calculating the semantic similarity by transforming all words of sentence in their noun counterpart and then calculate the maximum similarity score as in Mihalcea’s formula.. The extraction of the noun part of each token of the sentence can be performed using ‘morphy’ function in wordnet, see example in https://www.nltk.org/howto/wordnet.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Now consider a new sentence-to-sentence similarity where the similarity score is calculated as the cosine similarity of embedding vectors of the two sentences and where the embedding vector of each sentence is the average of FastText embedding vector of each word constituting the sentence prior to any pre-processing stage. Write a program that implements this similarity metric and compute the sentence-to-sentence similarity of T1 and T2.  Repeat this process when using word2vec embeddings and doc2vec embedding.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Implement a program that calculates the sentence-to-sentence similarity as the result of the FuzzyWuzzy score of comparison of string of both sentences, after initial preprocessing and lemmatization using wordnet lemmatizer. Calculate the new similarity score between sentence T1 and T2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
