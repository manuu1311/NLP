{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     c:\\Users\\luigi\\Desktop\\Uni\\Oulu University\\NLP\\Lab3...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     c:\\Users\\luigi\\Desktop\\Uni\\Oulu University\\NLP\\Lab3...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\luigi\\Desktop\\Uni\\Oulu University\\NLP\\Lab3...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# path='D:/misc/Projects/Python/NLP/misc'\n",
    "path=os.getcwd()\n",
    "nltk.download('wordnet_ic',download_dir=path)\n",
    "nltk.download('wordnet',download_dir=path)\n",
    "nltk.download('punkt_tab',download_dir=path)\n",
    "nltk.data.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn=nltk.corpus.wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['sock', 'pasta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---sock---\n",
      "Synset: ['sock']\n",
      "\n",
      "Hyponims:\n",
      "['knee-high', 'knee-hi']\n",
      "['argyle', 'argyll']\n",
      "['anklet', 'anklets', 'bobbysock', 'bobbysocks']\n",
      "['tabi', 'tabis']\n",
      "['athletic_sock', 'sweat_sock', 'varsity_sock']\n",
      "\n",
      "Hypernyms:\n",
      "['knee-high', 'knee-hi']\n",
      "['argyle', 'argyll']\n",
      "['anklet', 'anklets', 'bobbysock', 'bobbysocks']\n",
      "['tabi', 'tabis']\n",
      "['athletic_sock', 'sweat_sock', 'varsity_sock']\n",
      "Synset: ['windsock', 'wind_sock', 'sock', 'air_sock', 'air-sleeve', 'wind_sleeve', 'wind_cone', 'drogue']\n",
      "\n",
      "Hyponims:\n",
      "\n",
      "Hypernyms:\n",
      "Synset: ['sock', 'bop', 'whop', 'whap', 'bonk', 'bash']\n",
      "\n",
      "Hyponims:\n",
      "\n",
      "Hypernyms:\n",
      "---pasta---\n",
      "Synset: ['pasta']\n",
      "\n",
      "Hyponims:\n",
      "['lasagna', 'lasagne']\n",
      "['macaroni_and_cheese']\n",
      "['spaghetti']\n",
      "['cannelloni']\n",
      "\n",
      "Hypernyms:\n",
      "['lasagna', 'lasagne']\n",
      "['macaroni_and_cheese']\n",
      "['spaghetti']\n",
      "['cannelloni']\n",
      "Synset: ['pasta', 'alimentary_paste']\n",
      "\n",
      "Hyponims:\n",
      "['noodle']\n",
      "['ziti']\n",
      "['manicotti']\n",
      "['vermicelli']\n",
      "['couscous']\n",
      "['tagliatelle']\n",
      "['lasagna', 'lasagne']\n",
      "['farfalle', 'bowtie_pasta']\n",
      "['spaghettini']\n",
      "['fettuccine', 'fettuccini']\n",
      "['rigatoni']\n",
      "['macaroni']\n",
      "['penne']\n",
      "['mostaccioli']\n",
      "['fedelline']\n",
      "['spaghetti']\n",
      "['dumpling', 'dumplings']\n",
      "['orzo']\n",
      "['tortellini']\n",
      "['ravioli', 'cappelletti']\n",
      "['linguine', 'linguini']\n",
      "\n",
      "Hypernyms:\n",
      "['noodle']\n",
      "['ziti']\n",
      "['manicotti']\n",
      "['vermicelli']\n",
      "['couscous']\n",
      "['tagliatelle']\n",
      "['lasagna', 'lasagne']\n",
      "['farfalle', 'bowtie_pasta']\n",
      "['spaghettini']\n",
      "['fettuccine', 'fettuccini']\n",
      "['rigatoni']\n",
      "['macaroni']\n",
      "['penne']\n",
      "['mostaccioli']\n",
      "['fedelline']\n",
      "['spaghetti']\n",
      "['dumpling', 'dumplings']\n",
      "['orzo']\n",
      "['tortellini']\n",
      "['ravioli', 'cappelletti']\n",
      "['linguine', 'linguini']\n",
      "Synset('macaroni.n.01') - Synset('couscous.n.01'): 0.07692307692307693\n",
      "Synset('sock.n.01') - Synset('shoe.n.01'): 0.14285714285714285\n",
      "Synset('sock.n.01') - Synset('carbonara.n.01'): 0.05263157894736842\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f'---{word}---')\n",
    "    for syns in wn.synsets(word):\n",
    "        print(f'Synset: {syns.lemma_names()}')\n",
    "        print('\\nHyponims:')\n",
    "        for hypo in syns.hyponyms():\n",
    "            print(f'{hypo.lemma_names()}')\n",
    "        print('\\nHypernyms:')\n",
    "        for hyper in syns.hyponyms():\n",
    "            print(f'{hyper.lemma_names()}')\n",
    "\n",
    "w1=wn.synsets('macaroni')[0]\n",
    "w2=wn.synsets('couscous')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')\n",
    "w1=wn.synsets('sock')[0]\n",
    "w2=wn.synsets('shoe')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')\n",
    "w1=wn.synsets('sock')[0]\n",
    "w2=wn.synsets('carbonara')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('macaroni.n.01') - Synset('couscous.n.01'): 0.07692307692307693\n",
      "Synset('sock.n.01') - Synset('shoe.n.01'): 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "w1=wn.synsets('macaroni')[0]\n",
    "w2=wn.synsets('couscous')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')\n",
    "w1=wn.synsets('sock')[0]\n",
    "w2=wn.synsets('shoe')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car: \n",
      "First hypernim: motor_vehicle\n",
      "List of hyponims:\n",
      "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap', 'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon', 'wagon']\n",
      "Car: \n",
      "First hypernim: public_transport\n",
      "List of hyponims:\n",
      "['minibus', 'school_bus', 'trackless_trolley', 'trolley_coach', 'trolleybus']\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar=sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())\n",
    "hypernim=motorcar.hypernyms()[0]\n",
    "print(f'Car: \\nFirst hypernim: {hypernim.lemmas()[0].name()}\\nList of hyponims:\\n{types_of_motorcar}')\n",
    "\n",
    "motorcar = wn.synsets('bus')[0]\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar=sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())\n",
    "hypernim=motorcar.hypernyms()[0]\n",
    "print(f'Car: \\nFirst hypernim: {hypernim.lemmas()[0].name()}\\nList of hyponims:\\n{types_of_motorcar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# Get the most common synset\n",
    "car = wn.synsets('car', 'n')[0] \n",
    "# Get the first lemma\n",
    "print(car.lemmas()[0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine , occurrencies: 89\n",
      "definition: a wheeled vehicle adapted to the rails of railroad , occurrencies: 2\n",
      "definition: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant , occurrencies: 0\n",
      "definition: where passengers ride up and down , occurrencies: 0\n",
      "definition: a conveyance for passengers or freight on a cable railway , occurrencies: 0\n"
     ]
    }
   ],
   "source": [
    "car=wn.synsets('car','n')\n",
    "occs=[]\n",
    "for syns in car:\n",
    "    lemmas=syns.lemmas()\n",
    "    occ=sum(lemma.count() for lemma in lemmas)\n",
    "    occs.append(occ)\n",
    "    print('definition:',syns.definition(),', occurrencies:',occ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.96, S2: 0.09523809523809523, S3: 0.32313013268548524\n"
     ]
    }
   ],
   "source": [
    "#car and bus\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    for syns2 in w2:\n",
    "        similarities.append(syns1.wup_similarity(syns2))\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.9565217391304348, S2: 0.10526315789473684, S3: 0.3609781015028789\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hyper1=syns1.hypernyms()[0]\n",
    "    for syns2 in w2:\n",
    "        hyper2=syns2.hypernyms()[0]\n",
    "        similarities.append(hyper1.wup_similarity(hyper2))\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.6666666666666666, S2: 0.6086956521739131, S3: 0.6238785369220152\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hypers1=syns1.hyponyms()\n",
    "    for hyper1 in hypers1:\n",
    "        for syns2 in w2:\n",
    "            hypers2=syns2.hyponyms()\n",
    "            for hyper2 in hypers2:\n",
    "                similarities.append(hyper1.wup_similarity(hyper2))\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_ic = nltk.corpus.wordnet_ic.ic('ic-brown.dat')\n",
    "def jcsim(synset1, synset2):\n",
    "    try:\n",
    "        return synset1.jcn_similarity(synset2, brown_ic)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.34659468740185323, S2: 0.05161364962677664, S3: 0.09387159388812354\n"
     ]
    }
   ],
   "source": [
    "#car and bus\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    for syns2 in w2:\n",
    "        sim=jcsim(syns1, syns2)\n",
    "        if sim:\n",
    "            similarities.append(sim)\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:3.1448166368979993, S2: 0.058690483603655634, S3: 0.30601655661534644\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hyper1=syns1.hypernyms()[0]\n",
    "    for syns2 in w2:\n",
    "        hyper2=syns2.hypernyms()[0]\n",
    "        sim=jcsim(hyper1, hyper2)\n",
    "        if sim:\n",
    "            similarities.append(sim)\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:1e-300, S2: 5e-301, S3: 7.380952380952381e-301\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hypers1=syns1.hyponyms()\n",
    "    for hyper1 in hypers1:\n",
    "        for syns2 in w2:\n",
    "            hypers2=syns2.hyponyms()\n",
    "            for hyper2 in hypers2:\n",
    "                sim=jcsim(hyper1, hyper2)\n",
    "                if sim:\n",
    "                    similarities.append(sim)\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['students feel unhappy today class today', 'several students study hard classes recent days']\n",
      "[' student feel unhappi today about the class today', ' sever student studi hard at class in recent day']\n",
      "[' student feel unhappi today class today', ' sever student studi hard class recent day']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "\n",
    "sentences = [\"Students feel unhappy today about the class today\",\n",
    "             \"Several students study hard at classes in recent days\"]\n",
    "\n",
    "# Only stop word removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "tokenized_sentences = [[word for word in sentence if word not in stop_words] for sentence in tokenized_sentences]\n",
    "\n",
    "# Join tokenized sentences back into strings\n",
    "sw_sentences = [' '.join(sentence) for sentence in tokenized_sentences]\n",
    "print(sw_sentences)\n",
    "\n",
    "#Only stemming\n",
    "ps = PorterStemmer()\n",
    "stem_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    " \n",
    "    # using reduce to apply stemmer to each word and join them back into a string\n",
    "    stemmed_sentence = reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    "    stem_sentences.append(stemmed_sentence)\n",
    "\n",
    "print(stem_sentences)\n",
    "\n",
    "# Both stemmed and stopwords removal\n",
    "\n",
    "sw_stem_sentences = []\n",
    "for sentence in sw_sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    # using reduce to apply stemmer to each word and join them back into a string\n",
    "    stemmed_sentence = reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n",
    "    sw_stem_sentences.append(stemmed_sentence)\n",
    "\n",
    "print(sw_stem_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05629716]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "vectorizer = TfidfVectorizer()    \n",
    "\n",
    "#Similarity for original sentences\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences) \n",
    "dense_tfidf=tfidf_matrix.todense()\n",
    "s1=np.asarray(dense_tfidf[0])\n",
    "s2=np.asarray(dense_tfidf[1])\n",
    "cosim = cosine_similarity(s1, s2)\n",
    "print(cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07244082]]\n"
     ]
    }
   ],
   "source": [
    "#Similarity after stopword removal\n",
    "tfidf_matrix = vectorizer.fit_transform(sw_sentences) \n",
    "dense_tfidf=tfidf_matrix.todense()\n",
    "s1=np.asarray(dense_tfidf[0])\n",
    "s2=np.asarray(dense_tfidf[1])\n",
    "cosim = cosine_similarity(s1, s2)\n",
    "print(cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11914719]]\n"
     ]
    }
   ],
   "source": [
    "#Similarity after stemming\n",
    "tfidf_matrix = vectorizer.fit_transform(stem_sentences) \n",
    "dense_tfidf=tfidf_matrix.todense()\n",
    "s1=np.asarray(dense_tfidf[0])\n",
    "s2=np.asarray(dense_tfidf[1])\n",
    "cosim = cosine_similarity(s1, s2)\n",
    "print(cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15592893]]\n"
     ]
    }
   ],
   "source": [
    "#Similarity after stopword removal and stemming\n",
    "tfidf_matrix = vectorizer.fit_transform(sw_stem_sentences) \n",
    "dense_tfidf=tfidf_matrix.todense()\n",
    "s1=np.asarray(dense_tfidf[0])\n",
    "s2=np.asarray(dense_tfidf[1])\n",
    "cosim = cosine_similarity(s1, s2)\n",
    "print(cosim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student\n",
      "feel\n",
      "unhappy\n",
      "today\n",
      "about\n",
      "None\n",
      "class\n",
      "today\n",
      "several\n",
      "student\n",
      "study\n",
      "hard\n",
      "at\n",
      "class\n",
      "in\n",
      "recent\n",
      "days\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "noun_phrases=[]\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        noun_word = wn.morphy(word.lower())\n",
    "        print(noun_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\luigi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.8731748052629973\n"
     ]
    }
   ],
   "source": [
    "from math import log, sqrt\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "# 1. Max similarity function between a word and a list of words (T2)\n",
    "def max_sim(word, text, word_sim_func):\n",
    "    # Compute similarity between word and each word in text, return the maximum value\n",
    "    return max(word_sim_func(word, w) for w in text)\n",
    "\n",
    "# Example word similarity function (cosine similarity using a basic bag-of-words or embeddings)\n",
    "# This is a placeholder; you can replace this with real similarity calculations using word embeddings (Word2Vec, etc.)\n",
    "def word_similarity(word1, word2):\n",
    "    # Here we assume simple matching, but you can replace it with vector similarity (cosine similarity, etc.)\n",
    "    return 1.0 if word1 == word2 else 0.0\n",
    "\n",
    "def calculate_idf(word, corpus, total_documents):\n",
    "    # Calculate document frequency (df)\n",
    "    df = 0\n",
    "    for fileid in corpus.fileids():\n",
    "        if word in corpus.words(fileid):\n",
    "            df += 1\n",
    "    # Apply the IDF formula\n",
    "    idf = log(total_documents / (1 + df))\n",
    "    return idf\n",
    "\n",
    "# 3. Text similarity function based on the provided formula\n",
    "def text_similarity(T1, T2, idf_func, word_sim_func, corpus, total_docs):\n",
    "    # Compute the numerator and denominator for the first part (T1 -> T2)\n",
    "    num_T1_T2 = sum(max_sim(w, T2, word_sim_func) * idf_func(w, corpus, total_docs) for w in T1)\n",
    "    denom_T1 = sum(idf_func(w, corpus, total_docs) for w in T1)\n",
    "    \n",
    "    # Compute the numerator and denominator for the second part (T2 -> T1)\n",
    "    num_T2_T1 = sum(max_sim(w, T1, word_sim_func) * idf_func(w, corpus, total_docs) for w in T2)\n",
    "    denom_T2 = sum(idf_func(w, corpus, total_docs) for w in T2)\n",
    "    \n",
    "    # Calculate the final similarity score\n",
    "    sim_score = 0.5 * ((num_T1_T2 / denom_T1) + (num_T2_T1 / denom_T2))\n",
    "    \n",
    "    return sim_score\n",
    "\n",
    "\n",
    "total_docs = len(brown.fileids())  # Total number of documents\n",
    "\n",
    "T1= sentences[0]\n",
    "T2= sentences[1]\n",
    "\n",
    "# Calculate the similarity\n",
    "similarity = text_similarity(T1, T2, calculate_idf, word_similarity, brown, total_docs)\n",
    "print(\"Similarity score:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "import fasttext.util\n",
    "\n",
    "# Load pre-trained FastText vectors\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext similarity score: 0.58745515\n"
     ]
    }
   ],
   "source": [
    "def sentence_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_embeddings = [model.get_word_vector(word) for word in words]\n",
    "    return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "# Calculate the embedding for each sentence\n",
    "embedding_T1 = sentence_embedding(T1, ft)\n",
    "embedding_T2 = sentence_embedding(T2, ft)\n",
    "\n",
    "# Compute the cosine similarity between the two sentence embeddings\n",
    "similarity = cosine_similarity([embedding_T1], [embedding_T2])\n",
    "print(\"Fasttext similarity score:\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luigi/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path)\n",
    "\n",
    "# Load the word2vec model\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "def sentence_to_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2, model):\n",
    "    vec1 = sentence_to_vector(sentence1, model)\n",
    "    vec2 = sentence_to_vector(sentence2, model)\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a doc2vec model (on brown corpus) since there are not any pretrained models available\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Prepare the data\n",
    "documents = [TaggedDocument(words=brown.words(fileid), tags=[fileid]) for fileid in brown.fileids()]\n",
    "\n",
    "# Train the model\n",
    "model_doc2vec = Doc2Vec(documents, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Calculate the similarity between two sentences\n",
    "def doc2vec_similarity(sentence1, sentence2, model):\n",
    "    vec1 = model.infer_vector(sentence1.split())\n",
    "    vec2 = model.infer_vector(sentence2.split())\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec similarity score: 0.61980164\n"
     ]
    }
   ],
   "source": [
    "#Using word2vec\n",
    "similarity = sentence_similarity(T1, T2, model)\n",
    "print(\"Word2Vec similarity score:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec similarity score: 0.64530694\n"
     ]
    }
   ],
   "source": [
    "#Using doc2vec\n",
    "similarity = doc2vec_similarity(T1, T2, model_doc2vec)\n",
    "print(\"Doc2Vec similarity score:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score between the sentences is: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luigi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    return fuzz.ratio(sentence1, sentence2)\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        words = sentence.split()\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "sentence1 = preprocess_sentence(T1)\n",
    "sentence2 = preprocess_sentence(T2)\n",
    "\n",
    "similarity_score = sentence_similarity(sentence1, sentence2)\n",
    "print(f\"The similarity score between the sentences is: {similarity_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
