{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     D:/misc/Projects/Python/NLP/misc...\n",
      "[nltk_data]   Unzipping corpora\\wordnet_ic.zip.\n"
     ]
    }
   ],
   "source": [
    "path='D:/misc/Projects/Python/NLP/misc'\n",
    "nltk.download('wordnet_ic',download_dir=path)\n",
    "nltk.data.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn=nltk.corpus.wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['sock', 'pasta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---sock---\n",
      "Synset: ['sock']\n",
      "\n",
      "Hyponims:\n",
      "['knee-high', 'knee-hi']\n",
      "['argyle', 'argyll']\n",
      "['anklet', 'anklets', 'bobbysock', 'bobbysocks']\n",
      "['tabi', 'tabis']\n",
      "['athletic_sock', 'sweat_sock', 'varsity_sock']\n",
      "\n",
      "Hypernyms:\n",
      "['knee-high', 'knee-hi']\n",
      "['argyle', 'argyll']\n",
      "['anklet', 'anklets', 'bobbysock', 'bobbysocks']\n",
      "['tabi', 'tabis']\n",
      "['athletic_sock', 'sweat_sock', 'varsity_sock']\n",
      "Synset: ['windsock', 'wind_sock', 'sock', 'air_sock', 'air-sleeve', 'wind_sleeve', 'wind_cone', 'drogue']\n",
      "\n",
      "Hyponims:\n",
      "\n",
      "Hypernyms:\n",
      "Synset: ['sock', 'bop', 'whop', 'whap', 'bonk', 'bash']\n",
      "\n",
      "Hyponims:\n",
      "\n",
      "Hypernyms:\n",
      "---pasta---\n",
      "Synset: ['pasta']\n",
      "\n",
      "Hyponims:\n",
      "['macaroni_and_cheese']\n",
      "['lasagna', 'lasagne']\n",
      "['spaghetti']\n",
      "['cannelloni']\n",
      "\n",
      "Hypernyms:\n",
      "['macaroni_and_cheese']\n",
      "['lasagna', 'lasagne']\n",
      "['spaghetti']\n",
      "['cannelloni']\n",
      "Synset: ['pasta', 'alimentary_paste']\n",
      "\n",
      "Hyponims:\n",
      "['macaroni']\n",
      "['penne']\n",
      "['mostaccioli']\n",
      "['fedelline']\n",
      "['spaghetti']\n",
      "['dumpling', 'dumplings']\n",
      "['orzo']\n",
      "['ravioli', 'cappelletti']\n",
      "['tortellini']\n",
      "['linguine', 'linguini']\n",
      "['tagliatelle']\n",
      "['noodle']\n",
      "['ziti']\n",
      "['manicotti']\n",
      "['vermicelli']\n",
      "['couscous']\n",
      "['lasagna', 'lasagne']\n",
      "['farfalle', 'bowtie_pasta']\n",
      "['spaghettini']\n",
      "['fettuccine', 'fettuccini']\n",
      "['rigatoni']\n",
      "\n",
      "Hypernyms:\n",
      "['macaroni']\n",
      "['penne']\n",
      "['mostaccioli']\n",
      "['fedelline']\n",
      "['spaghetti']\n",
      "['dumpling', 'dumplings']\n",
      "['orzo']\n",
      "['ravioli', 'cappelletti']\n",
      "['tortellini']\n",
      "['linguine', 'linguini']\n",
      "['tagliatelle']\n",
      "['noodle']\n",
      "['ziti']\n",
      "['manicotti']\n",
      "['vermicelli']\n",
      "['couscous']\n",
      "['lasagna', 'lasagne']\n",
      "['farfalle', 'bowtie_pasta']\n",
      "['spaghettini']\n",
      "['fettuccine', 'fettuccini']\n",
      "['rigatoni']\n",
      "Synset('macaroni.n.01') - Synset('couscous.n.01'): 0.07692307692307693\n",
      "Synset('sock.n.01') - Synset('shoe.n.01'): 0.14285714285714285\n",
      "Synset('sock.n.01') - Synset('carbonara.n.01'): 0.05263157894736842\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f'---{word}---')\n",
    "    for syns in wn.synsets(word):\n",
    "        print(f'Synset: {syns.lemma_names()}')\n",
    "        print('\\nHyponims:')\n",
    "        for hypo in syns.hyponyms():\n",
    "            print(f'{hypo.lemma_names()}')\n",
    "        print('\\nHypernyms:')\n",
    "        for hyper in syns.hyponyms():\n",
    "            print(f'{hyper.lemma_names()}')\n",
    "\n",
    "w1=wn.synsets('macaroni')[0]\n",
    "w2=wn.synsets('couscous')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')\n",
    "w1=wn.synsets('sock')[0]\n",
    "w2=wn.synsets('shoe')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')\n",
    "w1=wn.synsets('sock')[0]\n",
    "w2=wn.synsets('carbonara')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('macaroni.n.01') - Synset('couscous.n.01'): 0.07692307692307693\n",
      "Synset('sock.n.01') - Synset('shoe.n.01'): 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "w1=wn.synsets('macaroni')[0]\n",
    "w2=wn.synsets('couscous')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')\n",
    "w1=wn.synsets('sock')[0]\n",
    "w2=wn.synsets('shoe')[0]\n",
    "print(f'{w1} - {w2}: {w1.path_similarity(w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car: \n",
      "First hypernim: motor_vehicle\n",
      "List of hyponims:\n",
      "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap', 'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon', 'wagon']\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar=sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())\n",
    "hypernim=motorcar.hypernyms()[0]\n",
    "print(f'Car: \\nFirst hypernim: {hypernim.lemmas()[0].name()}\\nList of hyponims:\\n{types_of_motorcar}')\n",
    "\n",
    "motorcar = wn.synsets('bus')[0]\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar=sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())\n",
    "hypernim=motorcar.hypernyms()[0]\n",
    "print(f'Car: \\nFirst hypernim: {hypernim.lemmas()[0].name()}\\nList of hyponims:\\n{types_of_motorcar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# Get the most common synset\n",
    "car = wn.synsets('car', 'n')[0] \n",
    "# Get the first lemma\n",
    "print(car.lemmas()[0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine , occurrencies: 89\n",
      "definition: a wheeled vehicle adapted to the rails of railroad , occurrencies: 2\n",
      "definition: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant , occurrencies: 0\n",
      "definition: where passengers ride up and down , occurrencies: 0\n",
      "definition: a conveyance for passengers or freight on a cable railway , occurrencies: 0\n"
     ]
    }
   ],
   "source": [
    "car=wn.synsets('car','n')\n",
    "occs=[]\n",
    "for syns in car:\n",
    "    lemmas=syns.lemmas()\n",
    "    occ=sum(lemma.count() for lemma in lemmas)\n",
    "    occs.append(occ)\n",
    "    print('definition:',syns.definition(),', occurrencies:',occ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.96, S2: 0.09523809523809523, S3: 0.3231301326854853\n"
     ]
    }
   ],
   "source": [
    "#car and bus\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    for syns2 in w2:\n",
    "        similarities.append(syns1.wup_similarity(syns2))\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:1.0, S2: 1.0, S3: 1.0\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hyper1=syns1.hypernyms()[0]\n",
    "    for syns2 in w2:\n",
    "        hyper2=syns2.hypernyms()[0]\n",
    "        similarities.append(hyper1.wup_similarity(hyper2))\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.6666666666666666, S2: 0.6086956521739131, S3: 0.6238785369220163\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hypers1=syns1.hyponyms()\n",
    "    for hyper1 in hypers1:\n",
    "        for syns2 in w2:\n",
    "            hypers2=syns2.hyponyms()\n",
    "            for hyper2 in hypers2:\n",
    "                similarities.append(hyper1.wup_similarity(hyper2))\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_ic = nltk.corpus.wordnet_ic.ic('ic-brown.dat')\n",
    "def jcsim(synset1, synset2):\n",
    "    try:\n",
    "        return synset1.jcn_similarity(synset2, brown_ic)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.34659468740185323, S2: 0.05161364962677664, S3: 0.09387159388812355\n"
     ]
    }
   ],
   "source": [
    "#car and bus\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    for syns2 in w2:\n",
    "        sim=jcsim(syns1, syns2)\n",
    "        if sim:\n",
    "            similarities.append(sim)\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:0.3471951500736978, S2: 0.058690483603655634, S3: 0.10464911313975415\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hyper1=syns1.hypernyms()[0]\n",
    "    for syns2 in w2:\n",
    "        hyper2=syns2.hypernyms()[0]\n",
    "        sim=jcsim(hyper1, hyper2)\n",
    "        if sim:\n",
    "            similarities.append(sim)\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1:1e-300, S2: 5e-301, S3: 7.380952380952381e-301\n"
     ]
    }
   ],
   "source": [
    "#car and bus hypernims\n",
    "w1=wn.synsets('car')\n",
    "w2=wn.synsets('bus')\n",
    "similarities=[]\n",
    "for syns1 in w1:\n",
    "    hypers1=syns1.hyponyms()\n",
    "    for hyper1 in hypers1:\n",
    "        for syns2 in w2:\n",
    "            hypers2=syns2.hyponyms()\n",
    "            for hyper2 in hypers2:\n",
    "                sim=jcsim(hyper1, hyper2)\n",
    "                if sim:\n",
    "                    similarities.append(sim)\n",
    "print(f'S1:{max(similarities)}, S2: {min(similarities)}, S3: {sum(similarities)/len(similarities)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
