{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] is compatible.\n"
     ]
    }
   ],
   "source": [
    "# This notebook requires Python 3.12.3 or higher\n",
    "\n",
    "import sys\n",
    "required_version = (3, 12, 3)\n",
    "if sys.version_info < required_version:\n",
    "    raise Exception(f\"This notebook requires Python {required_version} or higher!\")\n",
    "else:\n",
    "    print(f\"Python version {sys.version} is compatible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv312/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv312/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in ./.venv312/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv312/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv312/lib/python3.12/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in ./.venv312/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv312/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv312/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A- Information Retrieval 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just put all of the class in the A task here because I reuse them\n",
    "\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "class InformationRetrieval:\n",
    "    def __init__(self, abstracts_path: str) -> None:\n",
    "        self.abstracts: list[str] = []\n",
    "        self.load_abstracts(abstracts_path)\n",
    "\n",
    "    def load_abstracts(self, filename: str) -> list[str]:\n",
    "        with open(filename, 'r') as file:\n",
    "            self.abstracts = [line.lower() for line in file.readlines()]\n",
    "        return self.abstracts\n",
    "\n",
    "    # Using regex boundary \\b to matches keyword to satisfy exact matching.\n",
    "    # Because, for example, \"AI\" will matches \"pair\", but that is wrong.\n",
    "    #                       but we cannot split all the words because then multiple words such as \"machine learning\" won't be matched\n",
    "    @staticmethod\n",
    "    def query_match_abstract(query: str, abstract: str) -> bool:\n",
    "        pattern = re.compile(r'\\b' + re.escape(query) + r'\\b')\n",
    "        return bool(pattern.search(abstract))\n",
    "\n",
    "    def _query_match(self, query: str) -> bool:\n",
    "        return any(self.query_match_abstract(query, abstract) for abstract in self.abstracts)\n",
    "\n",
    "    def query_match(self, query: str) -> bool:\n",
    "        return self._query_match(query.lower())\n",
    "\n",
    "    def perform_query_matching(self, queries: list[str]) -> None:\n",
    "        for query in queries:\n",
    "            matches = self.query_match(query)\n",
    "            print(f\"Query: '{query}' - Match in abstracts: {1 if matches else 0}\")\n",
    "\n",
    "\n",
    "class InformationRetrievalIndexed():\n",
    "    def __init__(self, keywords_path: str, abstracts_folder_path: str) -> None:\n",
    "        self.abstracts: list[str] = []\n",
    "        self.keywords: list[list[str]] = []\n",
    "        self.keywords = self.load_keywords(keywords_path)\n",
    "        self.abstracts = self.load_abstracts(abstracts_folder_path)\n",
    "        self.inverted_index: dict[str, list[int]] = {}\n",
    "        self.inverted_index_built = False\n",
    "\n",
    "    def load_keywords(self, filename: str) -> list[list[str]]:\n",
    "        with open(filename, 'r') as file:\n",
    "            self.keywords = [line.strip().lower().split(\", \") for line in file.readlines()]\n",
    "        return self.keywords\n",
    "\n",
    "    def load_abstracts(self, folder_path: str) -> list[str]:\n",
    "        abstracts = []\n",
    "        # List all files in the directory (assuming files are named A1.txt, A2.txt, ..., A20.txt)\n",
    "        files = [f for f in os.listdir(folder_path) if f.startswith('A')]\n",
    "\n",
    "        for file in files:\n",
    "            with open(os.path.join(folder_path, file), 'r') as f:\n",
    "                content = f.read().lower()\n",
    "                abstracts.append(content)\n",
    "        return abstracts\n",
    "\n",
    "    def build_inverted_index(self) -> None:\n",
    "        # Build list of unique keywords, just set comprehension looping through all keywords\n",
    "        set_keywords = {\n",
    "            keyword\n",
    "            for keyword_list in self.keywords\n",
    "            for keyword in keyword_list\n",
    "        }\n",
    "\n",
    "        # Loop through each abstract and check for keywords\n",
    "        self.inverted_index = {keyword: [] for keyword in set_keywords}\n",
    "        for idx, abstract in enumerate(self.abstracts):\n",
    "            for kw in self.inverted_index:\n",
    "                # Reuse the query_match_abstract method because if we just do plain \"if kw in abstract\", it will get bugged: \"AI\" matches \"Pair\"\n",
    "                if InformationRetrieval.query_match_abstract(kw, abstract):\n",
    "                    self.inverted_index[kw].append(idx)\n",
    "\n",
    "        self.inverted_index_built = True\n",
    "\n",
    "    def export_inverted_index(self, filename: str) -> None:\n",
    "        with open(filename, 'w') as f:\n",
    "            for keyword, idxs in self.inverted_index.items():\n",
    "                f.write(f\"{keyword}: {idxs}\\n\")\n",
    "\n",
    "    def import_inverted_index(self, filename: str) -> None:\n",
    "        self.inverted_index = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                keyword, idxs = line.strip().split(': ')\n",
    "                if (idxs == '[]'):\n",
    "                    self.inverted_index[keyword] = []\n",
    "                else:\n",
    "                    self.inverted_index[keyword] = list(map(int, idxs.strip('[]\\n').split(', ')))\n",
    "        self.inverted_index_built = True\n",
    "\n",
    "    def _query_match(self, query: str) -> list[int]:\n",
    "        return self.inverted_index[query] if query in self.inverted_index and bool(self.inverted_index[query]) else []\n",
    "\n",
    "    def query_match(self, query: str) -> list[int]:\n",
    "        if not self.inverted_index_built:\n",
    "            self.build_inverted_index()\n",
    "        return self._query_match(query.lower())\n",
    "\n",
    "    def perform_query_matching(self, queries: list[str]) -> None:\n",
    "        for query in queries:\n",
    "            matches = self.query_match(query)\n",
    "            transformed_matches = [f\"A{match}\" for match in matches]\n",
    "            if matches:\n",
    "                print(f\"Query: '{query}' - Match in abstracts: {transformed_matches}\")\n",
    "            else:\n",
    "                print(f\"Query: '{query}' - No match in abstract\")\n",
    "\n",
    "    def display_inverted_index(self) -> None:\n",
    "        if not self.inverted_index_built:\n",
    "            self.build_inverted_index()\n",
    "        for keyword, idxs in self.inverted_index.items():\n",
    "            print(f\"Keyword: '{keyword}' - Found in files with indexes: {idxs}\")\n",
    "\n",
    "\n",
    "class InformationRetrievalIndexedRelaxed(InformationRetrievalIndexed):\n",
    "    def __init__(self, keywords_path: str, abstracts_folder: str, threshold: float = 0.9) -> None:\n",
    "        super().__init__(keywords_path, abstracts_folder)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def similarity(a: str, b: str) -> float:\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "    def _scan_abstract_for_match(self, query: str, abstract: str) -> tuple[bool, float, str]:\n",
    "        \"\"\"This function scan abstract for matches using word similarity with a sliding window\n",
    "        Note: the sliding window and the matching is done on a word basis, not a string basis\n",
    "\n",
    "        For example, the abstract saying something like \"Machine learning is a subset of AI\", then a sliding window\n",
    "        of 2 will be \"Machine learning\" and \"learning is\", \"is a\", \"a subset\", \"subset of\", \"of AI\"\n",
    "\n",
    "        The length of the window is determined by the number of words in the query.\n",
    "\n",
    "        Args:\n",
    "            query (str): search query\n",
    "            abstract (str): abstract to scan\n",
    "\n",
    "        Returns:\n",
    "            tuple[bool, float, str]: (if matched, similarity percentage, word that matched)\n",
    "        \"\"\"\n",
    "        query_length = len(query.split())  # Count words in the query\n",
    "\n",
    "        # Tokenize the abstract and scan with a sliding window approach\n",
    "        abstract_tokens = abstract.split()\n",
    "        for i in range(len(abstract_tokens) - query_length + 1):\n",
    "            # Get the substring of abstract that matches the length of the query (in words)\n",
    "            substring = ' '.join(abstract_tokens[i:i + query_length])\n",
    "\n",
    "            # Compare similarity between the query and this substring\n",
    "            similarity = self.similarity(query, substring)\n",
    "            if similarity >= self.threshold:\n",
    "                return (True, similarity, substring)\n",
    "\n",
    "        return (False, 0.0, '')\n",
    "\n",
    "    def _query_match(self, query: str) -> list[tuple[int, float]]:\n",
    "        query_lower = query.lower()\n",
    "        matches = []\n",
    "        for idx, abstract in enumerate(self.abstracts):\n",
    "            matched, score, word = self._scan_abstract_for_match(query_lower, abstract)\n",
    "            if matched:\n",
    "                matches.append((idx, score, word))\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def perform_query_matching(self, queries: list[str]) -> None:\n",
    "        for query in queries:\n",
    "            matches = self.query_match(query)\n",
    "            transformed_matches = [f\"{word} in A{match} at {score * 100:.2f}%\" for match, score, word in matches]\n",
    "            if matches:\n",
    "                print(f\"Query: '{query}' - Match {transformed_matches}\")\n",
    "            else:\n",
    "                print(f\"Query: '{query}' - No match in abstract\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C Information Retrieval\n",
    "\n",
    "## Task C-1: Consider an academic journal of your own choice and collect 30 abstracts using a method of your own (can be a simple manual copy-and-paste operation or webcrawling) in a single file using the same query term (E.g., Go to webscience or sciencedirect or Springer and input a query T of your choice). Save the first 30 results as documents, where each document has four metadata: Title, List of Authors, Abstract text, List of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total metadata: 30\n",
      "[{'abstract': 'Machine learning (ML) has drawn tremendous interest for its capacity to extract useful information that may be overlooked with conventional analysis techniques and for its versatility in a wide range of research domains, including biomedical sensing and imaging. In this perspective, we provide an overview focused on the uses and benefits of ML in areas of plasmonics in biology. ML methodologies for processing data from plasmonic biosensing and imaging systems by supervised and unsupervised learning to achieve enhanced detection and quantification of target analytes are described. In addition, deep learning-based approaches to improve the design of plasmonic structures are presented. Data analysis based on ML for classification, regression, and clustering by dimension reduction is presented. We also discuss ML-based prediction and design of plasmonic structures and sensors using discriminative and generative models. Challenges and the outlook for ML for plasmonics in biology are summarized. Based on these insights, we are convinced that ML can add value to plasmonics techniques in biological sensing and imaging applications to make them more powerful with improved detection and resolution.', 'keyword': 'machine learning, plasmonics, data analysis, structure design, deep learning, biosensors, imaging', 'title': 'Machine learning and its applications for plasmonics in biology', 'authors': 'Gwiyeong Moon, Jongha Lee, Hyunwoong Lee, Hajun Yoo, Kwanhwi Ko, Seongmin Im, Donghyun Kim'}, {'abstract': 'Background and objective Mechanistic-based Model simulations (MM) are an effective approach commonly employed, for research and learning purposes, to better investigate and understand the inherent behavior of biological systems. Recent advancements in modern technologies and the large availability of omics data allowed the application of Machine Learning (ML) techniques to different research fields, including systems biology. However, the availability of information regarding the analyzed biological context, sufficient experimental data, as well as the degree of computational complexity, represent some of the issues that both MMs and ML techniques could present individually. For this reason, recently, several studies suggest overcoming or significantly reducing these drawbacks by combining the above-mentioned two methods. In the wake of the growing interest in this hybrid analysis approach, with the present review, we want to systematically investigate the studies available in the scientific literature in which both MMs and ML have been combined to explain biological processes at genomics, proteomics, and metabolomics levels, or the behavior of entire cellular populations. Methods Elsevier Scopus®, Clarivate Web of Science™ and National Library of Medicine PubMed® databases were enquired using the queries reported in Table 1, resulting in 350 scientific articles. Results Only 14 of the 350 documents returned by the comprehensive search conducted on the three major online databases met our search criteria, i.e. present a hybrid approach consisting of the synergistic combination of MMs and ML to treat a particular aspect of systems biology. Conclusions Despite the recent interest in this methodology, from a careful analysis of the selected papers, it emerged how examples of integration between MMs and ML are already present in systems biology, highlighting the great potential of this hybrid approach to both at micro and macro biological scales.', 'keyword': 'Mathematical modeling, Machine learning, Reinforcement learning, Systems biology, Simulation, Systematic literature review', 'title': 'Combined mechanistic modeling and machine-learning approaches in systems biology - A systematic literature review', 'authors': 'Anna Procopio, Giuseppe Cesarelli, Leandro Donisi, Alessio Merola, Francesco Amato, Carlo Cosentino'}, {'abstract': 'This bibliometric research explores the global evolution of machine learning applications in medical and healthcare research for 3 decades (1994 to 2023). The study applies data mining techniques to a comprehensive dataset of published articles related to machine learning applications in the medical and healthcare sectors. The data extraction process includes the retrieval of relevant information from the source sources such as journals, books, and conference proceedings. An analysis of the extracted data is then conducted to identify the trends in the machine learning applications in medical and healthcare research. The Results revealed the publications published and indexed in the Scopus and PubMed database over the last 30 years. Bibliometric Analysis revealed that funding played a more significant role in publication productivity compared to collaboration (co-authorships), particularly at the country level. Hotspots analysis revealed three core research themes on MLHC research hence demonstrating the importance of machine learning applications to medical and healthcare research. Further, the study showed that the MLHC research landscape has largely focused on ML applications to tackle various issues ranging from chronic medical challenges (e.g., cardiological diseases) to patient data security. The findings of this research may be useful to policy makers and practitioners in the medical and healthcare sectors and to global research endeavours in the field. Future studies could include addressing issues such as growing ethical considerations, integration, and practical applications in wearable technology, IoT, and smart healthcare systems.', 'keyword': 'Machine Learning, Healthcare analytics, Artificial Intelligence, Medical research, IoT, Algorithms, Bibliometric Analysis', 'title': 'Evolution of Machine Learning Applications in Medical and Healthcare Analytics Research: A Bibliometric Analysis', 'authors': 'Samuel-Soma M. Ajibade, Gloria Nnadwa Alhassan, Abdelhamid Zaidi, Olukayode Ayodele Oki, Joseph Bamidele Awotunde, Emeka Ogbuju, Kayode A. Akintoye'}, {'abstract': 'Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures from the Cahn-Hilliard model. We analyze the accuracy and robustness of all models and elucidate the reasons for the differences in their performances. The impact of including domain knowledge through tailored features is studied, and general recommendations based on the availability and quality of training data are derived from this.', 'keyword': 'Structure-properties relation, Forward model, Feature engineering, Power spectrum density, Convolutional neural network, Support vector regression, Ising model, Cahn-Hilliard model', 'title': 'Efficient surrogate models for materials science simulations: Machine learning-based prediction of microstructure properties', 'authors': 'Binh Duong Nguyen, Pavlo Potapenko, Aytekin Demirci, Kishan Govind, Sébastien Bompas, Stefan Sandfeld'}, {'abstract': 'Drug discovery and development is a time-consuming process that involves identifying, designing, and testing new drugs to address critical medical needs. In recent years, machine learning (ML) has played a vital role in technological advancements and has shown promising results in various drug discovery and development stages. ML can be categorized into supervised, unsupervised, semi-supervised, and reinforcement learning. Supervised learning is the most used category, helping organizations solve several real-world problems. This study presents a comprehensive survey of supervised learning algorithms in drug design and development, focusing on their learning process and succinct mathematical formulations, which are lacking in the literature. Additionally, the study discusses widely encountered challenges in applying supervised learning for drug discovery and potential solutions. This study will be beneficial to researchers and practitioners in the pharmaceutical industry as it provides a simplified yet comprehensive review of the main concepts, algorithms, challenges, and prospects in supervised learning.', 'keyword': 'Artificial intelligence, Deep learning, Machine learning, Neural network, Supervised learning', 'title': 'Supervised machine learning in drug discovery and development: Algorithms, applications, challenges, and prospects', 'authors': 'George Obaido, Ibomoiye Domor Mienye, Oluwaseun F. Egbelowo, Ikiomoye Douglas Emmanuel, Adeola Ogunleye, Blessing Ogbuokiri, Pere Mienye, Kehinde Aruleba'}, {'abstract': 'Additive manufacturing (AM) has undergone significant development over the past decades, resulting in vast amounts of data that carry valuable information. Numerous research studies have been conducted to extract insights from AM data and utilize it for optimizing various aspects such as the manufacturing process, supply chain, and real-time monitoring. Data integration into proposed digital twin frameworks and the application of machine learning techniques is expected to play pivotal roles in advancing AM in the future. In this paper, we provide an overview of machine learning and digital twin-assisted AM. On one hand, we discuss the research domain and highlight the machine-learning methods utilized in this field, including material analysis, design optimization, process parameter optimization, defect detection and monitoring, and sustainability. On the other hand, we examine the status of digital twin-assisted AM from the current research status to the technical approach and offer insights into future developments and perspectives in this area. This review paper aims to examine present research and development in the convergence of big data, machine learning, and digital twin-assisted AM. Although there are numerous review papers on machine learning for additive manufacturing and others on digital twins for AM, no existing paper has considered how these concepts are intrinsically connected and interrelated. Our paper is the first to integrate the three concepts big data, machine learning, and digital twins and propose a cohesive framework for how they can work together to improve the efficiency, accuracy, and sustainability of AM processes. By exploring latest advancements and applications within these domains, our objective is to emphasize the potential advantages and future possibilities associated with integration of these technologies in AM.', 'keyword': 'Additive manufacturing, Big data, Machine learning, Digital twin, Data-driven', 'title': 'Big data, machine learning, and digital twin assisted additive manufacturing: A review', 'authors': 'Liuchao Jin, Xiaoya Zhai, Kang Wang, Kang Zhang, Dazhong Wu, Aamer Nazir, Jingchao Jiang, Wei-Hsin Liao'}, {'abstract': \"Microfluidic devices are increasingly widespread in the literature, being applied to numerous exciting applications, from chemical research to Point-of-Care devices, passing through drug development and clinical scenarios. Setting up these microenvironments, however, introduces the necessity of locally controlling the variables involved in the phenomena under investigation. For this reason, the literature has deeply explored the possibility of introducing sensing elements to investigate the physical quantities and the biochemical concentration inside microfluidic devices. Biosensors, particularly, are well known for their high accuracy, selectivity, and responsiveness. However, their signals could be challenging to interpret and must be carefully analysed to carry out the correct information. In addition, proper data analysis has been demonstrated even to increase biosensors' mentioned qualities. To this regard, machine learning algorithms are undoubtedly among the most suitable approaches to undertake this job, automatically learning from data and highlighting biosensor signals' characteristics at best. Interestingly, it was also demonstrated to benefit microfluidic devices themselves, in a new paradigm that the literature is starting to name “intelligent microfluidics”, ideally closing this benefic interaction among these disciplines. This review aims to demonstrate the advantages of the triad paradigm microfluidics-biosensors-machine learning, which is still little used but has a great perspective. After briefly describing the single entities, the different sections will demonstrate the benefits of the dual interactions, highlighting the applications where the reviewed triad paradigm was employed.\", 'keyword': 'Machine learning, Lab-on-a-Chip, Biosensing system, Intelligent microfluidics, Biosensors integration', 'title': 'Integrating machine learning and biosensors in microfluidic devices: A review', 'authors': \"Gianni Antonelli, Joanna Filippi, Michele D'Orazio, Giorgia Curci, Paola Casti, Arianna Mencattini, Eugenio Martinelli\"}, {'abstract': 'Carbon dots (CDs) have been a subject of great interest among researchers due to their diverse physicochemical properties and numerous advantageous attributes such as good biocompatibility, unique optical properties, low cost, eco-friendliness, abundant functional groups (e.g., amino, hydroxyl, and carboxyl) high stability, and excellent electron mobility. With the rapid advancement of data-driven technologies, machine learning (ML) has gained significant attention as a primary and indispensable tool in different applications in numerous research fields, including the monitoring of chemical reactions. By utilizing machine learning algorithms, the properties of carbon dots can be enhanced, such as fluorescence, stability, and electrocatalytic activity, as well as optimizing the synthesis process. Moreover, machine learning can be utilized to screen carbon dot precursors and predict their properties, providing various advantages in developing carbon dots with superior properties. As a result, machine learning offers numerous benefits in carbon dots synthesis, which has the potential to impact various fields. Photoelectrochemical sensors are a type of chemical sensor that use light to generate a photocurrent, which is then used to detect the presence of a target analyte. These sensors have gained significant attention due to their high sensitivity, selectivity, and low cost, making them a promising tool for a variety of applications in fields such as environmental monitoring and biomedical sensing. Due to their fascinating electronic and photonic properties, CQDs have gained considerable attention in the development of photoelectrochemical sensors. This review article provides an overview of recent advancements in the machine learning synthesis of CQDs and their applications in constructing photoelectrochemical sensors.', 'keyword': 'Carbon quantum dot, Machine learning, Synthesis, Photoelectrochemical sensors', 'title': 'Machine learning-driven approaches for synthesizing carbon dots and their applications in photoelectrochemical sensors', 'authors': 'Roya Mohammadzadeh kakhki, Mojtaba Mohammadpoor'}, {'abstract': 'Lithium-ion batteries play a pivotal role in a wide range of applications, from electronic devices to large-scale electrified transportation systems and grid-scale energy storage. Nevertheless, they are vulnerable to both progressive aging and unexpected failures, which can result in catastrophic events such as explosions or fires. Given their expanding global presence, the safety of these batteries and potential hazards from serious malfunctions are now major public concerns. Over the past decade, scholars and industry experts are intensively exploring methods to monitor battery safety, spanning from materials to cell, pack and system levels and across various spectral, spatial, and temporal scopes. In this Review, we start by summarizing the mechanisms and nature of battery failures. Following this, we explore the intricacies in predicting battery system evolution and delve into the specialized knowledge essential for data-driven, machine learning models. We offer an exhaustive review spotlighting the latest strides in battery fault diagnosis and failure prognosis via an array of machine learning approaches. Our discussion encompasses: (1) supervised and reinforcement learning integrated with battery models, apt for predicting faults/failures and probing into failure causes and safety protocols at the cell level; (2) unsupervised, semi-supervised, and self-supervised learning, advantageous for harnessing vast data sets from battery modules/packs; (3) few-shot learning tailored for gleaning insights from scarce examples, alongside physics-informed machine learning to bolster model generalization and optimize training in data-scarce settings. We conclude by casting light on the prospective horizons of comprehensive, real-world battery prognostics and management.', 'keyword': 'Lithium-ion batteries, Safety, Machine learning, Deep learning, Fault, Failure, Thermal runaway, Detection, Prediction', 'title': 'Battery safety: Machine learning-based prognostics', 'authors': 'Jingyuan Zhao, Xuning Feng, Quanquan Pang, Michael Fowler, Yubo Lian, Minggao Ouyang, Andrew F. Burke'}, {'abstract': 'Allosteric regulation is a fundamental biological mechanism that can control critical cellular processes via allosteric modulator binding to protein distal functional sites. The advantages of allosteric modulators over orthosteric ones have sparked the development of numerous computational approaches, such as the identification of allosteric binding sites, to facilitate allosteric drug discovery. Building on the success of machine learning (ML) models for solving complex problems in biology and chemistry, several ML models for predicting allosteric sites have been developed. In this review, we provide an overview of these models and discuss future perspectives powered by the field of artificial intelligence such as protein language models.', 'keyword': 'Allostery, Machine learning, Drug design, Protein binding sites', 'title': 'Machine learning approaches in predicting allosteric sites', 'authors': 'Francho Nerín-Fonz, Zoe Cournia'}, {'abstract': 'Machine learning (ML) is a range of powerful computational algorithms capable of generating predictive models via intelligent autonomous analysis of relatively large and often unstructured data. ML has become an integral part of our daily lives with a plethora of applications, including web, business, automotive industry, clinical diagnostics, scientific research, and more recently, forensic science. In the field of forensic DNA, the manual analysis of complex data can be challenging, time-consuming, and error-prone. The integration of novel ML-based methods may aid in streamlining this process while maintaining the high accuracy and reproducibility required for forensic tools. Due to the relative novelty of such applications, the forensic community is largely unaware of ML capabilities and limitations. Furthermore, computer science and ML professionals are often unfamiliar with the forensic science field and its specific requirements. This manuscript offers a brief introduction to the capabilities of machine learning methods and their applications in the context of forensic DNA analysis and offers a critical review of the current literature in this rapidly developing field.', 'keyword': 'ANN, artificial neural networks, AT, analytical threshold, BN, Bayesian networks, CART, classification and regression trees, CE, capillary electrophoresis, CNN, convolutional neural network, DBSCAN, density-based spatial clustering of applications with noise, DeT, Decision Tree, DL, deep learning, DT, dynamic threshold, EPG, electropherogram, GAN, generative adversarial networks, GDA, generalised discriminant analysis, HID, human identification, k-NN, k-nearest neighbours, LDA, linear discriminant analysis, LR, likelihood ratio, MCMC, Markov Chain Monte Carlo, MAC, maximum allele count, MCA, multiple correspondence analysis, ML, machine learning, MLE, maximum likelihood estimation, MLP, multilayer perception, MLR, multinomial logistic regression, MPS, massively parallel sequencing, NB, Naive Bayes, NGS, Next Generation Sequencing, NoC, number of contributors, NT, no threshold, PCA, principal component analysis, PCoA, principal coordinates analysis, PG, probabilistic genotyping, PGS, probabilistic genotyping software, RF, random forest, SNP, single nucleotide polymorphism, ST, stochastic threshold, STR, short tandem repeat, SVM, support vector machine, TAC, total allele count, t-SNE, t-distributed stochastic neighbour embedding, Machine learning, Forensic DNA profiling, Human identification, AI, STRs', 'title': 'Machine learning applications in forensic DNA profiling: A critical review', 'authors': 'Mark Barash, Dennis McNevin, Vladimir Fedorenko, Pavel Giverts'}, {'abstract': 'Recent advancements in immune sequencing and experimental techniques are generating extensive T cell receptor (TCR) repertoire data, enabling the development of models to predict TCR binding specificity. Despite the computational challenges posed by the vast diversity of TCRs and epitopes, significant progress has been made. This review explores the evolution of computational models designed for this task, emphasizing machine learning efforts, including early unsupervised clustering approaches, supervised models, and recent applications of Protein Language Models (PLMs), deep learning models pretrained on extensive collections of unlabeled protein sequences that capture crucial biological properties. We survey the most prominent models in each category and offer a critical discussion on recurrent challenges, including the lack of generalization to new epitopes, dataset biases, and shortcomings in model validation designs. Focusing on PLMs, we discuss the transformative impact of Transformer-based protein models in bioinformatics, particularly in TCR specificity analysis. We discuss recent studies that exploit PLMs to deliver notably competitive performances in TCR-related tasks, while also examining current limitations and future directions. Lastly, we address the pressing need for improved interpretability in these often opaque models, and examine current efforts to extract biological insights from large black box models.', 'keyword': 'Machine learning, T cell receptor, Specificity prediction, Protein language models, Interpretability', 'title': 'T-cell receptor binding prediction: A machine learning revolution', 'authors': 'Anna Weber, Aurélien Pélissier, María Rodríguez Martínez'}, {'abstract': \"Machine learning models used for energy conversion system optimization cannot extrapolate outside the bounds of training data and often produce physically unrealistic results when making predictions in regions of sparse training data. The toy model concept introduced in this work allows machine learning models to extrapolate to some extent and also reduces the possibility of physically unrealistic results. It uses physics to shrink the model input (feature space) of data-based models, so that extrapolations in the data-based feature space tend to become interpolations in the physics-based (toy variable) feature space. The physics-based model can be any model or experiment that can shrink the feature space without affecting interpolation and is termed a ‘toy model' because it does not need to be accurate or make predictions of interest. The concept has been applied to model experimental data obtained from three complex systems: a. Aerodynamic forces on a spinning and vibrating baseball with inclined axis of rotation (toy model: CFD model), b. Hydraulic turbine efficiency (toy model: PIV images of flow through stationary blades), and c. Combustion generated engine emissions (toy model: system-level 1-D model). All extrapolations were converted into interpolations for the first two systems while a 75% conversion was achieved for the emission predictions. The engine toy model produced 736,281 possible feature spaces from which one unique feature space was chosen for every prediction based on agreement between different machine learning algorithms. It is shown that the ability of the toy variables to reorganize the data is important, while their accuracy is relatively unimportant. The toy model concept was demonstrated to work with neural networks and regression, and can be used to increase model robustness or reduce training data requirements.\", 'keyword': 'Physics-Based Machine Learning, Extrapolation, Feature Selection, Feature Engineering, Engines, Turbines', 'title': 'Using physics to extend the range of machine learning models for an aerodynamic, hydraulic and combusting system: The toy model concept', 'authors': 'Indranil Brahma, Robert Jennings, Bradley Freid'}, {'abstract': 'Following other fields of science, Deep Learning models are gaining attention within the statistical physics community as a powerful and efficient way for analysing experimental and synthetic time series, and for quantifying properties thereof. Applying such models is nevertheless a path full of pitfalls, not only due to their inherent complexity, but also to a lack of understanding of some of their idiosyncrasies. We here discuss some of these pitfalls in the context of time series classification, covering from the selection of the best model hyperparameters, how the models have to be trained, to the way data have to be pre-processed. While not providing one-fits-all answers, the statistical physics practitioner will here find what questions ought to be posed, and a first guide about how to tackle them.', 'keyword': 'Deep Learning, Chaos, Classification', 'title': 'Deep Learning models for the analysis of time series: A practical introduction for the statistical physics practitioner', 'authors': 'Alfredo Crespo-Otero, Pau Esteve, Massimiliano Zanin'}, {'abstract': 'The optimization of the electrode manufacturing process is important for upscaling the application of Lithium-Ion Batteries (LIBs) to cater for growing energy demand. LIB manufacturing is important to be optimized because it determines the practical performance of the cells when the latter are being used in applications such as electric vehicles. In this study, we tackled the issue of high-performance electrodes for desired battery applications by proposing a data-driven approach supported by a deterministic machine learning-assisted pipeline for bi-objective optimization of the electrochemical performances. This pipeline allows the inverse design of the process parameters to adopt to manufacture electrodes for energy or power applications. This work is an analogy to our previous work that addressed the optimization of the electrode microstructures for kinetic, ionic, and electronic transport properties improvement. An electrochemical model is fed with the electrode properties characterizing the electrode microstructures generated by manufacturing simulations, and used to simulate the electrochemical performances. Secondly, the resulting dataset was used to train a deterministic model to implement fast optimizations to identify optimal electrodes. Our results suggested a high amount of active material, combined with intermediate values of solid content in the slurry and calendering degree, to achieve the optimal electrodes.', 'keyword': 'Battery cell manufacturing, Bayesian optimization, Machine learning, Electrode, Numerical simulation', 'title': 'Toward high-performance energy and power battery cells with machine learning-based optimization of electrode manufacturing', 'authors': 'Marc Duquesnoy, Chaoyue Liu, Vishank Kumar, Elixabete Ayerbe, Alejandro A. Franco'}, {'abstract': 'Owing to the hexagonal close-packed (HCP) crystal structure inherent in Mg alloys, strong basal texture can readily be induced through the processes of rolling or extrusion. The anisotropy of the texture of Mg alloys impacts their stamping and forming capabilities, limiting their use in certain applications. Microalloying and shear deformation are currently the most common methods of weakening the texture of Mg alloys. Many shearing processes have been extensively studied, and given that they require complex equipment and make it difficult to achieve mass production, major attention has turned to studying the design of microalloys. Traditional trial-and-error approaches for developing micro-alloying confront many challenges, including longer test cycles and increasing expenses. The rapid advancement of big data and artificial intelligence opens up a new channel for the efficient advancement of metallic materials, specifically the application of machine learning to aid in the design of Mg alloys. ML modeling can be used to find correlations between features and attributes in data, allowing for the development and design of high-performance Mg alloys. The article provides an extensive overview of machine learning applications in Mg alloys. These include the discovery of high-performance alloys, the selection of coating designs, the design of Mg matrix composites, the prediction of second phases, the microstructure modification, optimization of rolling or extrusion parameters, and the prediction of mechanical and corrosion properties. In conclusion, challenges and prospects for the rational design of alloys with machine learning support were discussed.', 'keyword': 'Mg alloy, Machine learning, Strength, Plasticity, Microalloying', 'title': 'A brief review of machine learning-assisted Mg alloy design, processing, and property predictions', 'authors': 'Yanhui Cheng, Lifei Wang, Chaoyang Yang, Yunli Bai, Hongxia Wang, Weili Cheng, Hanuma Reddy Tiyyagura, Alexander Komissarov, Kwang Seon Shin'}, {'abstract': 'This review aims to highlight the role that computational chemistry has played in advancing the supramolecular chemistry field. We demonstrated recent uses of computational methodologies to elucidate noncovalent interactions in various processes occurring in supramolecular systems. We also emphasized the contributions of these techniques to studying reactions within confined space, showing how computational methodologies help clarify the effects of reactivity and conformational locking. Furthermore, we underscore the utilization of Molecular Dynamics (MD) in elucidating dynamical processes, understanding temperature and pressure effects, and exploring conformational space within supramolecular chemistry. Finally, we highlight the impact that the age of machine learning has on computational chemistry, showing how these universal approximators can enhance existing methods, predict properties, and efficiently explore the chemical space encompassed by these complex systems. This article explores the use of electronic structure methods to understand noncovalent interactions in supramolecular systems and the chemical reactions that occur within their chemical space. It also highlights the use of molecular dynamics and machine learning techniques in supramolecular chemistry.', 'keyword': 'computational chemistry, machine learning, supramolecular chemistry, noncovalent interactions, molecular dynamics, density functional theory, electronic structure calculations', 'title': 'Supramolecular Chemistry: Exploring the Use of Electronic Structure, Molecular Dynamics, and Machine Learning Approaches', 'authors': 'Matheus C. Colaço, Vinícius A. Glitz, Amanda K. Jacobs, Vinícius C. Port, Giovanni F. Caramori'}, {'abstract': 'For decades, drug delivery scientists have been performing trial-and-error experimentation to manually sample parameter spaces and optimize release profiles through rational design. To enable this approach, scientists spend much of their career learning nuanced drug-material interactions that drive system behavior. In relatively simple systems, rational design criteria allow us to fine tune release profiles and enable efficacious therapies. However, as materials and drugs become increasingly sophisticated and their interactions have non-linear and compounding effects, the field is suffering the Curse of Dimensionality which prevents us from comprehending complex structure-function relationships. In the past, we have embraced this complexity by implementing high-throughput screens to increase the probability of finding ideal compositions. However, this brute force method was inefficient and led many to abandon these fishing expeditions. Fortunately, methods in data science including artificial intelligence / machine learning (AI/ML) are providing ideal analytical tools to model this complex data and ascertain quantitative structure-function relationships. In this Oration, I speak to the potential value of data science in drug delivery with particular focus on polymeric delivery systems. Here, I do not suggest that AI/ML will simply replace mechanistic understanding of complex systems. Rather, I propose that AI/ML should be yet another useful tool in the lab to navigate complex parameter spaces. The recent hype around AI/ML is breathtaking and potentially over inflated, but the value of these methods is poised to revolutionize how we perform science. Therefore, I encourage readers to consider adopting these skills and applying data science methods to their own problems. If done successfully, I believe we will all realize a paradigm shift in our approach to drug delivery.', 'keyword': 'Machine learning, Artificial intelligence, Drug delivery, Controlled release, Formulation, Encapsulation', 'title': 'Machine learning in drug delivery', 'authors': 'Adam J. Gormley'}, {'abstract': 'Machine learning (ML) has been rapidly transforming the landscape of natural sciences and has the potential to revolutionize the process of data analysis and hypothesis formulation as well as expand scientific knowledge. ML has been particularly instrumental in the advancement of cheminformatics and materials science, including membrane technology. In this review, we analyze the current state-of-the-art membrane-related ML applications from ML and membrane perspectives. We first discuss the ML foundations of different algorithms and design choices. Then, traditional and deep learning methods, including application examples from the membrane literature, are reported. We also discuss the importance of learning data and both molecular and membrane-system featurization. Moreover, we follow up on the discussion with examples of ML applications in membrane science and technology. We detail the literature using data-driven methods from property prediction to membrane fabrication. Various fields are also discussed, such as reverse osmosis, gas separation, and nanofiltration. We also differentiate between downstream predictive tasks and generative membrane design. Additionally, we formulate best practices and the minimum requirements for reporting reproducible ML studies in the field of membranes. This is the first systematic and comprehensive review of ML in membrane science.', 'keyword': 'Deep learning, Predictive models, Generative models, Molecular modeling, Cheminformatics', 'title': 'Machine learning for the advancement of membrane science and technology: A critical review', 'authors': 'Gergo Ignacz, Lana Bader, Aron K. Beke, Yasir Ghunaim, Tejus Shastry, Hakkim Vovusha, Matthew R. Carbone, Bernard Ghanem, Gyorgy Szekely'}, {'abstract': 'Not long ago, carbon quantum dots (CQDs) came into view as a revolutionary class of materials, propelling advancements in water remediation and electrochemical technology. This comprehensive review explores the cutting-edge developments in CQDs-based materials and their applications, addressing critical challenges in water treatment and electrochemical processes. Synthesized as ultra-tiny, dispersed particles with dimensions less than 10 nm, CQDs exhibit remarkable optical properties, including adjustable fluorescence emission across various colors. With a surge in published scientific articles, CQDs have garnered significant attention, offering potential solutions in heavy metal sensing, remediation, and electrocatalytic hydrogen evolution reactions (HER). The review highlights the high sensitivity of CQDs as fluorescent sensors, detecting contaminants in water with limits of detection down to femtomolar concentrations. Moreover, CQDs demonstrate excellent adsorptive capabilities for heavy metal removal, surpassing traditional adsorbents in terms of removal efficiency. Furthermore, CQDs serve as promising electrocatalysts, enhancing reaction kinetics and enabling efficient water splitting for clean energy generation. Furthermore, this review emphasizes the importance of machine learning in advancing CQDs-based materials, supported by case studies and examples that illustrate how machine learning techniques optimize CQDs synthesis, enhance their properties, and broaden their applications. However, challenges remain in the precise synthesis of CQDs, scalability of production processes, and understanding the interactions between CQDs and pollutants. Overcoming these challenges will unlock the full potential of CQDs-based materials, leading to sustainable and efficient solutions in water control and electrochemical processes.', 'keyword': 'Carbon quantum dots, Water remediation, Electrochemical advancements, Heavy metal sensing, Fluorescent sensors, Clean energy generation', 'title': 'The interface of machine learning and carbon quantum dots: From coordinated innovative synthesis to practical application in water control and electrochemistry', 'authors': \"Marwa El-Azazy, Ahmed I. Osman, Mahmoud Nasr, Yassmin Ibrahim, Nessreen Al-Hashimi, Khalid Al-Saad, Mohammad A. Al-Ghouti, Mohamed F. Shibl, Ala'a H. Al-Muhtaseb, David W. Rooney, Ahmed S. El-Shafie\"}, {'abstract': 'In Model Predictive Control (MPC) closed-loop performance heavily depends on the quality of the underlying prediction model, where such a model must be accurate and yet simple. A key feature in modern MPC applications is the potential for online model adaptation to cope with time-varying changes, part-to-part variations, and complex features of the system dynamics not caught by models derived from first principles. In this paper, we propose to use a physics-informed, or gray-box, model that extends the physics-based model with a data-driven component, namely a Recurrent Neural Network (RNN). Relying on physics-informed models allows for a rather limited size of the RNN, thereby enhancing online applicability compared to pure black-box models. This work presents a method based on Moving Horizon Estimation (MHE) for simultaneous state estimation and learning of the RNN sub-model, a potentially challenging issue due to limited information available in noisy input-output data and lack of knowledge of the internal state of the RNN. We provide a case study on a quadruple tank benchmark showing how the method can cope with part-to-part variations.', 'keyword': 'Learning-based MPC, Nonlinear MPC, Moving Horizon Estimation, Physics-informed learning, Adaptive MPC, Recurrent Neural Network, Gated Recurrent Unit', 'title': 'Physics-Informed Online Learning by Moving Horizon Estimation: Learning Recurrent Neural Networks in Gray-box Models', 'authors': 'Kristoffer Fink Løwenstein, Daniele Bernardini, Alberto Bemporad, Lorenzo Fagiano'}, {'abstract': 'Chronic lymphocytic leukemia (CLL) is a B cell neoplasm characterized by the accumulation of aberrant monoclonal B lymphocytes. CLL is the predominant type of leukemia in Western countries, accounting for 25% of cases. Although many patients remain asymptomatic, a subset may exhibit typical lymphoma symptoms, acquired immunodeficiency disorders, or autoimmune complications. Diagnosis involves blood tests showing increased lymphocytes and further examination using peripheral blood smear and flow cytometry to confirm the disease. With the significant advancements in machine learning (ML) and artificial intelligence (AI) in recent years, numerous models and algorithms have been proposed to support the diagnosis and classification of CLL. In this review, we discuss the benefits and drawbacks of recent applications of ML algorithms in the diagnosis and evaluation of patients diagnosed with CLL.', 'keyword': 'Artificial intelligence, Chronic lymphocytic leukemia, Diagnosis, Machine learning', 'title': 'Revolutionizing chronic lymphocytic leukemia diagnosis: A deep dive into the diverse applications of machine learning', 'authors': 'Mohamed Elhadary, Amgad Mohamed Elshoeibi, Ahmed Badr, Basel Elsayed, Omar Metwally, Ahmed Mohamed Elshoeibi, Mervat Mattar, Khalil Alfarsi, Salem AlShammari, Awni Alshurafa, Mohamed Yassin'}, {'abstract': 'As a follow-up to our recent Communication in the Journal of Chemical Physics [J. Chem. Phys. 159 071101 (2023)], we report and make available the Jupyter Notebook software here. This software performs binary machine learning classification (MLC) with the goal of learning negligible Hamiltonian matrix elements for vibrational dynamics. We illustrate its usefulness for a Hamiltonian matrix for H2O by using three MLC algorithms: Random Forest, Support Vector Machine, and Multi-layer Perceptron.', 'keyword': 'Machine Learning Classification, Vibrational Configuration Interaction', 'title': 'Machine learning software to learn negligible elements of the Hamiltonian matrix', 'authors': 'Chen Qu, Paul L. Houston, Qi Yu, Priyanka Pandey, Riccardo Conte, Apurba Nandi, Joel M. Bowman'}, {'abstract': 'While machine learning (ML) has made significant contributions to the biopharmaceutical field, its applications are still in the early stages in terms of providing direct support for quality-by-design based development and manufacturing of biologics, hindering the enormous potential for bioprocesses automation from their development to manufacturing. However, the adoption of ML-based models instead of conventional multivariate data analysis methods is significantly increasing due to the accumulation of large-scale production data. This trend is primarily driven by the real-time monitoring of process variables and quality attributes of biopharmaceutical products through the implementation of advanced process analytical technologies. Given the complexity and multidimensionality of a bioproduct design, bioprocess development, and product manufacturing data, ML-based approaches are increasingly being employed to achieve accurate, flexible, and high-performing predictive models to address the problems of analytics, monitoring, and control within the biopharma field. This paper aims to provide a comprehensive review of the current applications of ML solutions in the design, monitoring, control, and optimisation of upstream, downstream, and product formulation processes of monoclonal antibodies. Finally, this paper thoroughly discusses the main challenges related to the bioprocesses themselves, process data, and the use of machine learning models in monoclonal antibody process development and manufacturing. Moreover, it offers further insights into the adoption of innovative machine learning methods and novel trends in the development of new digital biopharma solutions.', 'keyword': 'Biopharmaceuticals, Machine learning, Upstream, Downstream, Bioprocesses, Digital twin, Soft sensors', 'title': 'Applications of machine learning in antibody discovery, process development, manufacturing and formulation: Current trends, challenges, and opportunities', 'authors': 'Thanh Tung Khuat, Robert Bassett, Ellen Otte, Alistair Grevis-James, Bogdan Gabrys'}, {'abstract': \"As the world's supply chains become disrupted through geopolitical instability and the race towards a net-zero future, policies have been implemented to improve the security of certain minerals and raw materials critical to a country's survival and sustainability goals. Circular economies (CE) are sought to be an ecosystem that will reduce virgin material consumption rates, lower carbon emissions, and decelerate the rate of landfilling. However, cost-effective and commercially attractive substitutes to conventional products are needed for this to be realised. Machine learning (ML) and the explosion of interest in artificial intelligence (AI) have led to growing interests in predictive and generative applications for sustainability. Phosphorous and, nutrients overall, operate on finite reserves essential for food supply chains; while such nutrients are largely present in municipal wastewater streams. Wastewater treatment plants (WWTPs) must then face a transformational force to become nutrient recovery centres, rather than follow a linear treat-for-disposal model. In this framework paper, ML is positioned as an enabler for scaled, cost-effective and safer recovery of nutrients and other valuable products — tying in economic, societal, technical and commercial factors through open data connectivity. Moreover, the paper issues a policy guide for institutions wishing to advance food, energy and water security through machine learning, circular economy wastewater treatment plants (ML CE WWTP).\", 'keyword': 'Machine learning, Circular economy, Nutrient, Sustainable supply chains', 'title': 'Machine learning framework for wastewater circular economy — Towards smarter nutrient recoveries', 'authors': 'Allan Soo, Li Gao, Ho Kyong Shon'}, {'abstract': 'In the past 40 years, therapeutic antibody discovery and development have advanced considerably, with machine learning (ML) offering a promising way to speed up the process by reducing costs and the number of experiments required. Recent progress in ML-guided antibody design and development (D&D) has been hindered by the diversity of data sets and evaluation methods, which makes it difficult to conduct comparisons and assess utility. Establishing standards and guidelines will be crucial for the wider adoption of ML and the advancement of the field. This perspective critically reviews current practices, highlights common pitfalls and proposes method development and evaluation guidelines for various ML-based techniques in therapeutic antibody D&D. Addressing challenges across the ML process, best practices are recommended for each stage to enhance reproducibility and progress.', 'keyword': 'machine learning, data curation, drug discovery, data standardisation, FAIR data, model performance, metrics, model evaluation, antibodies, protein language models', 'title': 'Best practices for machine learning in antibody discovery and development', 'authors': 'Leonard Wossnig, Norbert Furtmann, Andrew Buchanan, Sandeep Kumar, Victor Greiff'}, {'abstract': 'Climate change (CC) is one of the greatest threats to human health, safety, and the environment. Given its current and future impacts, numerous studies have employed computational tools (e.g., machine learning, ML) to understand, mitigate, and adapt to CC. Therefore, this paper seeks to comprehensively analyze the research/publications landscape on the MLCC research based on published documents from Scopus. The high productivity and research impact of MLCC has produced highly cited works categorized as science, technology, and engineering to the arts, humanities, and social sciences. The most prolific author is Shamsuddin Shahid (based at Universiti Teknologi Malaysia), whereas the Chinese Academy of Sciences is the most productive affiliation on MLCC research. The most influential countries are the United States and China, which is attributed to the funding activities of the National Science Foundation and the National Natural Science Foundation of China (NSFC), respectively. Collaboration through co-authorship in high-impact journals such as Remote Sensing was also identified as an important factor in the high rate of productivity among the most active stakeholders researching MLCC topics worldwide. Keyword co-occurrence analysis identified four major research hotspots/themes on MLCC research that describe the ML techniques, potential risky sectors, remote sensing, and sustainable development dynamics of CC. In conclusion, the paper finds that MLCC research has a significant socio-economic, environmental, and research impact, which points to increased discoveries, publications, and citations in the near future.', 'keyword': 'Machine learning, Climate change, Sustainable development, Bibliometric analysis', 'title': 'A research landscape bibliometric analysis on climate change for last decades: Evidence from applications of machine learning', 'authors': 'Samuel-Soma M. Ajibade, Abdelhamid Zaidi, Festus Victor Bekun, Anthonia Oluwatosin Adediran, Mbiatke Anthony Bassey'}, {'abstract': 'Background Virtual reality technology has been widely used in surgical simulators, providing new opportunities for assessing and training surgical skills. Machine learning algorithms are commonly used to analyze and evaluate the performance of participants. However, their interpretability limits the personalization of the training for individual participants. Methods Seventy-nine participants were recruited and divided into three groups based on their skill level in intracranial tumor resection. Data on the use of surgical tools were collected using a surgical simulator. Feature selection was performed using the Minimum Redundancy Maximum Relevance and SVM-RFE algorithms to obtain the final metrics for training the machine learning model. Five machine learning algorithms were trained to predict the skill level, and the support vector machine performed the best, with an accuracy of 92.41% and Area Under Curve value of0.98253. The machine learning model was interpreted using Shapley values to identify the important factors contributing to the skill level of each participant. Results This study demonstrates the effectiveness of machine learning in differentiating the evaluation and training of virtual reality neurosurgical per- formances. The use of Shapley values enables targeted training by identifying deficiencies in individual skills. Conclusions This study provides insights into the use of machine learning for personalized training in virtual reality neurosurgery. The interpretability of the machine learning models enables the development of individualized training programs. In addition, this study highlighted the potential of explanatory models in training external skills.', 'keyword': 'Machine learning, Neurosurgery, Shapley values, Virtual reality, Human-robot interaction', 'title': 'Personalized assessment and training of neurosurgical skills in virtual reality: An interpretable machine learning approach', 'authors': 'Fei Li, Zhibao Qin, Kai Qian, Shaojun Liang, Chengli Li, Yonghang Tai'}, {'abstract': 'The past decade has seen a sharp increase in machine learning (ML) applications in scientific research. This review introduces the basic constituents of ML, including databases, features, and algorithms, and highlights a few important achievements in chemistry that have been aided by ML techniques. The described databases include some of the most popular chemical databases for molecules and materials obtained from either experiments or computational calculations. Important two-dimensional (2D) and three-dimensional (3D) features representing the chemical environment of molecules and solids are briefly introduced. Decision tree and deep learning neural network algorithms are overviewed to emphasize their frameworks and typical application scenarios. Three important fields of ML in chemistry are discussed: ① retrosynthesis, in which ML predicts the likely routes of organic synthesis; ② atomic simulations, which utilize the ML potential to accelerate potential energy surface sampling; and ③ heterogeneous catalysis, in which ML assists in various aspects of catalytic design, ranging from synthetic condition optimization to reaction mechanism exploration. Finally, a prospect on future ML applications is provided.', 'keyword': 'Machine learning, Atomic simulation, Catalysis, Retrosynthesis, Neural network potential', 'title': 'Machine Learning for Chemistry: Basics and Applications', 'authors': 'Yun-Fei Shi, Zheng-Xin Yang, Sicong Ma, Pei-Lin Kang, Cheng Shang, P. Hu, Zhi-Pan Liu'}, {'abstract': 'Hyperspectral Imaging (HSI) plays a crucial role in detecting, identifying, and classifying a wide range of natural resources, including minerals, geological phenomena like volcanic eruptions, and vegetation. Segmentation and classification of HSI play vital roles in extracting meaningful information and identifying different land cover or land use categories within the scene. One of the primary limitations associated with HSI is the scarcity of labeled samples. Obtaining annotated samples is a laborious and time-consuming process, posing a significant challenge in the field. This work presents an Enhanced Affinity Propagation Clustering (EAPC) and Modified Extreme Learning Machine (MELM) for segmentation and classification of HSI. Initially, the HSI images are pre-processed by the non-linear diffusion partial differential equation. Then, the segmentation process is performed by the EAPC and it is the combination of Affinity Propagation Clustering (APC) with Light Spectrum Algorithm (LSA). Finally, the classification is performed by the MELM and the experimentation is demonstrated on the Salinas dataset and achieved better accuracy and sensitivity of 97.3 % and 98.2 % respectively.', 'keyword': 'Hyperspectral imaging, Enhanced affinity propagation clustering, Classification, Modified extreme learning machine', 'title': 'Enhanced affinity propagation clustering with a modified extreme learning machine for segmentation and classification of hyperspectral imaging', 'authors': 'V. Antony Asir Daniel, K. Vijayalakshmi, Priyanka Pramod Pawar, Deepak Kumar, A. Bhuvanesh, A. Josephine Christilda'}]\n"
     ]
    }
   ],
   "source": [
    "# Get the abstract from arXiv. Many papers does not have keyword section, so I used advanced search and find \"keyword\" in the abstract.\n",
    "# I know the instruction asked to write the content to a file, but I will write the content here so that it can be reproduced easily.\n",
    "\n",
    "# The content of the abstracts and keywords are as follows:\n",
    "# abstract.txt and keywords.txt contains the content of the abstracts and keywords, separated by newlines.\n",
    "\n",
    "# This cell will write the content to the files. So SKIP running this cell if you want to use the content you provided.\n",
    "\n",
    "query = \"Advancement in Physic and Biology Science by Machine learning\"\n",
    "\n",
    "\n",
    "abstracts_raw = \"\"\"\n",
    "Machine learning (ML) has drawn tremendous interest for its capacity to extract useful information that may be overlooked with conventional analysis techniques and for its versatility in a wide range of research domains, including biomedical sensing and imaging. In this perspective, we provide an overview focused on the uses and benefits of ML in areas of plasmonics in biology. ML methodologies for processing data from plasmonic biosensing and imaging systems by supervised and unsupervised learning to achieve enhanced detection and quantification of target analytes are described. In addition, deep learning-based approaches to improve the design of plasmonic structures are presented. Data analysis based on ML for classification, regression, and clustering by dimension reduction is presented. We also discuss ML-based prediction and design of plasmonic structures and sensors using discriminative and generative models. Challenges and the outlook for ML for plasmonics in biology are summarized. Based on these insights, we are convinced that ML can add value to plasmonics techniques in biological sensing and imaging applications to make them more powerful with improved detection and resolution.\n",
    "Background and objective Mechanistic-based Model simulations (MM) are an effective approach commonly employed, for research and learning purposes, to better investigate and understand the inherent behavior of biological systems. Recent advancements in modern technologies and the large availability of omics data allowed the application of Machine Learning (ML) techniques to different research fields, including systems biology. However, the availability of information regarding the analyzed biological context, sufficient experimental data, as well as the degree of computational complexity, represent some of the issues that both MMs and ML techniques could present individually. For this reason, recently, several studies suggest overcoming or significantly reducing these drawbacks by combining the above-mentioned two methods. In the wake of the growing interest in this hybrid analysis approach, with the present review, we want to systematically investigate the studies available in the scientific literature in which both MMs and ML have been combined to explain biological processes at genomics, proteomics, and metabolomics levels, or the behavior of entire cellular populations. Methods Elsevier Scopus®, Clarivate Web of Science™ and National Library of Medicine PubMed® databases were enquired using the queries reported in Table 1, resulting in 350 scientific articles. Results Only 14 of the 350 documents returned by the comprehensive search conducted on the three major online databases met our search criteria, i.e. present a hybrid approach consisting of the synergistic combination of MMs and ML to treat a particular aspect of systems biology. Conclusions Despite the recent interest in this methodology, from a careful analysis of the selected papers, it emerged how examples of integration between MMs and ML are already present in systems biology, highlighting the great potential of this hybrid approach to both at micro and macro biological scales.\n",
    "This bibliometric research explores the global evolution of machine learning applications in medical and healthcare research for 3 decades (1994 to 2023). The study applies data mining techniques to a comprehensive dataset of published articles related to machine learning applications in the medical and healthcare sectors. The data extraction process includes the retrieval of relevant information from the source sources such as journals, books, and conference proceedings. An analysis of the extracted data is then conducted to identify the trends in the machine learning applications in medical and healthcare research. The Results revealed the publications published and indexed in the Scopus and PubMed database over the last 30 years. Bibliometric Analysis revealed that funding played a more significant role in publication productivity compared to collaboration (co-authorships), particularly at the country level. Hotspots analysis revealed three core research themes on MLHC research hence demonstrating the importance of machine learning applications to medical and healthcare research. Further, the study showed that the MLHC research landscape has largely focused on ML applications to tackle various issues ranging from chronic medical challenges (e.g., cardiological diseases) to patient data security. The findings of this research may be useful to policy makers and practitioners in the medical and healthcare sectors and to global research endeavours in the field. Future studies could include addressing issues such as growing ethical considerations, integration, and practical applications in wearable technology, IoT, and smart healthcare systems.\n",
    "Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures from the Cahn-Hilliard model. We analyze the accuracy and robustness of all models and elucidate the reasons for the differences in their performances. The impact of including domain knowledge through tailored features is studied, and general recommendations based on the availability and quality of training data are derived from this.\n",
    "Drug discovery and development is a time-consuming process that involves identifying, designing, and testing new drugs to address critical medical needs. In recent years, machine learning (ML) has played a vital role in technological advancements and has shown promising results in various drug discovery and development stages. ML can be categorized into supervised, unsupervised, semi-supervised, and reinforcement learning. Supervised learning is the most used category, helping organizations solve several real-world problems. This study presents a comprehensive survey of supervised learning algorithms in drug design and development, focusing on their learning process and succinct mathematical formulations, which are lacking in the literature. Additionally, the study discusses widely encountered challenges in applying supervised learning for drug discovery and potential solutions. This study will be beneficial to researchers and practitioners in the pharmaceutical industry as it provides a simplified yet comprehensive review of the main concepts, algorithms, challenges, and prospects in supervised learning.\n",
    "Additive manufacturing (AM) has undergone significant development over the past decades, resulting in vast amounts of data that carry valuable information. Numerous research studies have been conducted to extract insights from AM data and utilize it for optimizing various aspects such as the manufacturing process, supply chain, and real-time monitoring. Data integration into proposed digital twin frameworks and the application of machine learning techniques is expected to play pivotal roles in advancing AM in the future. In this paper, we provide an overview of machine learning and digital twin-assisted AM. On one hand, we discuss the research domain and highlight the machine-learning methods utilized in this field, including material analysis, design optimization, process parameter optimization, defect detection and monitoring, and sustainability. On the other hand, we examine the status of digital twin-assisted AM from the current research status to the technical approach and offer insights into future developments and perspectives in this area. This review paper aims to examine present research and development in the convergence of big data, machine learning, and digital twin-assisted AM. Although there are numerous review papers on machine learning for additive manufacturing and others on digital twins for AM, no existing paper has considered how these concepts are intrinsically connected and interrelated. Our paper is the first to integrate the three concepts big data, machine learning, and digital twins and propose a cohesive framework for how they can work together to improve the efficiency, accuracy, and sustainability of AM processes. By exploring latest advancements and applications within these domains, our objective is to emphasize the potential advantages and future possibilities associated with integration of these technologies in AM.\n",
    "Microfluidic devices are increasingly widespread in the literature, being applied to numerous exciting applications, from chemical research to Point-of-Care devices, passing through drug development and clinical scenarios. Setting up these microenvironments, however, introduces the necessity of locally controlling the variables involved in the phenomena under investigation. For this reason, the literature has deeply explored the possibility of introducing sensing elements to investigate the physical quantities and the biochemical concentration inside microfluidic devices. Biosensors, particularly, are well known for their high accuracy, selectivity, and responsiveness. However, their signals could be challenging to interpret and must be carefully analysed to carry out the correct information. In addition, proper data analysis has been demonstrated even to increase biosensors' mentioned qualities. To this regard, machine learning algorithms are undoubtedly among the most suitable approaches to undertake this job, automatically learning from data and highlighting biosensor signals' characteristics at best. Interestingly, it was also demonstrated to benefit microfluidic devices themselves, in a new paradigm that the literature is starting to name “intelligent microfluidics”, ideally closing this benefic interaction among these disciplines. This review aims to demonstrate the advantages of the triad paradigm microfluidics-biosensors-machine learning, which is still little used but has a great perspective. After briefly describing the single entities, the different sections will demonstrate the benefits of the dual interactions, highlighting the applications where the reviewed triad paradigm was employed.\n",
    "Carbon dots (CDs) have been a subject of great interest among researchers due to their diverse physicochemical properties and numerous advantageous attributes such as good biocompatibility, unique optical properties, low cost, eco-friendliness, abundant functional groups (e.g., amino, hydroxyl, and carboxyl) high stability, and excellent electron mobility. With the rapid advancement of data-driven technologies, machine learning (ML) has gained significant attention as a primary and indispensable tool in different applications in numerous research fields, including the monitoring of chemical reactions. By utilizing machine learning algorithms, the properties of carbon dots can be enhanced, such as fluorescence, stability, and electrocatalytic activity, as well as optimizing the synthesis process. Moreover, machine learning can be utilized to screen carbon dot precursors and predict their properties, providing various advantages in developing carbon dots with superior properties. As a result, machine learning offers numerous benefits in carbon dots synthesis, which has the potential to impact various fields. Photoelectrochemical sensors are a type of chemical sensor that use light to generate a photocurrent, which is then used to detect the presence of a target analyte. These sensors have gained significant attention due to their high sensitivity, selectivity, and low cost, making them a promising tool for a variety of applications in fields such as environmental monitoring and biomedical sensing. Due to their fascinating electronic and photonic properties, CQDs have gained considerable attention in the development of photoelectrochemical sensors. This review article provides an overview of recent advancements in the machine learning synthesis of CQDs and their applications in constructing photoelectrochemical sensors.\n",
    "Lithium-ion batteries play a pivotal role in a wide range of applications, from electronic devices to large-scale electrified transportation systems and grid-scale energy storage. Nevertheless, they are vulnerable to both progressive aging and unexpected failures, which can result in catastrophic events such as explosions or fires. Given their expanding global presence, the safety of these batteries and potential hazards from serious malfunctions are now major public concerns. Over the past decade, scholars and industry experts are intensively exploring methods to monitor battery safety, spanning from materials to cell, pack and system levels and across various spectral, spatial, and temporal scopes. In this Review, we start by summarizing the mechanisms and nature of battery failures. Following this, we explore the intricacies in predicting battery system evolution and delve into the specialized knowledge essential for data-driven, machine learning models. We offer an exhaustive review spotlighting the latest strides in battery fault diagnosis and failure prognosis via an array of machine learning approaches. Our discussion encompasses: (1) supervised and reinforcement learning integrated with battery models, apt for predicting faults/failures and probing into failure causes and safety protocols at the cell level; (2) unsupervised, semi-supervised, and self-supervised learning, advantageous for harnessing vast data sets from battery modules/packs; (3) few-shot learning tailored for gleaning insights from scarce examples, alongside physics-informed machine learning to bolster model generalization and optimize training in data-scarce settings. We conclude by casting light on the prospective horizons of comprehensive, real-world battery prognostics and management.\n",
    "Allosteric regulation is a fundamental biological mechanism that can control critical cellular processes via allosteric modulator binding to protein distal functional sites. The advantages of allosteric modulators over orthosteric ones have sparked the development of numerous computational approaches, such as the identification of allosteric binding sites, to facilitate allosteric drug discovery. Building on the success of machine learning (ML) models for solving complex problems in biology and chemistry, several ML models for predicting allosteric sites have been developed. In this review, we provide an overview of these models and discuss future perspectives powered by the field of artificial intelligence such as protein language models.\n",
    "Machine learning (ML) is a range of powerful computational algorithms capable of generating predictive models via intelligent autonomous analysis of relatively large and often unstructured data. ML has become an integral part of our daily lives with a plethora of applications, including web, business, automotive industry, clinical diagnostics, scientific research, and more recently, forensic science. In the field of forensic DNA, the manual analysis of complex data can be challenging, time-consuming, and error-prone. The integration of novel ML-based methods may aid in streamlining this process while maintaining the high accuracy and reproducibility required for forensic tools. Due to the relative novelty of such applications, the forensic community is largely unaware of ML capabilities and limitations. Furthermore, computer science and ML professionals are often unfamiliar with the forensic science field and its specific requirements. This manuscript offers a brief introduction to the capabilities of machine learning methods and their applications in the context of forensic DNA analysis and offers a critical review of the current literature in this rapidly developing field.\n",
    "Recent advancements in immune sequencing and experimental techniques are generating extensive T cell receptor (TCR) repertoire data, enabling the development of models to predict TCR binding specificity. Despite the computational challenges posed by the vast diversity of TCRs and epitopes, significant progress has been made. This review explores the evolution of computational models designed for this task, emphasizing machine learning efforts, including early unsupervised clustering approaches, supervised models, and recent applications of Protein Language Models (PLMs), deep learning models pretrained on extensive collections of unlabeled protein sequences that capture crucial biological properties. We survey the most prominent models in each category and offer a critical discussion on recurrent challenges, including the lack of generalization to new epitopes, dataset biases, and shortcomings in model validation designs. Focusing on PLMs, we discuss the transformative impact of Transformer-based protein models in bioinformatics, particularly in TCR specificity analysis. We discuss recent studies that exploit PLMs to deliver notably competitive performances in TCR-related tasks, while also examining current limitations and future directions. Lastly, we address the pressing need for improved interpretability in these often opaque models, and examine current efforts to extract biological insights from large black box models.\n",
    "Machine learning models used for energy conversion system optimization cannot extrapolate outside the bounds of training data and often produce physically unrealistic results when making predictions in regions of sparse training data. The toy model concept introduced in this work allows machine learning models to extrapolate to some extent and also reduces the possibility of physically unrealistic results. It uses physics to shrink the model input (feature space) of data-based models, so that extrapolations in the data-based feature space tend to become interpolations in the physics-based (toy variable) feature space. The physics-based model can be any model or experiment that can shrink the feature space without affecting interpolation and is termed a ‘toy model' because it does not need to be accurate or make predictions of interest. The concept has been applied to model experimental data obtained from three complex systems: a. Aerodynamic forces on a spinning and vibrating baseball with inclined axis of rotation (toy model: CFD model), b. Hydraulic turbine efficiency (toy model: PIV images of flow through stationary blades), and c. Combustion generated engine emissions (toy model: system-level 1-D model). All extrapolations were converted into interpolations for the first two systems while a 75% conversion was achieved for the emission predictions. The engine toy model produced 736,281 possible feature spaces from which one unique feature space was chosen for every prediction based on agreement between different machine learning algorithms. It is shown that the ability of the toy variables to reorganize the data is important, while their accuracy is relatively unimportant. The toy model concept was demonstrated to work with neural networks and regression, and can be used to increase model robustness or reduce training data requirements.\n",
    "Following other fields of science, Deep Learning models are gaining attention within the statistical physics community as a powerful and efficient way for analysing experimental and synthetic time series, and for quantifying properties thereof. Applying such models is nevertheless a path full of pitfalls, not only due to their inherent complexity, but also to a lack of understanding of some of their idiosyncrasies. We here discuss some of these pitfalls in the context of time series classification, covering from the selection of the best model hyperparameters, how the models have to be trained, to the way data have to be pre-processed. While not providing one-fits-all answers, the statistical physics practitioner will here find what questions ought to be posed, and a first guide about how to tackle them.\n",
    "The optimization of the electrode manufacturing process is important for upscaling the application of Lithium-Ion Batteries (LIBs) to cater for growing energy demand. LIB manufacturing is important to be optimized because it determines the practical performance of the cells when the latter are being used in applications such as electric vehicles. In this study, we tackled the issue of high-performance electrodes for desired battery applications by proposing a data-driven approach supported by a deterministic machine learning-assisted pipeline for bi-objective optimization of the electrochemical performances. This pipeline allows the inverse design of the process parameters to adopt to manufacture electrodes for energy or power applications. This work is an analogy to our previous work that addressed the optimization of the electrode microstructures for kinetic, ionic, and electronic transport properties improvement. An electrochemical model is fed with the electrode properties characterizing the electrode microstructures generated by manufacturing simulations, and used to simulate the electrochemical performances. Secondly, the resulting dataset was used to train a deterministic model to implement fast optimizations to identify optimal electrodes. Our results suggested a high amount of active material, combined with intermediate values of solid content in the slurry and calendering degree, to achieve the optimal electrodes.\n",
    "Owing to the hexagonal close-packed (HCP) crystal structure inherent in Mg alloys, strong basal texture can readily be induced through the processes of rolling or extrusion. The anisotropy of the texture of Mg alloys impacts their stamping and forming capabilities, limiting their use in certain applications. Microalloying and shear deformation are currently the most common methods of weakening the texture of Mg alloys. Many shearing processes have been extensively studied, and given that they require complex equipment and make it difficult to achieve mass production, major attention has turned to studying the design of microalloys. Traditional trial-and-error approaches for developing micro-alloying confront many challenges, including longer test cycles and increasing expenses. The rapid advancement of big data and artificial intelligence opens up a new channel for the efficient advancement of metallic materials, specifically the application of machine learning to aid in the design of Mg alloys. ML modeling can be used to find correlations between features and attributes in data, allowing for the development and design of high-performance Mg alloys. The article provides an extensive overview of machine learning applications in Mg alloys. These include the discovery of high-performance alloys, the selection of coating designs, the design of Mg matrix composites, the prediction of second phases, the microstructure modification, optimization of rolling or extrusion parameters, and the prediction of mechanical and corrosion properties. In conclusion, challenges and prospects for the rational design of alloys with machine learning support were discussed.\n",
    "This review aims to highlight the role that computational chemistry has played in advancing the supramolecular chemistry field. We demonstrated recent uses of computational methodologies to elucidate noncovalent interactions in various processes occurring in supramolecular systems. We also emphasized the contributions of these techniques to studying reactions within confined space, showing how computational methodologies help clarify the effects of reactivity and conformational locking. Furthermore, we underscore the utilization of Molecular Dynamics (MD) in elucidating dynamical processes, understanding temperature and pressure effects, and exploring conformational space within supramolecular chemistry. Finally, we highlight the impact that the age of machine learning has on computational chemistry, showing how these universal approximators can enhance existing methods, predict properties, and efficiently explore the chemical space encompassed by these complex systems. This article explores the use of electronic structure methods to understand noncovalent interactions in supramolecular systems and the chemical reactions that occur within their chemical space. It also highlights the use of molecular dynamics and machine learning techniques in supramolecular chemistry.\n",
    "For decades, drug delivery scientists have been performing trial-and-error experimentation to manually sample parameter spaces and optimize release profiles through rational design. To enable this approach, scientists spend much of their career learning nuanced drug-material interactions that drive system behavior. In relatively simple systems, rational design criteria allow us to fine tune release profiles and enable efficacious therapies. However, as materials and drugs become increasingly sophisticated and their interactions have non-linear and compounding effects, the field is suffering the Curse of Dimensionality which prevents us from comprehending complex structure-function relationships. In the past, we have embraced this complexity by implementing high-throughput screens to increase the probability of finding ideal compositions. However, this brute force method was inefficient and led many to abandon these fishing expeditions. Fortunately, methods in data science including artificial intelligence / machine learning (AI/ML) are providing ideal analytical tools to model this complex data and ascertain quantitative structure-function relationships. In this Oration, I speak to the potential value of data science in drug delivery with particular focus on polymeric delivery systems. Here, I do not suggest that AI/ML will simply replace mechanistic understanding of complex systems. Rather, I propose that AI/ML should be yet another useful tool in the lab to navigate complex parameter spaces. The recent hype around AI/ML is breathtaking and potentially over inflated, but the value of these methods is poised to revolutionize how we perform science. Therefore, I encourage readers to consider adopting these skills and applying data science methods to their own problems. If done successfully, I believe we will all realize a paradigm shift in our approach to drug delivery.\n",
    "Machine learning (ML) has been rapidly transforming the landscape of natural sciences and has the potential to revolutionize the process of data analysis and hypothesis formulation as well as expand scientific knowledge. ML has been particularly instrumental in the advancement of cheminformatics and materials science, including membrane technology. In this review, we analyze the current state-of-the-art membrane-related ML applications from ML and membrane perspectives. We first discuss the ML foundations of different algorithms and design choices. Then, traditional and deep learning methods, including application examples from the membrane literature, are reported. We also discuss the importance of learning data and both molecular and membrane-system featurization. Moreover, we follow up on the discussion with examples of ML applications in membrane science and technology. We detail the literature using data-driven methods from property prediction to membrane fabrication. Various fields are also discussed, such as reverse osmosis, gas separation, and nanofiltration. We also differentiate between downstream predictive tasks and generative membrane design. Additionally, we formulate best practices and the minimum requirements for reporting reproducible ML studies in the field of membranes. This is the first systematic and comprehensive review of ML in membrane science.\n",
    "Not long ago, carbon quantum dots (CQDs) came into view as a revolutionary class of materials, propelling advancements in water remediation and electrochemical technology. This comprehensive review explores the cutting-edge developments in CQDs-based materials and their applications, addressing critical challenges in water treatment and electrochemical processes. Synthesized as ultra-tiny, dispersed particles with dimensions less than 10 nm, CQDs exhibit remarkable optical properties, including adjustable fluorescence emission across various colors. With a surge in published scientific articles, CQDs have garnered significant attention, offering potential solutions in heavy metal sensing, remediation, and electrocatalytic hydrogen evolution reactions (HER). The review highlights the high sensitivity of CQDs as fluorescent sensors, detecting contaminants in water with limits of detection down to femtomolar concentrations. Moreover, CQDs demonstrate excellent adsorptive capabilities for heavy metal removal, surpassing traditional adsorbents in terms of removal efficiency. Furthermore, CQDs serve as promising electrocatalysts, enhancing reaction kinetics and enabling efficient water splitting for clean energy generation. Furthermore, this review emphasizes the importance of machine learning in advancing CQDs-based materials, supported by case studies and examples that illustrate how machine learning techniques optimize CQDs synthesis, enhance their properties, and broaden their applications. However, challenges remain in the precise synthesis of CQDs, scalability of production processes, and understanding the interactions between CQDs and pollutants. Overcoming these challenges will unlock the full potential of CQDs-based materials, leading to sustainable and efficient solutions in water control and electrochemical processes.\n",
    "In Model Predictive Control (MPC) closed-loop performance heavily depends on the quality of the underlying prediction model, where such a model must be accurate and yet simple. A key feature in modern MPC applications is the potential for online model adaptation to cope with time-varying changes, part-to-part variations, and complex features of the system dynamics not caught by models derived from first principles. In this paper, we propose to use a physics-informed, or gray-box, model that extends the physics-based model with a data-driven component, namely a Recurrent Neural Network (RNN). Relying on physics-informed models allows for a rather limited size of the RNN, thereby enhancing online applicability compared to pure black-box models. This work presents a method based on Moving Horizon Estimation (MHE) for simultaneous state estimation and learning of the RNN sub-model, a potentially challenging issue due to limited information available in noisy input-output data and lack of knowledge of the internal state of the RNN. We provide a case study on a quadruple tank benchmark showing how the method can cope with part-to-part variations.\n",
    "Chronic lymphocytic leukemia (CLL) is a B cell neoplasm characterized by the accumulation of aberrant monoclonal B lymphocytes. CLL is the predominant type of leukemia in Western countries, accounting for 25% of cases. Although many patients remain asymptomatic, a subset may exhibit typical lymphoma symptoms, acquired immunodeficiency disorders, or autoimmune complications. Diagnosis involves blood tests showing increased lymphocytes and further examination using peripheral blood smear and flow cytometry to confirm the disease. With the significant advancements in machine learning (ML) and artificial intelligence (AI) in recent years, numerous models and algorithms have been proposed to support the diagnosis and classification of CLL. In this review, we discuss the benefits and drawbacks of recent applications of ML algorithms in the diagnosis and evaluation of patients diagnosed with CLL.\n",
    "As a follow-up to our recent Communication in the Journal of Chemical Physics [J. Chem. Phys. 159 071101 (2023)], we report and make available the Jupyter Notebook software here. This software performs binary machine learning classification (MLC) with the goal of learning negligible Hamiltonian matrix elements for vibrational dynamics. We illustrate its usefulness for a Hamiltonian matrix for H2O by using three MLC algorithms: Random Forest, Support Vector Machine, and Multi-layer Perceptron.\n",
    "While machine learning (ML) has made significant contributions to the biopharmaceutical field, its applications are still in the early stages in terms of providing direct support for quality-by-design based development and manufacturing of biologics, hindering the enormous potential for bioprocesses automation from their development to manufacturing. However, the adoption of ML-based models instead of conventional multivariate data analysis methods is significantly increasing due to the accumulation of large-scale production data. This trend is primarily driven by the real-time monitoring of process variables and quality attributes of biopharmaceutical products through the implementation of advanced process analytical technologies. Given the complexity and multidimensionality of a bioproduct design, bioprocess development, and product manufacturing data, ML-based approaches are increasingly being employed to achieve accurate, flexible, and high-performing predictive models to address the problems of analytics, monitoring, and control within the biopharma field. This paper aims to provide a comprehensive review of the current applications of ML solutions in the design, monitoring, control, and optimisation of upstream, downstream, and product formulation processes of monoclonal antibodies. Finally, this paper thoroughly discusses the main challenges related to the bioprocesses themselves, process data, and the use of machine learning models in monoclonal antibody process development and manufacturing. Moreover, it offers further insights into the adoption of innovative machine learning methods and novel trends in the development of new digital biopharma solutions.\n",
    "As the world's supply chains become disrupted through geopolitical instability and the race towards a net-zero future, policies have been implemented to improve the security of certain minerals and raw materials critical to a country's survival and sustainability goals. Circular economies (CE) are sought to be an ecosystem that will reduce virgin material consumption rates, lower carbon emissions, and decelerate the rate of landfilling. However, cost-effective and commercially attractive substitutes to conventional products are needed for this to be realised. Machine learning (ML) and the explosion of interest in artificial intelligence (AI) have led to growing interests in predictive and generative applications for sustainability. Phosphorous and, nutrients overall, operate on finite reserves essential for food supply chains; while such nutrients are largely present in municipal wastewater streams. Wastewater treatment plants (WWTPs) must then face a transformational force to become nutrient recovery centres, rather than follow a linear treat-for-disposal model. In this framework paper, ML is positioned as an enabler for scaled, cost-effective and safer recovery of nutrients and other valuable products — tying in economic, societal, technical and commercial factors through open data connectivity. Moreover, the paper issues a policy guide for institutions wishing to advance food, energy and water security through machine learning, circular economy wastewater treatment plants (ML CE WWTP).\n",
    "In the past 40 years, therapeutic antibody discovery and development have advanced considerably, with machine learning (ML) offering a promising way to speed up the process by reducing costs and the number of experiments required. Recent progress in ML-guided antibody design and development (D&D) has been hindered by the diversity of data sets and evaluation methods, which makes it difficult to conduct comparisons and assess utility. Establishing standards and guidelines will be crucial for the wider adoption of ML and the advancement of the field. This perspective critically reviews current practices, highlights common pitfalls and proposes method development and evaluation guidelines for various ML-based techniques in therapeutic antibody D&D. Addressing challenges across the ML process, best practices are recommended for each stage to enhance reproducibility and progress.\n",
    "Climate change (CC) is one of the greatest threats to human health, safety, and the environment. Given its current and future impacts, numerous studies have employed computational tools (e.g., machine learning, ML) to understand, mitigate, and adapt to CC. Therefore, this paper seeks to comprehensively analyze the research/publications landscape on the MLCC research based on published documents from Scopus. The high productivity and research impact of MLCC has produced highly cited works categorized as science, technology, and engineering to the arts, humanities, and social sciences. The most prolific author is Shamsuddin Shahid (based at Universiti Teknologi Malaysia), whereas the Chinese Academy of Sciences is the most productive affiliation on MLCC research. The most influential countries are the United States and China, which is attributed to the funding activities of the National Science Foundation and the National Natural Science Foundation of China (NSFC), respectively. Collaboration through co-authorship in high-impact journals such as Remote Sensing was also identified as an important factor in the high rate of productivity among the most active stakeholders researching MLCC topics worldwide. Keyword co-occurrence analysis identified four major research hotspots/themes on MLCC research that describe the ML techniques, potential risky sectors, remote sensing, and sustainable development dynamics of CC. In conclusion, the paper finds that MLCC research has a significant socio-economic, environmental, and research impact, which points to increased discoveries, publications, and citations in the near future.\n",
    "Background Virtual reality technology has been widely used in surgical simulators, providing new opportunities for assessing and training surgical skills. Machine learning algorithms are commonly used to analyze and evaluate the performance of participants. However, their interpretability limits the personalization of the training for individual participants. Methods Seventy-nine participants were recruited and divided into three groups based on their skill level in intracranial tumor resection. Data on the use of surgical tools were collected using a surgical simulator. Feature selection was performed using the Minimum Redundancy Maximum Relevance and SVM-RFE algorithms to obtain the final metrics for training the machine learning model. Five machine learning algorithms were trained to predict the skill level, and the support vector machine performed the best, with an accuracy of 92.41% and Area Under Curve value of0.98253. The machine learning model was interpreted using Shapley values to identify the important factors contributing to the skill level of each participant. Results This study demonstrates the effectiveness of machine learning in differentiating the evaluation and training of virtual reality neurosurgical per- formances. The use of Shapley values enables targeted training by identifying deficiencies in individual skills. Conclusions This study provides insights into the use of machine learning for personalized training in virtual reality neurosurgery. The interpretability of the machine learning models enables the development of individualized training programs. In addition, this study highlighted the potential of explanatory models in training external skills.\n",
    "The past decade has seen a sharp increase in machine learning (ML) applications in scientific research. This review introduces the basic constituents of ML, including databases, features, and algorithms, and highlights a few important achievements in chemistry that have been aided by ML techniques. The described databases include some of the most popular chemical databases for molecules and materials obtained from either experiments or computational calculations. Important two-dimensional (2D) and three-dimensional (3D) features representing the chemical environment of molecules and solids are briefly introduced. Decision tree and deep learning neural network algorithms are overviewed to emphasize their frameworks and typical application scenarios. Three important fields of ML in chemistry are discussed: ① retrosynthesis, in which ML predicts the likely routes of organic synthesis; ② atomic simulations, which utilize the ML potential to accelerate potential energy surface sampling; and ③ heterogeneous catalysis, in which ML assists in various aspects of catalytic design, ranging from synthetic condition optimization to reaction mechanism exploration. Finally, a prospect on future ML applications is provided.\n",
    "Hyperspectral Imaging (HSI) plays a crucial role in detecting, identifying, and classifying a wide range of natural resources, including minerals, geological phenomena like volcanic eruptions, and vegetation. Segmentation and classification of HSI play vital roles in extracting meaningful information and identifying different land cover or land use categories within the scene. One of the primary limitations associated with HSI is the scarcity of labeled samples. Obtaining annotated samples is a laborious and time-consuming process, posing a significant challenge in the field. This work presents an Enhanced Affinity Propagation Clustering (EAPC) and Modified Extreme Learning Machine (MELM) for segmentation and classification of HSI. Initially, the HSI images are pre-processed by the non-linear diffusion partial differential equation. Then, the segmentation process is performed by the EAPC and it is the combination of Affinity Propagation Clustering (APC) with Light Spectrum Algorithm (LSA). Finally, the classification is performed by the MELM and the experimentation is demonstrated on the Salinas dataset and achieved better accuracy and sensitivity of 97.3 % and 98.2 % respectively.\n",
    "\"\"\"\n",
    "\n",
    "keywords_raw = \"\"\"\n",
    "machine learning, plasmonics, data analysis, structure design, deep learning, biosensors, imaging\n",
    "Mathematical modeling, Machine learning, Reinforcement learning, Systems biology, Simulation, Systematic literature review\n",
    "Machine Learning, Healthcare analytics, Artificial Intelligence, Medical research, IoT, Algorithms, Bibliometric Analysis\n",
    "Structure-properties relation, Forward model, Feature engineering, Power spectrum density, Convolutional neural network, Support vector regression, Ising model, Cahn-Hilliard model\n",
    "Artificial intelligence, Deep learning, Machine learning, Neural network, Supervised learning\n",
    "Additive manufacturing, Big data, Machine learning, Digital twin, Data-driven\n",
    "Machine learning, Lab-on-a-Chip, Biosensing system, Intelligent microfluidics, Biosensors integration\n",
    "Carbon quantum dot, Machine learning, Synthesis, Photoelectrochemical sensors\n",
    "Lithium-ion batteries, Safety, Machine learning, Deep learning, Fault, Failure, Thermal runaway, Detection, Prediction\n",
    "Allostery, Machine learning, Drug design, Protein binding sites\n",
    "ANN, artificial neural networks, AT, analytical threshold, BN, Bayesian networks, CART, classification and regression trees, CE, capillary electrophoresis, CNN, convolutional neural network, DBSCAN, density-based spatial clustering of applications with noise, DeT, Decision Tree, DL, deep learning, DT, dynamic threshold, EPG, electropherogram, GAN, generative adversarial networks, GDA, generalised discriminant analysis, HID, human identification, k-NN, k-nearest neighbours, LDA, linear discriminant analysis, LR, likelihood ratio, MCMC, Markov Chain Monte Carlo, MAC, maximum allele count, MCA, multiple correspondence analysis, ML, machine learning, MLE, maximum likelihood estimation, MLP, multilayer perception, MLR, multinomial logistic regression, MPS, massively parallel sequencing, NB, Naive Bayes, NGS, Next Generation Sequencing, NoC, number of contributors, NT, no threshold, PCA, principal component analysis, PCoA, principal coordinates analysis, PG, probabilistic genotyping, PGS, probabilistic genotyping software, RF, random forest, SNP, single nucleotide polymorphism, ST, stochastic threshold, STR, short tandem repeat, SVM, support vector machine, TAC, total allele count, t-SNE, t-distributed stochastic neighbour embedding, Machine learning, Forensic DNA profiling, Human identification, AI, STRs\n",
    "Machine learning, T cell receptor, Specificity prediction, Protein language models, Interpretability\n",
    "Physics-Based Machine Learning, Extrapolation, Feature Selection, Feature Engineering, Engines, Turbines\n",
    "Deep Learning, Chaos, Classification\n",
    "Battery cell manufacturing, Bayesian optimization, Machine learning, Electrode, Numerical simulation\n",
    "Mg alloy, Machine learning, Strength, Plasticity, Microalloying\n",
    "computational chemistry, machine learning, supramolecular chemistry, noncovalent interactions, molecular dynamics, density functional theory, electronic structure calculations\n",
    "Machine learning, Artificial intelligence, Drug delivery, Controlled release, Formulation, Encapsulation\n",
    "Deep learning, Predictive models, Generative models, Molecular modeling, Cheminformatics\n",
    "Carbon quantum dots, Water remediation, Electrochemical advancements, Heavy metal sensing, Fluorescent sensors, Clean energy generation\n",
    "Learning-based MPC, Nonlinear MPC, Moving Horizon Estimation, Physics-informed learning, Adaptive MPC, Recurrent Neural Network, Gated Recurrent Unit\n",
    "Artificial intelligence, Chronic lymphocytic leukemia, Diagnosis, Machine learning\n",
    "Machine Learning Classification, Vibrational Configuration Interaction\n",
    "Biopharmaceuticals, Machine learning, Upstream, Downstream, Bioprocesses, Digital twin, Soft sensors\n",
    "Machine learning, Circular economy, Nutrient, Sustainable supply chains\n",
    "machine learning, data curation, drug discovery, data standardisation, FAIR data, model performance, metrics, model evaluation, antibodies, protein language models\n",
    "Machine learning, Climate change, Sustainable development, Bibliometric analysis\n",
    "Machine learning, Neurosurgery, Shapley values, Virtual reality, Human-robot interaction\n",
    "Machine learning, Atomic simulation, Catalysis, Retrosynthesis, Neural network potential\n",
    "Hyperspectral imaging, Enhanced affinity propagation clustering, Classification, Modified extreme learning machine\n",
    "\"\"\"\n",
    "\n",
    "title_raw = \"\"\"\n",
    "Machine learning and its applications for plasmonics in biology\n",
    "Combined mechanistic modeling and machine-learning approaches in systems biology - A systematic literature review\n",
    "Evolution of Machine Learning Applications in Medical and Healthcare Analytics Research: A Bibliometric Analysis\n",
    "Efficient surrogate models for materials science simulations: Machine learning-based prediction of microstructure properties\n",
    "Supervised machine learning in drug discovery and development: Algorithms, applications, challenges, and prospects\n",
    "Big data, machine learning, and digital twin assisted additive manufacturing: A review\n",
    "Integrating machine learning and biosensors in microfluidic devices: A review\n",
    "Machine learning-driven approaches for synthesizing carbon dots and their applications in photoelectrochemical sensors\n",
    "Battery safety: Machine learning-based prognostics\n",
    "Machine learning approaches in predicting allosteric sites\n",
    "Machine learning applications in forensic DNA profiling: A critical review\n",
    "T-cell receptor binding prediction: A machine learning revolution\n",
    "Using physics to extend the range of machine learning models for an aerodynamic, hydraulic and combusting system: The toy model concept\n",
    "Deep Learning models for the analysis of time series: A practical introduction for the statistical physics practitioner\n",
    "Toward high-performance energy and power battery cells with machine learning-based optimization of electrode manufacturing\n",
    "A brief review of machine learning-assisted Mg alloy design, processing, and property predictions\n",
    "Supramolecular Chemistry: Exploring the Use of Electronic Structure, Molecular Dynamics, and Machine Learning Approaches\n",
    "Machine learning in drug delivery\n",
    "Machine learning for the advancement of membrane science and technology: A critical review\n",
    "The interface of machine learning and carbon quantum dots: From coordinated innovative synthesis to practical application in water control and electrochemistry\n",
    "Physics-Informed Online Learning by Moving Horizon Estimation: Learning Recurrent Neural Networks in Gray-box Models\n",
    "Revolutionizing chronic lymphocytic leukemia diagnosis: A deep dive into the diverse applications of machine learning\n",
    "Machine learning software to learn negligible elements of the Hamiltonian matrix\n",
    "Applications of machine learning in antibody discovery, process development, manufacturing and formulation: Current trends, challenges, and opportunities\n",
    "Machine learning framework for wastewater circular economy — Towards smarter nutrient recoveries\n",
    "Best practices for machine learning in antibody discovery and development\n",
    "A research landscape bibliometric analysis on climate change for last decades: Evidence from applications of machine learning\n",
    "Personalized assessment and training of neurosurgical skills in virtual reality: An interpretable machine learning approach\n",
    "Machine Learning for Chemistry: Basics and Applications\n",
    "Enhanced affinity propagation clustering with a modified extreme learning machine for segmentation and classification of hyperspectral imaging\n",
    "\"\"\"\n",
    "\n",
    "authors_raw = \"\"\"\n",
    "Gwiyeong Moon, Jongha Lee, Hyunwoong Lee, Hajun Yoo, Kwanhwi Ko, Seongmin Im, Donghyun Kim\n",
    "Anna Procopio, Giuseppe Cesarelli, Leandro Donisi, Alessio Merola, Francesco Amato, Carlo Cosentino\n",
    "Samuel-Soma M. Ajibade, Gloria Nnadwa Alhassan, Abdelhamid Zaidi, Olukayode Ayodele Oki, Joseph Bamidele Awotunde, Emeka Ogbuju, Kayode A. Akintoye\n",
    "Binh Duong Nguyen, Pavlo Potapenko, Aytekin Demirci, Kishan Govind, Sébastien Bompas, Stefan Sandfeld\n",
    "George Obaido, Ibomoiye Domor Mienye, Oluwaseun F. Egbelowo, Ikiomoye Douglas Emmanuel, Adeola Ogunleye, Blessing Ogbuokiri, Pere Mienye, Kehinde Aruleba\n",
    "Liuchao Jin, Xiaoya Zhai, Kang Wang, Kang Zhang, Dazhong Wu, Aamer Nazir, Jingchao Jiang, Wei-Hsin Liao\n",
    "Gianni Antonelli, Joanna Filippi, Michele D'Orazio, Giorgia Curci, Paola Casti, Arianna Mencattini, Eugenio Martinelli\n",
    "Roya Mohammadzadeh kakhki, Mojtaba Mohammadpoor\n",
    "Jingyuan Zhao, Xuning Feng, Quanquan Pang, Michael Fowler, Yubo Lian, Minggao Ouyang, Andrew F. Burke\n",
    "Francho Nerín-Fonz, Zoe Cournia\n",
    "Mark Barash, Dennis McNevin, Vladimir Fedorenko, Pavel Giverts\n",
    "Anna Weber, Aurélien Pélissier, María Rodríguez Martínez\n",
    "Indranil Brahma, Robert Jennings, Bradley Freid\n",
    "Alfredo Crespo-Otero, Pau Esteve, Massimiliano Zanin\n",
    "Marc Duquesnoy, Chaoyue Liu, Vishank Kumar, Elixabete Ayerbe, Alejandro A. Franco\n",
    "Yanhui Cheng, Lifei Wang, Chaoyang Yang, Yunli Bai, Hongxia Wang, Weili Cheng, Hanuma Reddy Tiyyagura, Alexander Komissarov, Kwang Seon Shin\n",
    "Matheus C. Colaço, Vinícius A. Glitz, Amanda K. Jacobs, Vinícius C. Port, Giovanni F. Caramori\n",
    "Adam J. Gormley\n",
    "Gergo Ignacz, Lana Bader, Aron K. Beke, Yasir Ghunaim, Tejus Shastry, Hakkim Vovusha, Matthew R. Carbone, Bernard Ghanem, Gyorgy Szekely\n",
    "Marwa El-Azazy, Ahmed I. Osman, Mahmoud Nasr, Yassmin Ibrahim, Nessreen Al-Hashimi, Khalid Al-Saad, Mohammad A. Al-Ghouti, Mohamed F. Shibl, Ala'a H. Al-Muhtaseb, David W. Rooney, Ahmed S. El-Shafie\n",
    "Kristoffer Fink Løwenstein, Daniele Bernardini, Alberto Bemporad, Lorenzo Fagiano\n",
    "Mohamed Elhadary, Amgad Mohamed Elshoeibi, Ahmed Badr, Basel Elsayed, Omar Metwally, Ahmed Mohamed Elshoeibi, Mervat Mattar, Khalil Alfarsi, Salem AlShammari, Awni Alshurafa, Mohamed Yassin\n",
    "Chen Qu, Paul L. Houston, Qi Yu, Priyanka Pandey, Riccardo Conte, Apurba Nandi, Joel M. Bowman\n",
    "Thanh Tung Khuat, Robert Bassett, Ellen Otte, Alistair Grevis-James, Bogdan Gabrys\n",
    "Allan Soo, Li Gao, Ho Kyong Shon\n",
    "Leonard Wossnig, Norbert Furtmann, Andrew Buchanan, Sandeep Kumar, Victor Greiff\n",
    "Samuel-Soma M. Ajibade, Abdelhamid Zaidi, Festus Victor Bekun, Anthonia Oluwatosin Adediran, Mbiatke Anthony Bassey\n",
    "Fei Li, Zhibao Qin, Kai Qian, Shaojun Liang, Chengli Li, Yonghang Tai\n",
    "Yun-Fei Shi, Zheng-Xin Yang, Sicong Ma, Pei-Lin Kang, Cheng Shang, P. Hu, Zhi-Pan Liu\n",
    "V. Antony Asir Daniel, K. Vijayalakshmi, Priyanka Pramod Pawar, Deepak Kumar, A. Bhuvanesh, A. Josephine Christilda\n",
    "\"\"\"\n",
    "\n",
    "metadata = []\n",
    "abstracts = abstracts_raw.split('\\n')\n",
    "keywords = keywords_raw.split('\\n')\n",
    "titles = title_raw.split('\\n')\n",
    "authors = authors_raw.split('\\n')\n",
    "\n",
    "# #read files\n",
    "# query = \"artificial intelligence\"\n",
    "# with open('resources/abstracts2.txt','r') as f:\n",
    "#     data=f.read().lower().strip()\n",
    "\n",
    "# data=data.split('-next-')\n",
    "# data=[ab.strip().split('\\n')[:] for ab in data][:]\n",
    "\n",
    "# titles = [ab[0] for ab in data]\n",
    "# authors = [ab[1] for ab in data]\n",
    "# abstracts = [ab[2] for ab in data]\n",
    "# keywords = [ab[3] for ab in data]\n",
    "# ###end\n",
    "\n",
    "for i in range(len(abstracts)):\n",
    "    if not abstracts[i]:\n",
    "        continue\n",
    "    metadata.append({\n",
    "        'abstract': abstracts[i],\n",
    "        'keyword': keywords[i],\n",
    "        'title': titles[i],\n",
    "        'authors': authors[i]\n",
    "    })\n",
    "output_file_path = 'resources/abstracts3.txt'\n",
    "\n",
    "# Write the metadata back to the file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for entry in metadata:\n",
    "        f.write(f\"{entry['title']}\\n\")\n",
    "        f.write(f\"{entry['authors']}\\n\")\n",
    "        f.write(f\"{entry['abstract']}\\n\")\n",
    "        f.write(f\"{entry['keyword']}\\n\")\n",
    "        f.write('-next-\\n')\n",
    "\n",
    "print(f\"Total metadata: {len(metadata)}\")\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C-2: We want to test how much the keyword metadata content matches the document and query. Concatenate the list of keywords (in metadata) of all documents and construct an inverted file, stating for each keyword the document number it matches. Let KK be this indexed file of keywords (ordered in alphabetical order). Write a script that computes the edit similarity between each keyword in the metadata and the query T (If T contains more than one query term, then the similarity is understood as the maximum similarity score (Edit metric) among all terms of query, e.g., if T = T1, T2, T3, then for keyword K, we have Sim(K,T) = max(Sim(K,T1), Sim(K,T2), Sim(K,T2)). Save the result as an array X whose size is size of all keywords (in metadata) in all documents (excluding repetition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "\n",
    "class InformationRetrievalIndexedFrequency(InformationRetrievalIndexed):  # Could have improved InformationRetrievalIndexed, but it's in separated task, so I would override inheritance. This is not elegant in real project, requiring refactoring\n",
    "    def __init__(self, metadata: list[dict[str, str]] = [], keyword_separator: list[str] = [',']) -> None:\n",
    "        self.metadata: list[dict[str, str]] = metadata\n",
    "        self.inverted_index: dict[str, list[int]] = {}\n",
    "        self.inverted_index_built = False\n",
    "        if len(keyword_separator) == 0:\n",
    "            raise ValueError(\"Keyword separators must not be empty\")\n",
    "        self.keyword_separator = keyword_separator\n",
    "        self.indexed_keywords: list[str] = []\n",
    "        self.keyword_index: dict[str, int] = {}\n",
    "\n",
    "    def build_inverted_index(self) -> None:\n",
    "        for idx, article in enumerate(self.metadata):\n",
    "            keywords = article['keyword']\n",
    "            if len(self.keyword_separator) > 1:\n",
    "                for separator in self.keyword_separator:\n",
    "                    keywords = keywords.replace(separator, self.keyword_separator[0])\n",
    "            keywords = keywords.split(',')\n",
    "            self.metadata[idx]['keyword_parsed'] = sorted(list({\n",
    "                keyword.strip().lower()\n",
    "                for keyword in keywords if keyword.strip()\n",
    "            }))\n",
    "\n",
    "        set_keywords = {\n",
    "            keyword\n",
    "            for article in self.metadata\n",
    "            for keyword in article['keyword_parsed']\n",
    "        }\n",
    "\n",
    "        self.indexed_keywords = sorted(list(set_keywords))\n",
    "        self.keyword_index = {keyword: idx for idx, keyword in enumerate(self.indexed_keywords)}\n",
    "\n",
    "        # Loop through each abstract and check for keywords\n",
    "        self.inverted_index = {keyword: [] for keyword in set_keywords}\n",
    "        for idx, article in enumerate(self.metadata):\n",
    "            for kw in self.inverted_index:\n",
    "                # Reuse the query_match_abstract method because if we just do plain \"if kw in abstract\", it will get bugged: \"AI\" matches \"Pair\"\n",
    "                if InformationRetrieval.query_match_abstract(kw, article['abstract']):\n",
    "                    self.inverted_index[kw].append(idx)\n",
    "\n",
    "        self.inverted_index_built = True\n",
    "\n",
    "    @staticmethod\n",
    "    def min_edit_distance(keyword: str, text: str) -> tuple[int, str]:\n",
    "        # Normalize the input by converting to lowercase\n",
    "        keyword = keyword.lower()\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize\n",
    "        text_words = text.split()\n",
    "        keyword_words = keyword.split()\n",
    "\n",
    "        keyword_length = len(keyword_words)\n",
    "        min_distance = float('inf')\n",
    "        min_string = ''\n",
    "\n",
    "        # # old approach, sliding window on character-basis\n",
    "        # for i in range(len(text)):\n",
    "        #     for j in range(i + 1, len(text)):\n",
    "        #         substring = text[i:j]\n",
    "        #         distance = edit_distance(keyword, substring)\n",
    "        #         if distance < min_distance:\n",
    "        #             min_distance = distance\n",
    "        #             min_string = substring\n",
    "\n",
    "        # return (min_distance, min_string)\n",
    "\n",
    "        # Sliding window over the text on word-basis\n",
    "        for i in range(len(text_words) - keyword_length + 1):\n",
    "            # Extract the current window of words from the text\n",
    "            window = ' '.join(text_words[i:i + keyword_length])\n",
    "\n",
    "            # Compute the edit distance between the keyword and this window\n",
    "            distance = edit_distance(keyword, window)\n",
    "\n",
    "            # If this distance is the minimum found so far, update the min_distance and min_string\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                min_string = window\n",
    "\n",
    "        # handle the case where the text is shorter than the keyword\n",
    "        if min_string == '':\n",
    "            min_string = text\n",
    "            min_distance = edit_distance(keyword, text)\n",
    "\n",
    "        return (min_distance, min_string)\n",
    "\n",
    "    @staticmethod\n",
    "    def similarity(keyword: str, text: str) -> float:\n",
    "        dist, _ = InformationRetrievalIndexedFrequency.min_edit_distance(keyword, text)\n",
    "\n",
    "        # Normalize the similarity score\n",
    "        # the score is determined as (100% - edit distance percentage)\n",
    "        # where edit distance percentage is the edit distance divided by maximum distance, which is length of longest string\n",
    "\n",
    "        max_len = max(len(keyword), len(text))\n",
    "        similarity = 1 - dist / max_len if max_len != 0 else 0\n",
    "        return similarity\n",
    "\n",
    "    def compute_similarity_with_string(self, str: str, keywords = None) -> list[float]:\n",
    "        if keywords is None:\n",
    "            keywords = self.indexed_keywords\n",
    "\n",
    "        X = []\n",
    "        for keyword in keywords:\n",
    "            similarity = self.similarity(keyword, str)\n",
    "            X.append(similarity)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    def compute_similarity_with_query(self, query: str) -> list[float]:\n",
    "        return self.compute_similarity_with_string(query)\n",
    "\n",
    "    # Part of task C-3\n",
    "    def compute_similarity_keyword_with_title(self) -> list[list[float]]:\n",
    "        result = [0.0] * len(self.indexed_keywords)\n",
    "\n",
    "        for article in self.metadata:\n",
    "            title_similarity = self.compute_similarity_with_string(article['title'], article['keyword_parsed'])\n",
    "            for idx, similarity in enumerate(title_similarity):\n",
    "                result[self.keyword_index[article['keyword_parsed'][idx]]] = max(similarity, result[self.keyword_index[article['keyword_parsed'][idx]]])\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Part of task C-5\n",
    "    # Using regex boundary \\b to matches keyword to satisfy exact matching.\n",
    "    # Because, for example, \"AI\" will matches \"pair\", but that is wrong.\n",
    "    #                       but we cannot split all the words because then multiple words such as \"machine learning\" won't be matched\n",
    "    @staticmethod\n",
    "    def query_count_match_abstract(query: str, abstract: str) -> bool:\n",
    "        pattern = re.compile(r'\\b' + re.escape(query) + r'\\b')\n",
    "        return len(pattern.findall(abstract))\n",
    "\n",
    "    # Part of task C-5\n",
    "    def calculate_keyword_frequencies_in_text(self, text: str) -> list[int]:\n",
    "        num_keywords = len(self.indexed_keywords)\n",
    "        Mi = [0 for _ in range(num_keywords)]\n",
    "\n",
    "        for j, keyword in enumerate(self.indexed_keywords):\n",
    "            Mi[j] = InformationRetrievalIndexedFrequency.query_count_match_abstract(keyword, text.lower())\n",
    "\n",
    "        return Mi\n",
    "\n",
    "    # Part of task C-5\n",
    "    def calculate_keyword_frequencies(self) -> list[list[int]]:\n",
    "        M = []\n",
    "        for article in self.metadata:\n",
    "            M.append(self.calculate_keyword_frequencies_in_text(article['abstract']))\n",
    "\n",
    "        return M\n",
    "\n",
    "\n",
    "# irie = InformationRetrievalIndexedFrequency(metadata=metadata)\n",
    "irie = InformationRetrievalIndexedFrequency(metadata=metadata, keyword_separator = [',', ' '])\n",
    "irie.build_inverted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9180327868852459, 0.9016393442622951, 0.9836065573770492, 0.8688524590163934, 0.9016393442622951, 0.9672131147540983, 0.8688524590163934, 0.9180327868852459, 0.8852459016393442, 0.9344262295081968, 0.9180327868852459, 0.8852459016393442, 0.9016393442622951, 1.0, 0.9836065573770492, 0.8852459016393442, 0.8524590163934427, 0.8688524590163934, 0.9672131147540983, 0.9344262295081968, 0.8852459016393442, 0.9180327868852459, 0.9508196721311475, 0.9180327868852459, 0.8688524590163934, 0.9672131147540983, 0.9180327868852459, 1.0, 0.7704918032786885, 0.8688524590163934, 0.9016393442622951, 0.9016393442622951, 0.9836065573770492, 0.8360655737704918, 0.8688524590163934, 0.9016393442622951, 0.9180327868852459, 0.9344262295081968, 0.9508196721311475, 0.9016393442622951, 0.9672131147540983, 0.9344262295081968, 0.9508196721311475, 0.9344262295081968, 0.9344262295081968, 0.9344262295081968, 0.8032786885245902, 0.8852459016393442, 0.9344262295081968, 0.8852459016393442, 0.819672131147541, 0.9344262295081968, 0.9180327868852459, 0.9016393442622951, 0.9672131147540983, 0.8852459016393442, 0.8360655737704918, 0.819672131147541, 0.8360655737704918, 0.8688524590163934, 0.8360655737704918, 0.8688524590163934, 0.8524590163934427, 0.9344262295081968, 0.9016393442622951, 0.9508196721311475, 0.8688524590163934, 0.9180327868852459, 0.9016393442622951, 0.9344262295081968, 0.8852459016393442, 0.9180327868852459, 0.819672131147541, 0.9344262295081968, 0.9508196721311475, 0.8852459016393442, 0.9016393442622951, 0.8852459016393442, 0.9016393442622951, 0.9016393442622951, 0.8688524590163934, 0.9672131147540983, 0.9672131147540983, 0.9508196721311475, 0.9344262295081968, 0.8688524590163934, 0.9344262295081968, 0.9672131147540983, 0.9180327868852459, 0.9016393442622951, 0.9344262295081968, 0.819672131147541, 0.8852459016393442, 0.9016393442622951, 0.8032786885245902, 0.819672131147541, 0.9016393442622951, 0.819672131147541, 0.9180327868852459, 0.8852459016393442, 0.9180327868852459, 0.9180327868852459, 0.9508196721311475, 0.8688524590163934, 0.8688524590163934, 0.819672131147541, 0.9016393442622951, 0.9180327868852459, 0.9508196721311475, 0.9344262295081968, 0.9016393442622951, 0.8688524590163934, 0.9180327868852459, 0.9016393442622951, 0.8524590163934427, 0.9180327868852459, 0.8852459016393442, 0.9672131147540983, 0.9508196721311475, 0.9508196721311475, 0.8688524590163934, 0.8852459016393442, 0.8852459016393442, 0.9016393442622951, 0.8852459016393442, 0.9344262295081968, 0.9672131147540983, 0.9180327868852459, 0.9344262295081968, 0.8688524590163934, 0.8360655737704918, 0.8032786885245902, 0.9344262295081968, 0.8688524590163934, 0.8852459016393442, 0.8688524590163934, 0.8688524590163934, 0.8688524590163934, 0.7868852459016393, 0.9672131147540983, 0.9508196721311475, 0.8852459016393442, 0.9508196721311475, 0.8360655737704918, 0.9016393442622951, 0.9508196721311475, 1.0, 0.9016393442622951, 0.9016393442622951, 0.8852459016393442, 0.9344262295081968, 0.8688524590163934, 0.8524590163934427, 0.9180327868852459, 0.9672131147540983, 0.9016393442622951, 0.9508196721311475, 1.0, 0.8688524590163934, 0.9180327868852459, 0.9180327868852459, 0.8688524590163934, 0.9180327868852459, 0.9508196721311475, 0.9344262295081968, 0.9016393442622951, 0.9180327868852459, 0.9180327868852459, 0.9672131147540983, 0.8524590163934427, 0.8360655737704918, 0.9672131147540983, 0.9508196721311475, 0.9508196721311475, 0.9508196721311475, 0.9344262295081968, 0.9180327868852459, 0.9180327868852459, 0.9016393442622951, 0.8852459016393442, 0.9344262295081968, 0.9344262295081968, 0.9508196721311475, 0.9508196721311475, 0.8852459016393442, 0.8688524590163934, 0.9180327868852459, 0.9344262295081968, 0.9672131147540983, 0.8852459016393442, 0.8688524590163934, 0.9016393442622951, 0.8852459016393442, 0.9016393442622951, 0.8524590163934427, 0.9344262295081968, 0.9508196721311475, 0.9672131147540983, 0.9508196721311475, 0.9344262295081968, 0.8688524590163934, 0.9016393442622951, 0.9672131147540983, 0.8852459016393442, 0.9180327868852459, 0.8852459016393442, 0.9016393442622951, 0.9672131147540983, 0.8360655737704918, 0.9016393442622951, 0.9508196721311475, 0.9344262295081968, 0.8688524590163934, 0.8688524590163934, 0.9672131147540983, 0.9508196721311475, 0.7377049180327868, 0.8852459016393442, 0.8360655737704918, 0.9016393442622951, 0.9016393442622951, 0.8524590163934427, 0.8852459016393442, 0.9180327868852459, 0.8852459016393442, 0.8852459016393442, 0.8852459016393442, 0.8524590163934427, 0.9016393442622951, 0.8524590163934427, 0.9180327868852459, 0.9180327868852459, 0.9508196721311475, 0.9344262295081968, 0.9180327868852459, 0.8852459016393442, 0.9016393442622951, 0.8852459016393442, 0.8852459016393442, 0.9016393442622951, 0.9180327868852459, 0.8524590163934427, 0.9016393442622951, 0.9016393442622951, 0.819672131147541, 0.9180327868852459, 0.9672131147540983, 0.9016393442622951, 0.9180327868852459, 0.8852459016393442, 0.9344262295081968, 0.9016393442622951, 0.9180327868852459, 0.9016393442622951, 0.9180327868852459, 0.8688524590163934, 0.9344262295081968, 0.9344262295081968, 0.9672131147540983, 0.9344262295081968, 0.9016393442622951, 0.9016393442622951, 0.8852459016393442, 0.8852459016393442, 0.9672131147540983, 0.8032786885245902, 0.9016393442622951, 0.9508196721311475, 0.9180327868852459, 0.9344262295081968, 0.8852459016393442, 0.7377049180327868, 0.8852459016393442, 0.9180327868852459, 0.9016393442622951, 0.8032786885245902, 0.8688524590163934, 0.9508196721311475, 0.9016393442622951, 0.9180327868852459, 0.8852459016393442, 0.9016393442622951, 0.9672131147540983, 0.8360655737704918, 0.9344262295081968, 0.9508196721311475, 0.9508196721311475, 0.9180327868852459, 0.9016393442622951, 0.8852459016393442, 0.9180327868852459, 0.9344262295081968, 0.9180327868852459, 0.9180327868852459, 0.9672131147540983, 0.9508196721311475, 0.8852459016393442, 0.9180327868852459, 0.9016393442622951, 0.8688524590163934, 0.9016393442622951, 0.9344262295081968, 0.9508196721311475]\n"
     ]
    }
   ],
   "source": [
    "X = irie.compute_similarity_with_query(query)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C-3: Repeat C-2 when considering the Edit similarity between each keyword K in the metadata and the title of the document containing K (considering the same rule applies when the title is made of several tokens so that the maximum similarity over all token is computed, and also when the keyword is found in more than one document). Save the result as new array Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9482758620689655, 1.0, 0.9433962264150944, 0.8918918918918919, 1.0, 0.9864864864864865, 0.9375, 0.9324324324324325, 0.9655172413793104, 1.0, 1.0, 0.9324324324324325, 1.0, 0.972972972972973, 0.972972972972973, 0.9589041095890412, 1.0, 0.9464285714285714, 0.9864864864864865, 0.9090909090909091, 0.94, 1.0, 0.9459459459459459, 0.9508196721311475, 1.0, 1.0, 0.9137931034482758, 1.0, 0.9215686274509804, 0.9673202614379085, 0.961038961038961, 1.0, 0.9864864864864865, 0.9193548387096774, 0.9333333333333333, 0.9054054054054054, 1.0, 0.9459459459459459, 0.9594594594594594, 0.8909090909090909, 0.972972972972973, 0.9918032786885246, 0.9594594594594594, 0.9583333333333334, 1.0, 0.9663865546218487, 0.8777777777777778, 0.9916666666666667, 1.0, 1.0, 1.0, 0.9748427672955975, 1.0, 1.0, 0.972972972972973, 0.9054054054054054, 0.9166666666666666, 0.875, 0.8783783783783784, 0.7272727272727273, 0.935483870967742, 0.8918918918918919, 0.8783783783783784, 0.9459459459459459, 0.9178082191780822, 0.9883720930232558, 0.9186046511627907, 0.9459459459459459, 0.9189189189189189, 1.0, 1.0, 0.9516129032258065, 0.8648648648648649, 0.9365079365079365, 0.972972972972973, 0.86, 0.9359999999999999, 0.9914529914529915, 1.0, 1.0, 0.9054054054054054, 0.972972972972973, 1.0, 0.9915254237288136, 0.9937106918238994, 0.9477124183006536, 1.0, 0.972972972972973, 0.9459459459459459, 0.9916666666666667, 1.0, 0.9748427672955975, 1.0, 1.0, 0.8378378378378378, 0.8513513513513513, 0.9189189189189189, 0.6666666666666667, 0.9685534591194969, 0.9481481481481482, 0.962962962962963, 1.0, 0.9594594594594594, 0.9913793103448276, 0.8904109589041096, 0.9333333333333333, 1.0, 0.9, 0.9726027397260274, 0.9, 0.962962962962963, 0.949685534591195, 1.0, 0.9594594594594594, 0.7272727272727273, 0.967741935483871, 0.95, 0.972972972972973, 0.9568965517241379, 0.972972972972973, 0.8918918918918919, 0.9622641509433962, 0.9333333333333333, 0.9189189189189189, 1.0, 0.9748427672955975, 0.972972972972973, 1.0, 0.9459459459459459, 0.9349593495934959, 1.0, 0.8918918918918919, 1.0, 0.974025974025974, 0.9298245614035088, 0.922077922077922, 0.943089430894309, 0.9333333333333333, 0.8, 0.9821428571428571, 0.9596774193548387, 0.9054054054054054, 0.9594594594594594, 0.8701298701298701, 0.9178082191780822, 0.972972972972973, 1.0, 0.9482758620689655, 1.0, 0.8918918918918919, 0.9459459459459459, 1.0, 0.8200000000000001, 0.9324324324324325, 0.972972972972973, 1.0, 0.972972972972973, 1.0, 1.0, 0.9324324324324325, 0.9324324324324325, 0.9469026548672567, 0.9324324324324325, 0.972972972972973, 0.9459459459459459, 1.0, 0.9748427672955975, 0.9315068493150684, 1.0, 0.9175257731958762, 0.987012987012987, 0.972972972972973, 0.9594594594594594, 0.9594594594594594, 0.9594594594594594, 0.9919354838709677, 1.0, 0.9444444444444444, 1.0, 1.0, 0.9459459459459459, 1.0, 0.9741379310344828, 0.9594594594594594, 0.9054054054054054, 0.9054054054054054, 0.9324324324324325, 0.9459459459459459, 0.972972972972973, 0.8918918918918919, 0.8783783783783784, 0.9913793103448276, 0.9054054054054054, 1.0, 0.967479674796748, 0.9459459459459459, 0.9594594594594594, 0.972972972972973, 0.9594594594594594, 0.9459459459459459, 0.925, 0.9741379310344828, 0.972972972972973, 0.9054054054054054, 0.9324324324324325, 0.9426229508196722, 1.0, 0.972972972972973, 1.0, 0.9189189189189189, 0.972972972972973, 0.9594594594594594, 0.9054054054054054, 0.8904109589041096, 0.972972972972973, 0.9594594594594594, 1.0, 0.9555555555555556, 1.0, 1.0, 0.9278350515463918, 0.8783783783783784, 0.8727272727272728, 0.9758064516129032, 0.9846153846153847, 0.9222222222222223, 0.9459459459459459, 0.9054054054054054, 0.9864864864864865, 1.0, 0.9315068493150684, 1.0, 0.9324324324324325, 0.9459459459459459, 0.991869918699187, 1.0, 1.0, 0.9516129032258065, 0.911504424778761, 0.967741935483871, 0.8181818181818181, 0.9622641509433962, 0.9459459459459459, 0.9910714285714286, 0.8, 1.0, 0.972972972972973, 0.88, 0.98, 0.9481481481481482, 0.9748427672955975, 1.0, 0.9324324324324325, 0.959349593495935, 0.9324324324324325, 0.9590163934426229, 0.9459459459459459, 1.0, 0.972972972972973, 0.9869281045751634, 0.9054054054054054, 0.9459459459459459, 0.8615384615384616, 0.9435483870967742, 0.972972972972973, 0.8356164383561644, 0.9189189189189189, 0.9594594594594594, 0.9381443298969072, 0.9459459459459459, 0.9916666666666667, 0.9193548387096774, 1.0, 0.9375, 0.9596774193548387, 1.0, 0.9359999999999999, 0.9594594594594594, 0.9661016949152542, 0.935064935064935, 1.0, 1.0, 0.9846153846153847, 0.8648648648648649, 0.9459459459459459, 0.972972972972973, 0.9324324324324325, 0.975, 0.88, 0.9054054054054054, 0.9459459459459459, 0.9459459459459459, 0.9459459459459459, 0.962962962962963, 1.0, 0.9741379310344828, 0.9607843137254902, 0.959349593495935, 0.967741935483871, 0.9, 1.0, 1.0, 0.9594594594594594]\n"
     ]
    }
   ],
   "source": [
    "Y = irie.compute_similarity_keyword_with_title()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C-4: Use appropriate script to compute the Person correlation coefficient between X and Y and the associated p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.42031665728783296\n",
      "P-value: 2.1206328784571897e-14\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "correlation, p_value = pearsonr(X, Y)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Pearson correlation coefficient: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaskC-5: Now we want to exploit the content of the abstract text. Initially, we want to test the extent to which the keywords of the metadata are part of the tokens of the abstract. Write a script that calculates for each keyword K, the frequency of K in the corresponding abstract. Save the result as a matrix M (n x m) where n stands for the number of documents and m the number of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 13, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 11, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 12, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 18, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 5, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, 0, 1, 0, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 9, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 10, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 13, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 3, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3], [0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 11, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 11, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 3, 0, 1], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]]\n"
     ]
    }
   ],
   "source": [
    "M = irie.calculate_keyword_frequencies()\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C-6: Consider the query T used originally to construct the database, write a script that uses Boolean model to find out relevant document, utilizing matrix M (we assume the vocabulary is made only of keywords in KK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant documents matrix: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Relevant documents: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "keyword_frequencies_in_query = irie.calculate_keyword_frequencies_in_text(query)\n",
    "\n",
    "relevant_documents = [0] * len(irie.metadata)\n",
    "for i, document_frequencies in enumerate(M):\n",
    "    for j, keyword_frequency in enumerate(document_frequencies):\n",
    "        if keyword_frequency > 0 and keyword_frequencies_in_query[j] > 0:\n",
    "            relevant_documents[i] = 1\n",
    "            break\n",
    "\n",
    "relevant_document_indices = [i for i, val in enumerate(relevant_documents) if val == 1]\n",
    "\n",
    "print(f\"Relevant documents matrix: {relevant_documents}\")\n",
    "print(f\"Relevant documents: {relevant_document_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C-7: Repeat C-6 when you use tf-idf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant documents matrix: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Relevant documents: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class InformationRetrievalIndexedTFIDF(InformationRetrievalIndexedFrequency):\n",
    "    \"\"\"\n",
    "    Because TF-IDF does not support multiple word keyword, we have to rebuild inverted index again\n",
    "    \"\"\"\n",
    "    def __init__(self, metadata: list[dict[str, str]] = [], keyword_separator: list[str] = [',', ' ']) -> None:\n",
    "        super().__init__(metadata, keyword_separator)\n",
    "        self.tf_idf_matrix = []\n",
    "        self.idf = []\n",
    "\n",
    "    def compute_tf(self, abstract: str, keywords: list[str]) -> list[float]:\n",
    "        \"\"\"Compute term frequency for each keyword in the abstract.\"\"\"\n",
    "        abstract_words = abstract.lower().split()\n",
    "        total_words = len(abstract_words)\n",
    "\n",
    "        tf = []\n",
    "        for keyword in keywords:\n",
    "            keyword_count = abstract_words.count(keyword.lower())\n",
    "            tf.append(keyword_count / total_words if total_words > 0 else 0)\n",
    "\n",
    "        return tf\n",
    "\n",
    "    def compute_idf(self, documents: list[str], keywords: list[str]) -> list[float]:\n",
    "        \"\"\"Compute inverse document frequency for each keyword.\"\"\"\n",
    "        num_documents = len(documents)\n",
    "        idf = []\n",
    "        for keyword in keywords:\n",
    "            # Count the number of documents that contain the keyword\n",
    "            doc_count = sum(1 for doc in documents if keyword.lower() in doc.lower().split())\n",
    "            idf_value = math.log(num_documents / (1 + doc_count))  # Add 1 to avoid division by zero\n",
    "            idf.append(idf_value)\n",
    "\n",
    "        return idf\n",
    "\n",
    "    def build_tf_idf_matrix(self) -> None:\n",
    "        \"\"\"Build the TF-IDF matrix using the TF and IDF for each keyword and document.\"\"\"\n",
    "        documents = [article['abstract'] for article in self.metadata]\n",
    "\n",
    "        # Calculate IDF for all keywords\n",
    "        self.idf = self.compute_idf(documents, self.indexed_keywords)\n",
    "\n",
    "        # Calculate TF and TF-IDF for each document\n",
    "        self.tf_idf_matrix = []\n",
    "        for article in self.metadata:\n",
    "            tf = self.compute_tf(article['abstract'], self.indexed_keywords)\n",
    "            tf_idf = [tf_val * idf_val for tf_val, idf_val in zip(tf, self.idf)]\n",
    "            self.tf_idf_matrix.append(tf_idf)\n",
    "\n",
    "    def calculate_tfidf_for_query(self, query: str) -> list[float]:\n",
    "        \"\"\"Compute TF-IDF scores for the query.\"\"\"\n",
    "        query_tf = self.compute_tf(query, self.indexed_keywords)\n",
    "        query_tfidf = [tf_val * idf_val for tf_val, idf_val in zip(query_tf, self.idf)]\n",
    "        return query_tfidf\n",
    "\n",
    "    def find_relevant_documents(self, query: str) -> list[int]:\n",
    "        \"\"\"Find relevant documents using the TF-IDF model.\"\"\"\n",
    "        query_tfidf = self.calculate_tfidf_for_query(query)\n",
    "        relevant_documents = [0] * len(self.metadata)\n",
    "\n",
    "        for i, doc_tfidf in enumerate(self.tf_idf_matrix):\n",
    "            # Check if any keyword in the query has a TF-IDF score > 0 in the document\n",
    "            for j, tfidf_score in enumerate(doc_tfidf):\n",
    "                if tfidf_score > 0 and query_tfidf[j] > 0:\n",
    "                    relevant_documents[i] = 1\n",
    "                    break\n",
    "\n",
    "        return relevant_documents\n",
    "\n",
    "\n",
    "irie_tfidf = InformationRetrievalIndexedTFIDF(metadata=metadata)\n",
    "irie_tfidf.build_inverted_index()\n",
    "irie_tfidf.build_tf_idf_matrix()\n",
    "relevant_documents = irie_tfidf.find_relevant_documents(query)\n",
    "relevant_document_indices = [i for i, val in enumerate(relevant_documents) if val == 1]\n",
    "\n",
    "print(f\"Relevant documents matrix: {relevant_documents}\")\n",
    "print(f\"Relevant documents: {relevant_document_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C-8: We want to test the consistency between title of each document and the abstract text. Write a script that computes for each title, the FuzzyWuzzy score between the title and the corresponding abstract. Save the result in an array Z. We assume that the matching is accepted if the Fuzzy-wuzzy score is greater than 80%. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy[speedup] in ./.venv312/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-levenshtein>=0.12 in ./.venv312/lib/python3.12/site-packages (from fuzzywuzzy[speedup]) (0.26.0)\n",
      "Requirement already satisfied: Levenshtein==0.26.0 in ./.venv312/lib/python3.12/site-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.26.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in ./.venv312/lib/python3.12/site-packages (from Levenshtein==0.26.0->python-levenshtein>=0.12->fuzzywuzzy[speedup]) (3.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fuzzywuzzy[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy scores Z: [10, 10, 12, 16, 18, 9, 8, 12, 5, 14, 12, 8, 13, 24, 15, 10, 15, 3, 12, 15, 15, 20, 26, 16, 11, 14, 13, 13, 8, 20]\n",
      "Accepted documents matrix: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Relevant documents: []\n",
      "Acceptance rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "class InformationRetrievalConsistency:\n",
    "    def __init__(self, metadata: list[dict[str, str]]) -> None:\n",
    "        self.metadata = metadata\n",
    "        self.scores = []\n",
    "\n",
    "    def compute_fuzzy_scores(self) -> list[float]:\n",
    "        \"\"\"\n",
    "        Compute the FuzzyWuzzy score between the title and abstract of each document.\n",
    "        \"\"\"\n",
    "        self.scores = []\n",
    "        for article in self.metadata:\n",
    "            # Compute the FuzzyWuzzy score (Levenshtein distance-based similarity ratio)\n",
    "            score = fuzz.ratio(article['title'], article['abstract'])\n",
    "            self.scores.append(score)\n",
    "\n",
    "        return self.scores\n",
    "\n",
    "    def check_consistency(self, threshold: int = 80) -> tuple[list[bool], float]:\n",
    "        \"\"\"\n",
    "        Check consistency between title and abstract based on a threshold fuzzy score.\n",
    "\n",
    "        Args:\n",
    "            threshold (int): Minimum score to accept the match (default is 80%).\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[bool], float]: List of booleans indicating if the match is accepted for each document\n",
    "                                      and the percentage of accepted documents.\n",
    "        \"\"\"\n",
    "        if len(self.scores) == 0:\n",
    "            self.scores = self.compute_fuzzy_scores()\n",
    "\n",
    "        accepted_documents = [score > threshold for score in self.scores]\n",
    "        acceptance_rate = sum(accepted_documents) / len(self.scores) * 100\n",
    "\n",
    "        return accepted_documents, acceptance_rate\n",
    "\n",
    "consistency_checker = InformationRetrievalConsistency(metadata)\n",
    "Z = consistency_checker.compute_fuzzy_scores()\n",
    "accepted_documents, acceptance_rate = consistency_checker.check_consistency()\n",
    "\n",
    "print(f\"Fuzzy scores Z: {Z}\")\n",
    "print(f\"Accepted documents matrix: {accepted_documents}\")\n",
    "relevant_document_indices = [i for i, val in enumerate(accepted_documents) if val == 1]\n",
    "print(f\"Relevant documents: {relevant_document_indices}\")\n",
    "print(f\"Acceptance rate: {acceptance_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My comment on the task**: Well, the result is quite expected, since the title is quite short - around 10 words, compare to the abstract, which can be 1 paragraph long to 1 A4 page, with probably in range of 250 words. That's 10 - 50x size. So it can be expected that Title cannot get to 80% Fuzzywuzzy score, because by default, FuzzyWuzzy `ratio` score use Levenshtein distance.\n",
    "\n",
    "There are also different Fuzzywuzzy methods, such as `partial_ratio`, `token_sort_ratio` and `token_set_ratio`. In cases like this, it might be well suited to use `token_sort_ratio` and decrease the threshold, because this will help checking for similarity between 2 strings. Aisde from that, it might be better to use other sematic similarity methods with text understanding capability in order to do the task like this, because it's better to capture the underlying meaning of the text, rather than focusing on exact word matches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
