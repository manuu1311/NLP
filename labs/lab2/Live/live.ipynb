{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     D:/misc/Projects/Python/NLP/misc...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     D:/misc/Projects/Python/NLP/misc...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     D:/misc/Projects/Python/NLP/misc...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download tokenizer\n",
    "path='D:/misc/Projects/Python/NLP/misc'\n",
    "nltk.download('punkt_tab',download_dir=path)\n",
    "nltk.download('wordnet',download_dir=path)\n",
    "nltk.download('omw-1.4',download_dir=path)\n",
    "nltk.data.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['advancements and challenges of fuel cell integration in electric vehicles: a comprehensive analysis artificial intelligence',\n",
       "  ['manpreet singh',\n",
       "   ' manish kumar singla',\n",
       "   ' murodbek safaraliev',\n",
       "   ' kulwinder singh',\n",
       "   ' ismoil odinaev',\n",
       "   ' amir abdel menaem'],\n",
       "  'fuel cell technology emerges as a promising green solution, offering mitigation to global warming, air pollution, and energy crises. this eco-friendly approach is witnessing a surge in adoption within the automotive sector, with fuel cell buses, cars, scooters, forklifts, and more, becoming increasingly prevalent. the automobile industry has been rapidly advancing fuel cell technology, inching closer to the commercialization of fuel cell vehicles. as various technical hurdles are surmounted and costs are reduced, fuel cell vehicles are poised to become a competitive force in the automobile market, presenting an excellent solution for environmental sustainability and energy efficiency. this review paper delves into the fundamentals of fuel cells, their characteristics, and their applications in the automotive realm, exploring their prospects in comparison to traditional technologies. furthermore, it sheds light on the existing research and industrial developments in hydrogen and fuel cell technologies. additionally, a comprehensive comparison is provided between various fuel cell cars that have already been commercialized, enabling readers to understand the current market landscape. the review also analyses the advantages and challenges associated with fuel cell technology, offering insights into its future development trajectory. through this comprehensive exploration, readers can gain a deeper understanding of fuel cell technology and its potential in revolutionizing the automotive industry.',\n",
       "  ['fuel cell',\n",
       "   ' hydrogen',\n",
       "   ' electric vehicles',\n",
       "   ' renewable energy',\n",
       "   'sustainability']],\n",
       " ['the kidney precision medicine project and single-cell biology of the injured proximal tubule artificial intelligence',\n",
       "  ['danielle janosevic',\n",
       "   ' thomas de luca ',\n",
       "   ' kidney precision medicine project',\n",
       "   ' michael t. eadon'],\n",
       "  'single-cell rna sequencing (scrna-seq) has led to major advances in our understanding of proximal tubule subtypes in health and disease. the proximal tubule serves essential functions in overall homeostasis, but pathologic or physiologic perturbations can affect its transcriptomic signature and corresponding tasks. these alterations in proximal tubular cells are often described within a scrna-seq atlas as cell states, which are pathophysiologic subclassifications based on molecular and morphological changes in a cellâ€™s response to that injury compared to its native state. this review describes the major cell states defined in the kidney precision medicine projectâ€™s (kpmp) scrna-seq atlas. the review then identifies the overlap between kpmp and other seminal works which may use different nomenclature or cluster proximal tubule cells at different resolutions to define cell state subtypes. the goal is for the reader to understand the key transcriptomic markers of important cellular injury and regeneration processes across this highly dynamic and evolving field.',\n",
       "  ['chronic kidney disease',\n",
       "   ' acute kidney injury',\n",
       "   ' failed repair',\n",
       "   ' adaptive cell state',\n",
       "   ' maladaptive repair']],\n",
       " ['introducing phosphoric acid to fluorinated polyimide towards high performance laser induced graphene electrodes for high artificial intelligence energy micro-supercapacitors',\n",
       "  ['yi zhao',\n",
       "   ' wenjing qiao',\n",
       "   ' haozhe wang',\n",
       "   ' yangyang xie',\n",
       "   ' botao teng',\n",
       "   ' jiongru li',\n",
       "   ' yunlong sun',\n",
       "   ' abdullah saad alsubaie',\n",
       "   ' tong wan',\n",
       "   ' salah m. el-bahy',\n",
       "   ' dapeng cui',\n",
       "   ' zeinhom m. el-bahy',\n",
       "   ' jing zhang',\n",
       "   ' huige wei',\n",
       "   ' zhanhu guo'],\n",
       "  'micro-supercapacitors (mscs) have wide application prospects in microelectronic fields such as wearable electronics due to merits of stable performance, high safety and easy integration. however, the relatively low energy density of mscs limits their practical application. in this context, phosphorus and fluorine co-doped laser-induced graphene (fp-lig) microelectrodes were fabricated from fluorinated polyimide containing phosphoric acid by laser direct writing (ldw) method. the introduced phosphoric acid slows down the decomposition of -cf3 during the ldw process, resulting in much more ordered and stable pores; meanwhile, phosphorus entered the graphene lattice to replace some carbon atoms, forming a c3po structure, which not only stabilizes the interface between the electrode and the electrolyte and therefore achieves an enlarged working potential of 1.4 v, but also increases the wettability of the electrode. using fp-3-lig microelectrodes and pva/h2so4 as the gel electrolyte, the assembled fp-3-msc demonstrates significantly enhanced energy density, delivering an energy density of 10.40 î¼wh cm-2 (@0.09 ma cm-2), 2.7 times that of f-msc and 346.7 times that of msc. fp-3-msc has excellent cyclic stability, displaying an areal capacitance retention rate of above 90 % after 10,000 long cycles. in addition, fp-3-msc demonstrates excellent flexibility, indicating promising potential in the field of flexible wearable electronics.',\n",
       "  ['micro-supercapacitors',\n",
       "   ' laser-induced graphene',\n",
       "   ' fluorinated polyimide',\n",
       "   ' phosphorus doping']],\n",
       " ['the performance of artificial intelligence-based large language models on ophthalmology-related artificial intelligence questions in swedish proficiency test for medicine: chatgpt-4 omni vs gemini 1.5 pro',\n",
       "  ['mehmet cem sabaner',\n",
       "   ' arzu seyhan karatepe hashas',\n",
       "   ' kemal mert mutibayraktaroglu',\n",
       "   ' zubeyir yozgat',\n",
       "   ' oliver niels klefter',\n",
       "   ' yousif subhi'],\n",
       "  'to compare the interpretation and response context of two commonly used artificial intelligence (ai)-based large language model (llm) platforms to ophthalmology-related multiple choice questions (mcqs) in the swedish proficiency test for medicine (\"kunskapsprov fã¶r lã¤kare\") exams.',\n",
       "  ['artificial intelligence',\n",
       "   ' chatgpt-4',\n",
       "   ' omnie-learning',\n",
       "   ' gemini 1.5',\n",
       "   ' prolarge language model',\n",
       "   ' medical education',\n",
       "   ' ophthalmology']],\n",
       " ['evaluation of probability distribution methods for flood frequency analysis artificial intelligence in the jhelum basin of north-western himalayas, india',\n",
       "  ['asif iqbal shah', ' nibedita das '],\n",
       "  \"the kashmir valley has frequently endured devastating floods, presenting significant challenges for flood management due to unpredictable flood magnitudes and limited pre-disaster preparedness. a major difficulty in addressing these challenges is the lack of reliable flood frequency analysis (ffa) for effective planning and mitigation. this study seeks to overcome these issues by employing a rigorous quantitative analysis of annual peak discharge data over a 51-year period (1971-2021). one key challenge was the presence of low outliers, which could compromise the integrity of the data. to address this, the multiple grubbs-beck test was applied to remove these outliers before conducting ffa. the study's original achievement lies in its application of multiple distribution models which include gumbel (ev1), generalized extreme variations (gev), log-normal, log pearson iii (lp iii), gamma and normal distribution. goodness-of-fit tests, including kolmogorov-smirnov (ks), anderson-darling (ad), and chi-square at the 5% significance level, along with visualization techniques such as probability plots (pp), quantile plots (qq), and probabilistic distribution (pd) graphs, were used to identify the most suitable distribution methods. the log pearson type iii (lp-iii) was identified as the best fit for the sangam gauge site (upper jhelum), the gamma distribution for ram munshibagh (middle jhelum), and the generalized extreme value (gev) and lp-iii for asham (lower jhelum). for sangam, the estimated discharges for 2, 5, 10, 50, 100, 150, 200, and 250-year return periods were 549.63, 1028.43, 1471.34, 2907.64, 3758.92, 4338.61, 4790.99, and 5167.23 cumecs, respectively, using lp-iii. for ram munshibagh, the discharges were 602.13, 911.03, 1107.04, 1512.12, 1674.35, 1767.0, 1831.87, and 1881.74 cumecs using the gamma distribution. for asham, the discharges were 685.8, 998.0, 1193.3, 1593.2, 1750.6, 1839.4, 1901.0, and 1948.0 cumecs using the gev distribution. the findings indicate that the jhelum river cannot accommodate excess discharge for return periods of 5 years or more, underscoring the need for enhanced flood management strategies.\",\n",
       "  ['climate change',\n",
       "   ' extreme value theory',\n",
       "   ' artificial intelligence',\n",
       "   ' flood frequency analysis',\n",
       "   ' flood hazard',\n",
       "   ' goodness of fit',\n",
       "   ' hydrological modeling',\n",
       "   ' kashmir valley']]]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read files\n",
    "with open('prova.txt','r') as f:\n",
    "    data=f.read().lower().strip()\n",
    "\n",
    "punctuation=['\"','!','.',',',\"'\",'(',')',';','``',\"''\",'?','_',':','-']\n",
    "punctuation=[]\n",
    "\n",
    "data=data.split('-next-')\n",
    "data=[ab.strip().split('\\n')[:] for ab in data][:]\n",
    "\n",
    "#remove punctuation from abstracts\n",
    "for punct in punctuation:\n",
    "    for i in range(len(data)):\n",
    "        data[i][2]=data[i][2].replace(punct,'')\n",
    "\n",
    "#split authors and keywords\n",
    "for i in range(len(data)):\n",
    "    data[i][1]=data[i][1].split(',')\n",
    "    data[i][3]=data[i][3].split(',')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'fuel cell': [0],\n",
       "             'hydrogen': [0],\n",
       "             'electric vehicles': [0],\n",
       "             'renewable energy': [0],\n",
       "             'sustainability': [0],\n",
       "             'chronic kidney disease': [1],\n",
       "             'acute kidney injury': [1],\n",
       "             'failed repair': [1],\n",
       "             'adaptive cell state': [1],\n",
       "             'maladaptive repair': [1],\n",
       "             'micro-supercapacitors': [2],\n",
       "             'laser-induced graphene': [2],\n",
       "             'fluorinated polyimide': [2],\n",
       "             'phosphorus doping': [2],\n",
       "             'artificial intelligence': [3, 4],\n",
       "             'chatgpt-4': [3],\n",
       "             'omnie-learning': [3],\n",
       "             'gemini 1.5': [3],\n",
       "             'prolarge language model': [3],\n",
       "             'medical education': [3],\n",
       "             'ophthalmology': [3],\n",
       "             'climate change': [4],\n",
       "             'extreme value theory': [4],\n",
       "             'flood frequency analysis': [4],\n",
       "             'flood hazard': [4],\n",
       "             'goodness of fit': [4],\n",
       "             'hydrological modeling': [4],\n",
       "             'kashmir valley': [4]})"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom dictionary class to conveniently add new items\n",
    "class inverted_index_dict:\n",
    "    def __init__(self) -> None:\n",
    "        self.dict=defaultdict()\n",
    "        \n",
    "    def add_item(self,token,doc):\n",
    "        if self.dict.__contains__(token):\n",
    "            if doc not in self.dict[token]:\n",
    "                self.dict[token].append(doc)\n",
    "        else:\n",
    "            self.dict[token]=[doc]\n",
    "    \n",
    "inverted_index=inverted_index_dict()\n",
    "\n",
    "for j,ab in enumerate(data):\n",
    "    for i,kw in enumerate(ab[3]):\n",
    "        inverted_index.add_item(kw.strip(),j)\n",
    "\n",
    "inverted_index.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acute kidney injury', [1]),\n",
       " ('adaptive cell state', [1]),\n",
       " ('artificial intelligence', [3, 4]),\n",
       " ('chatgpt-4', [3]),\n",
       " ('chronic kidney disease', [1]),\n",
       " ('climate change', [4]),\n",
       " ('electric vehicles', [0]),\n",
       " ('extreme value theory', [4]),\n",
       " ('failed repair', [1]),\n",
       " ('flood frequency analysis', [4]),\n",
       " ('flood hazard', [4]),\n",
       " ('fluorinated polyimide', [2]),\n",
       " ('fuel cell', [0]),\n",
       " ('gemini 1.5', [3]),\n",
       " ('goodness of fit', [4]),\n",
       " ('hydrogen', [0]),\n",
       " ('hydrological modeling', [4]),\n",
       " ('kashmir valley', [4]),\n",
       " ('laser-induced graphene', [2]),\n",
       " ('maladaptive repair', [1]),\n",
       " ('medical education', [3]),\n",
       " ('micro-supercapacitors', [2]),\n",
       " ('omnie-learning', [3]),\n",
       " ('ophthalmology', [3]),\n",
       " ('phosphorus doping', [2]),\n",
       " ('prolarge language model', [3]),\n",
       " ('renewable energy', [0]),\n",
       " ('sustainability', [0])]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ordering\n",
    "kk=list(inverted_index.dict.items())\n",
    "kk=sorted(kk,key=lambda x: x[0])\n",
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T: artificial intelligence\n",
      "micro-supercapacitors: 18\n",
      "sustainability: 10\n",
      "energy: 9\n",
      "laser-induced: 9\n",
      "analysis: 8\n",
      "phosphorus: 10\n",
      "kashmir: 9\n",
      "ophthalmology: 11\n",
      "renewable: 9\n",
      "value: 10\n",
      "gemini: 8\n",
      "electric: 9\n",
      "fuel: 8\n",
      "hydrological: 8\n",
      "state: 9\n",
      "model: 9\n",
      "disease: 9\n",
      "fluorinated: 10\n",
      "climate: 9\n",
      "hydrogen: 9\n",
      "theory: 9\n",
      "maladaptive: 9\n",
      "doping: 9\n",
      "omnie-learning: 10\n",
      "intelligence: 0\n",
      "flood: 9\n",
      "kidney: 9\n",
      "polyimide: 8\n",
      "valley: 9\n",
      "adaptive: 8\n",
      "modeling: 8\n",
      "artificial: 0\n",
      "language: 9\n",
      "medical: 6\n",
      "goodness: 10\n",
      "education: 9\n",
      "acute: 8\n",
      "fit: 8\n",
      "1.5: 10\n",
      "cell: 8\n",
      "injury: 9\n",
      "graphene: 9\n",
      "extreme: 9\n",
      "chronic: 8\n",
      "repair: 8\n",
      "failed: 9\n",
      "vehicles: 8\n",
      "change: 10\n",
      "frequency: 8\n",
      "hazard: 10\n",
      "prolarge: 9\n",
      "chatgpt-4: 10\n",
      "of: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "as suggested by the TA, if we have multiple words as keyword (e.g. artificial intelligence),\n",
    "we split the keyword as 'artificial' and 'intelligence'\n",
    "'''\n",
    "#concatenate keywords\n",
    "concatenated_keywords=set([word.strip() for i,ab in enumerate(data) for word in ab[3]])\n",
    "T='artificial intelligence'\n",
    "output='T: artificial intelligence\\n'\n",
    "#flatten each word in keyword\n",
    "keywords=[k.split() for k in concatenated_keywords]\n",
    "keywords=[p for x in keywords for p in x]\n",
    "keywords=set(keywords)\n",
    "X=[[k,0] for k in keywords]\n",
    "\n",
    "for i,kw in enumerate(keywords):\n",
    "    simil=min(nltk.distance.edit_distance('artificial',kw),nltk.distance.edit_distance('intelligence',kw))\n",
    "    X[i][1]=simil\n",
    "    output+=f'{kw}: {simil}\\n'\n",
    "X=sorted(X,key=lambda x: x[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1.5', 10], ['acute', 8], ['adaptive', 8], ['analysis', 8], ['artificial', 0], ['cell', 8], ['change', 10], ['chatgpt-4', 10], ['chronic', 8], ['climate', 9], ['disease', 9], ['doping', 9], ['education', 9], ['electric', 9], ['energy', 9], ['extreme', 9], ['failed', 9], ['fit', 8], ['flood', 9], ['fluorinated', 10], ['frequency', 8], ['fuel', 8], ['gemini', 8], ['goodness', 10], ['graphene', 9], ['hazard', 10], ['hydrogen', 9], ['hydrological', 8], ['injury', 9], ['intelligence', 0], ['kashmir', 9], ['kidney', 9], ['language', 9], ['laser-induced', 9], ['maladaptive', 9], ['medical', 6], ['micro-supercapacitors', 18], ['model', 9], ['modeling', 8], ['of', 9], ['omnie-learning', 10], ['ophthalmology', 11], ['phosphorus', 10], ['polyimide', 8], ['prolarge', 9], ['renewable', 9], ['repair', 8], ['state', 9], ['sustainability', 10], ['theory', 9], ['valley', 9], ['value', 10], ['vehicles', 8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53, 53)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X)\n",
    "len(X),len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]\n",
    "concatenated_keywords=set([word.strip() for i,ab in enumerate(data) for word in ab[3]])\n",
    "\n",
    "split_kk=[[k[0].split(),k[1]] for k in kk]\n",
    "for kws,ids in split_kk:\n",
    "    for kw in kws:\n",
    "        for id in ids:\n",
    "            simils=[]\n",
    "            for word in data[id][0].split():\n",
    "                simils.append(nltk.distance.edit_distance(word,kw))\n",
    "            simil=min(simils)\n",
    "            Y.append([kw,id,simil])\n",
    "#each element: keyword, document it appears in, edit distance with the title of that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1.5', 0],\n",
       " ['acute', 4],\n",
       " ['adaptive', 6],\n",
       " ['analysis', 0],\n",
       " ['artificial', 0],\n",
       " ['cell', 0],\n",
       " ['change', 4],\n",
       " ['chatgpt-4', 0],\n",
       " ['chronic', 5],\n",
       " ['climate', 6],\n",
       " ['disease', 6],\n",
       " ['doping', 5],\n",
       " ['education', 5],\n",
       " ['electric', 0],\n",
       " ['energy', 5],\n",
       " ['extreme', 5],\n",
       " ['failed', 4],\n",
       " ['fit', 2],\n",
       " ['flood', 0],\n",
       " ['fluorinated', 0],\n",
       " ['frequency', 0],\n",
       " ['fuel', 0],\n",
       " ['gemini', 0],\n",
       " ['goodness', 6],\n",
       " ['graphene', 0],\n",
       " ['hazard', 5],\n",
       " ['hydrogen', 7],\n",
       " ['hydrological', 8],\n",
       " ['injury', 2],\n",
       " ['intelligence', 0],\n",
       " ['kashmir', 4],\n",
       " ['kidney', 0],\n",
       " ['language', 0],\n",
       " ['laser-induced', 6],\n",
       " ['maladaptive', 7],\n",
       " ['medical', 4],\n",
       " ['micro-supercapacitors', 0],\n",
       " ['model', 1],\n",
       " ['modeling', 6],\n",
       " ['of', 0],\n",
       " ['omnie-learning', 10],\n",
       " ['ophthalmology', 8],\n",
       " ['phosphorus', 2],\n",
       " ['polyimide', 0],\n",
       " ['prolarge', 3],\n",
       " ['renewable', 7],\n",
       " ['repair', 5],\n",
       " ['state', 3],\n",
       " ['sustainability', 10],\n",
       " ['theory', 3],\n",
       " ['valley', 5],\n",
       " ['value', 4],\n",
       " ['vehicles', 1]]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=sorted(Y,key=lambda x: x[0])\n",
    "i=0\n",
    "new_Y=[]\n",
    "w=Y[0][0]\n",
    "distances=[]\n",
    "while i<len(Y):\n",
    "    if Y[i][0]!=w:\n",
    "        new_Y.append([w,min(distances)])\n",
    "        w=Y[i][0]\n",
    "        distances=[Y[i][2]]\n",
    "    else:\n",
    "        distances.append(Y[i][2])\n",
    "    i+=1\n",
    "new_Y.append([w,min(distances)])\n",
    "new_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.15189963058878994, pvalue=0.2775708184984361)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npX=np.array([x[1] for x in X])\n",
    "npY=np.array([y[1] for y in new_Y])\n",
    "stats.pearsonr(npX,npY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        11.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "         5.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  4.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         8.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         3.],\n",
       "       [ 1.,  0.,  3.,  1.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         2.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        10.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         2.],\n",
       "       [ 0.,  0.,  0.,  0.,  2.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  0.,\n",
       "         0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         7.]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all unique keywords\n",
    "concatenated_keywords=set([word.strip() for i,ab in enumerate(data) for word in ab[3]])\n",
    "keywords=[k.split() for k in concatenated_keywords]\n",
    "keywords=[p for x in keywords for p in x]\n",
    "keywords=list(set(keywords))\n",
    "M=np.zeros((len(data),len(keywords)))\n",
    "\n",
    "for i,kw in enumerate(keywords):\n",
    "    for j,ab in enumerate(data):\n",
    "        abstract=data[j][2]\n",
    "        num=abstract.count(kw)\n",
    "        M[j,i]=num\n",
    "\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boolean_matching(query,db=M,kws=keywords):\n",
    "    qkeys=query.split()\n",
    "    result=np.ones(len(data))\n",
    "    for qkey in qkeys:\n",
    "        i=kws.index(qkey)\n",
    "        doc_res=db[:,i]\n",
    "        doc_res[doc_res>0]=1\n",
    "        result=np.logical_and(result,doc_res)\n",
    "    return result\n",
    "\n",
    "query='artificial intelligence'\n",
    "boolean_matching(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emanu\\AppData\\Local\\Temp\\ipykernel_9968\\3859817929.py:6: RuntimeWarning: divide by zero encountered in divide\n",
      "  idf=np.log10(n/df)\n",
      "C:\\Users\\emanu\\AppData\\Local\\Temp\\ipykernel_9968\\3859817929.py:8: RuntimeWarning: invalid value encountered in multiply\n",
      "  tfidf_M=M*idf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.39794001, 1.39794001, 1.39794001, 1.39794001, 1.39794001])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=len(data)\n",
    "df=np.zeros(len(keywords))\n",
    "for i,col in enumerate(M.T):\n",
    "    df[i]=len(col[col>0])\n",
    "\n",
    "idf=np.log10(n/df)\n",
    "\n",
    "tfidf_M=M*idf\n",
    "\n",
    "def tfidf_matching(query,tfidf=tfidf_M,kws=keywords,df=df):\n",
    "    qkeys=query.split()\n",
    "    results=np.zeros(len(data))\n",
    "    for qkey in qkeys:\n",
    "        if qkey not in keywords:\n",
    "            continue\n",
    "        i=kws.index(qkey)\n",
    "        for col in M:\n",
    "            results+=col[i]*np.log10(n/df[i])\n",
    "    return results\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "query='artificial intelligence'\n",
    "tfidf_matching(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 2., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 3., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 5.])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in M.T:\n",
    "    print(len(i[i>0]))\n",
    "    print(i)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
