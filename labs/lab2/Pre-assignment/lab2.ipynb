{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     D:/misc/Projects/Python/NLP/misc...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     D:/misc/Projects/Python/NLP/misc...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     D:/misc/Projects/Python/NLP/misc...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download tokenizer\n",
    "path='D:/misc/Projects/Python/NLP/misc'\n",
    "nltk.download('punkt_tab',download_dir=path)\n",
    "nltk.download('wordnet',download_dir=path)\n",
    "nltk.download('omw-1.4',download_dir=path)\n",
    "nltk.data.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keywords.txt','r') as f:\n",
    "    kws=f.read().lower()\n",
    "with open('abstracts.txt','r') as f:\n",
    "    abstracts=f.read().lower()\n",
    "    \n",
    "#remove punctuation from abstracts\n",
    "punctuation=['\"','!','.',',',\"'\",'(',')',';','``',\"''\",'?','--',':']\n",
    "for punct in punctuation:\n",
    "    abstracts=abstracts.replace(punct,'')\n",
    "\n",
    "#get all the keywords for each abstract\n",
    "kws=kws.replace('\\n','').split('-next-')\n",
    "kws=[kw.split(',') for kw in kws]\n",
    "for kw in kws:\n",
    "    for i in range(len(kw)):\n",
    "        kw[i]=kw[i].strip()\n",
    "\n",
    "#get text for each abstract\n",
    "abstracts=abstracts.replace('\\n','').split('-next-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deep learning', 'abnormality detection', 'social media forensics', 'text classification', 'rules generation']\n",
      "\n",
      "\n",
      "deep learning 1\n",
      "abnormality detection 0\n",
      "social media forensics 1\n",
      "text classification 1\n",
      "rules generation 0\n"
     ]
    }
   ],
   "source": [
    "#single string from all abstracts\n",
    "doc=''.join(abstract for abstract in abstracts)\n",
    "#keywords to be used as queries from the keywords file\n",
    "keywords=[kws[0][1],kws[19][3],kws[2][4],kws[5][2],kws[10][3]]\n",
    "print(keywords)\n",
    "\n",
    "tokenized_doc=nltk.tokenize.word_tokenize(doc)\n",
    "output='\\n\\n'\n",
    "\n",
    "#logical query matching \n",
    "for keyword in keywords:\n",
    "    tokenized_keyword=nltk.tokenize.word_tokenize(keyword)\n",
    "    check=True\n",
    "    for word in tokenized_keyword:\n",
    "        if word not in tokenized_doc:\n",
    "            check=False\n",
    "            break\n",
    "    output+=f'{keyword} {int(check)}\\n'\n",
    "print(output[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dictionary class to conveniently add new items\n",
    "class inverted_index_dict:\n",
    "    def __init__(self) -> None:\n",
    "        self.dict=defaultdict()\n",
    "        \n",
    "    def add_item(self,token,doc):\n",
    "        if self.dict.__contains__(token):\n",
    "            if doc not in self.dict[token]:\n",
    "                self.dict[token].append(doc)\n",
    "        else:\n",
    "            self.dict[token]=[doc]\n",
    "    \n",
    "inverted_index=inverted_index_dict()\n",
    "\n",
    "#iterate over all abstracts, for each keyword add the document id to the dictionary\n",
    "for i,abstract in enumerate(abstracts):\n",
    "    doc_id=f'A{i}'\n",
    "    tokenized=nltk.tokenize.word_tokenize(abstract)\n",
    "    for token in tokenized:\n",
    "        inverted_index.add_item(token,doc_id)\n",
    "\n",
    "#output text\n",
    "output=''\n",
    "items=list(inverted_index.dict.items())\n",
    "for item in items:\n",
    "    output+=f'{item[0]} '\n",
    "    for document_id in item[1]:\n",
    "        output+=f'-> {document_id}'\n",
    "    output+='\\n'\n",
    "\n",
    "#save to file since its size is too large\n",
    "with open('inverted_index.txt','w') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the documents where a keyword appears it is enough to call the dictionary with the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep-> ['A0', 'A1']\n",
      "learning-> ['A0', 'A1', 'A10', 'A12', 'A15']\n",
      "neural-> ['A1', 'A7', 'A12', 'A18']\n",
      "networks-> ['A8']\n"
     ]
    }
   ],
   "source": [
    "#for instance, for these keywords:\n",
    "keywords_example=['deep','learning','neural','networks']\n",
    "for k in keywords_example:\n",
    "    print(f'{k}-> {inverted_index.dict[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relaxed matching definition\n",
    "def relaxed_matching(w1,w2):\n",
    "    w2=list(w2)\n",
    "    counter=0\n",
    "    for c in w1:\n",
    "        try:\n",
    "            i=w2.index(c)\n",
    "            w2[i]=''\n",
    "            counter+=1\n",
    "        except:\n",
    "            continue\n",
    "    return counter/len(w1)>=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Query on:   ['deep', 'learning']\n",
      "deep - deep    ||    learning - learning-based    ||    \n",
      "query result: 1\n",
      "\n",
      "Query on:   ['abnormality', 'detection']\n",
      "abnormality - abnormalities    ||    detection - detection    ||    \n",
      "query result: 1\n",
      "\n",
      "Query on:   ['social', 'media', 'forensics']\n",
      "social - accomplishing    ||    media - mediation    ||    forensics - forensics    ||    \n",
      "query result: 1\n",
      "\n",
      "Query on:   ['text', 'classification']\n",
      "text - extraction    ||    classification - classification    ||    \n",
      "query result: 1\n",
      "\n",
      "Query on:   ['rules', 'generation']\n",
      "rules - result    ||    generation - presentation    ||    \n",
      "query result: 1\n"
     ]
    }
   ],
   "source": [
    "#single string from all abstracts\n",
    "doc=''.join(abstract for abstract in abstracts)\n",
    "#keywords to be used as queries from the keywords file\n",
    "keywords=[kws[0][1],kws[19][3],kws[2][4],kws[5][2],kws[10][3]]\n",
    "#print(keywords)\n",
    "\n",
    "tokenized_doc=nltk.tokenize.word_tokenize(doc)\n",
    "output='\\n\\n'\n",
    "\n",
    "#logical query matching \n",
    "for keyword in keywords:\n",
    "    tokenized_keyword=nltk.tokenize.word_tokenize(keyword)\n",
    "    found_words=['not found']*len(tokenized_keyword)\n",
    "    check=True\n",
    "    for i,word in enumerate(tokenized_keyword):\n",
    "        for word2 in tokenized_doc:\n",
    "            kwcheck=False\n",
    "            if relaxed_matching(word,word2):\n",
    "                found_words[i]=word2\n",
    "                kwcheck=True\n",
    "                break\n",
    "        if not kwcheck:\n",
    "            check=False\n",
    "    output+=f'Query on:   {tokenized_keyword}\\n'\n",
    "    for i in range(len(found_words)):\n",
    "        output+=f'{tokenized_keyword[i]} - {found_words[i]}    ||    '\n",
    "    output+=f'\\nquery result: {int(check)}\\n\\n'\n",
    "print(output[:-2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
